{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    },
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "#### 1. Importing all the libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os\n",
    "import shutil\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib.collections import LineCollection\n",
    "import seaborn as sns\n",
    "import sys\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn import metrics\n",
    "from sklearn import tree\n",
    "from sklearn.ensemble import ExtraTreesClassifier\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.ensemble import AdaBoostClassifier\n",
    "from sklearn.ensemble import GradientBoostingClassifier"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "#### 2. Define the Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "# Function that create the episode data - sample randomaly\n",
    "def get_data(episode_size,policy,mode):\n",
    "    global dataset\n",
    "    if mode=='train':\n",
    "        if policy==0:\n",
    "             dataset=data.sample(n=episode_size)\n",
    "        else:\n",
    "            dataset=data\n",
    "    else:\n",
    "        dataset = pd.read_csv(location + '/' + file +'_test_int.csv', index_col=0)\n",
    "    return dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "# Function that separate the episode data into features and label\n",
    "def data_separate (dataset):\n",
    "    global X\n",
    "    global y    \n",
    "    X = dataset.iloc[:,0:dataset.shape[1]-1]  # all rows, all the features and no labels\n",
    "    y = dataset.iloc[:, -1]  # all rows, label only\n",
    "    return X,y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "# Function that split the episode data into train and test\n",
    "def data_split(X,y):\n",
    "    global X_train_main\n",
    "    global X_test_main   \n",
    "    global y_train\n",
    "    global y_test  \n",
    "    from sklearn.model_selection import train_test_split\n",
    "    X_train_main, X_test_main, y_train, y_test = train_test_split(X,y,test_size=0.2,random_state=4)\n",
    "    return X_train_main, X_test_main, y_train, y_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "# Function that chooses exploration or explotation method\n",
    "def exploration_explotation(epsilon):\n",
    "    global exploration \n",
    "    if np.random.rand() < epsilon:  \n",
    "        exploration=1\n",
    "    else:\n",
    "        exploration=0    \n",
    "    return exploration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "# Function that returns all available actions in the state given as an argument: \n",
    "def available_actions(number_of_columns,columns,initial_state,current_state,trashold, exploration):\n",
    "    global exclude\n",
    "    global all_columns\n",
    "#    exclude=[]\n",
    "    all_columns=np.arange(number_of_columns+1)\n",
    "    # remove columns that have been already selected\n",
    "    exclude=columns.copy()\n",
    "    # remove the initial_state and the current_state\n",
    "    exclude.extend([initial_state, current_state])\n",
    "    available_act = list(set(all_columns)-set(exclude))\n",
    "    # remove actions that have negetiv Q value\n",
    "    if exploration==0:\n",
    "        index = np.where(Q[current_state,available_act] > trashold)[1]\n",
    "        available_act= [available_act[i] for i in index.tolist()]\n",
    "    return available_act"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "def sample_next_action(current_state, Q, available_act, exploration):\n",
    "    global available_act_q_value\n",
    "    available_act_q_value = [float(q) for q in np.array(Q[current_state, available_act]).reshape(-1)]\n",
    "    \n",
    "    if exploration == 1: \n",
    "        # Random selection\n",
    "        next_action = int(np.random.choice(available_act, 1).item())\n",
    "    else: \n",
    "        # Greedy selection according to max value\n",
    "        maxQ = max(available_act_q_value)\n",
    "        count = available_act_q_value.count(maxQ)\n",
    "        \n",
    "        if count > 1:\n",
    "            max_columns = [i for i in range(len(available_act_q_value)) if available_act_q_value[i] == maxQ]\n",
    "            i = int(np.random.choice(max_columns, 1).item())\n",
    "        else:\n",
    "            i = available_act_q_value.index(maxQ)\n",
    "        \n",
    "        next_action = available_act[i]  \n",
    "    \n",
    "    return next_action\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "# function that update a list with all selected columns in the episode\n",
    "def update_columns(action, columns):\n",
    "    update_columns=columns\n",
    "    update_columns.append(action)\n",
    "    return update_columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "\n",
    "def update_X_train_X_test(episode_columns, X_train_main_episode, X_test_main_episode):\n",
    "    # Convert numeric indices to column names if necessary\n",
    "    if episode_columns and isinstance(episode_columns[0], int):\n",
    "        actual_columns = list(X_train_main_episode.columns)  # Get the column names\n",
    "        episode_columns = [actual_columns[i] for i in episode_columns if i < len(actual_columns)]\n",
    "    \n",
    "    # Ensure episode_columns exist in X_train_main_episode\n",
    "    valid_columns = [col for col in episode_columns if col in X_train_main_episode.columns]\n",
    "\n",
    "    if not valid_columns:\n",
    "        print(f\"Warning: No valid columns found. Given: {episode_columns}, Available: {list(X_train_main_episode.columns)}\")\n",
    "\n",
    "    return X_train_main_episode[valid_columns], X_test_main_episode[valid_columns]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "# Function that run the learner and get the error to the current episode columns list\n",
    "def Learner(X_train, X_test,y_train, y_test):\n",
    "    global learner\n",
    "    global y_pred\n",
    "    if learner_model == 'DT':\n",
    "        learner = tree.DecisionTreeClassifier()\n",
    "        learner = learner.fit(X_train, y_train)\n",
    "        y_pred = learner.predict(X_test)\n",
    "    elif learner_model == 'KNN':\n",
    "        learner = KNeighborsClassifier(metric='hamming',n_neighbors=5)\n",
    "        learner = learner.fit(X_train, y_train)\n",
    "        y_pred = learner.predict(X_test)        \n",
    "    elif learner_model == 'SVM':\n",
    "        learner = SVC()\n",
    "        learner = learner.fit(X_train, y_train)\n",
    "        y_pred = learner.predict(X_test)        \n",
    "    elif learner_model == 'NB':\n",
    "        learner = MultinomialNB()\n",
    "        learner = learner.fit(X_train, y_train)\n",
    "        y_pred = learner.predict(X_test)\n",
    "    elif learner_model == 'AB':\n",
    "        learner = AdaBoostClassifier()\n",
    "        learner = learner.fit(X_train, y_train)\n",
    "        y_pred = learner.predict(X_test)  \n",
    "    elif learner_model == 'GB':\n",
    "        learner = GradientBoostingClassifier()\n",
    "        learner = learner.fit(X_train, y_train)\n",
    "        y_pred = learner.predict(X_test)\n",
    "    elif learner_model == 'VQC':\n",
    "        # VQC Code Integration\n",
    "        from pennylane import numpy as np\n",
    "        import pennylane as qml\n",
    "        from pennylane.optimize import AdamOptimizer\n",
    "        import time, math\n",
    "        \n",
    "        num_qubits = X_train.shape[1]\n",
    "        num_layers = 2\n",
    "        dev = qml.device(\"default.qubit\", wires=num_qubits)\n",
    "\n",
    "        def statepreparation(x):\n",
    "            qml.AngleEmbedding(x, wires=range(num_qubits), rotation='Y')\n",
    "\n",
    "        def layer(W):\n",
    "            for i in range(num_qubits):\n",
    "                qml.Rot(W[i, 0], W[i, 1], W[i, 2], wires=i)\n",
    "            for i in range(num_qubits - 1):\n",
    "                qml.CNOT(wires=[i, i + 1])\n",
    "\n",
    "        @qml.qnode(dev, interface=\"autograd\")\n",
    "        def circuit(weights, x):\n",
    "            statepreparation(x)\n",
    "            for W in weights:\n",
    "                layer(W)\n",
    "            return qml.expval(qml.PauliZ(0))\n",
    "\n",
    "        def variational_classifier(weights, bias, x):\n",
    "            return circuit(weights, x) + bias\n",
    "\n",
    "        def square_loss(labels, predictions):\n",
    "            loss = sum((l - p) ** 2 for l, p in zip(labels, predictions)) / len(labels)\n",
    "            return loss\n",
    "\n",
    "        def cost(weights, bias, X, Y):\n",
    "            predictions = [variational_classifier(weights, bias, x) for x in X]\n",
    "            return square_loss(Y, predictions)\n",
    "\n",
    "        np.random.seed(0)\n",
    "        weights_init = 0.01 * np.random.randn(num_layers, num_qubits, 3, requires_grad=True)\n",
    "        bias_init = np.array(0.0, requires_grad=True)\n",
    "        opt = AdamOptimizer(0.125)\n",
    "        \n",
    "        num_it = 70\n",
    "        batch_size = max(1, math.floor(len(X_train) / num_it))\n",
    "        weights = weights_init\n",
    "        bias = bias_init\n",
    "\n",
    "        for _ in range(num_it):\n",
    "            batch_index = np.random.randint(0, len(X_train), (batch_size,))\n",
    "            X_batch = X_train[batch_index]\n",
    "            Y_batch = y_train[batch_index]\n",
    "            weights, bias, _, _ = opt.step(cost, weights, bias, X_batch, Y_batch)\n",
    "        \n",
    "        y_pred = [np.sign(variational_classifier(weights, bias, x)) for x in X_test]\n",
    "        y_pred = (np.array(y_pred) > 0).astype(int)  # Convert back to 0/1 format\n",
    "    # Add the Qiskit-based VQC\n",
    "    elif learner_model == 'QiskitVQC':\n",
    "        from qiskit.circuit.library import ZZFeatureMap, RealAmplitudes\n",
    "        from qiskit_machine_learning.algorithms import VQC\n",
    "        from qiskit_algorithms.optimizers import COBYLA\n",
    "        from qiskit.primitives import Sampler\n",
    "        import time\n",
    "\n",
    "        num_features = X_train.shape[1]\n",
    "        feature_map = ZZFeatureMap(feature_dimension=num_features, reps=1)\n",
    "        ansatz = RealAmplitudes(num_qubits=num_features, reps=3)\n",
    "        optimizer = COBYLA(maxiter=100)\n",
    "        sampler = Sampler()\n",
    "\n",
    "        vqc = VQC(\n",
    "            sampler=sampler,\n",
    "            feature_map=feature_map,\n",
    "            ansatz=ansatz,\n",
    "            optimizer=optimizer,\n",
    "        )\n",
    "\n",
    "        start = time.time()\n",
    "        vqc.fit(X_train, y_train)\n",
    "        elapsed = time.time() - start\n",
    "        print(f\"Qiskit VQC training time: {round(elapsed)} seconds\")\n",
    "\n",
    "        y_pred = vqc.predict(X_test)\n",
    "\n",
    "    accuracy=metrics.accuracy_score(y_test, y_pred)\n",
    "    error=1-accuracy\n",
    "    return error"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "def q_update(current_state, action, learning_rate, reward):\n",
    "    # next_state = current action\n",
    "    max_index = np.where(Q[action,] == np.max(Q[action,]))[0]  # Use [0] instead of [1] for 1D arrays\n",
    "    \n",
    "    if max_index.shape[0] > 1:\n",
    "        # Resolve tie by selecting one randomly\n",
    "        max_index = int(np.random.choice(max_index, size=1).item())\n",
    "    else:\n",
    "        max_index = int(max_index[0])  # Convert the first element to a scalar\n",
    "\n",
    "    max_value = Q[action, max_index]\n",
    "\n",
    "    # Update the Q matrix\n",
    "    if Q[current_state, action] == 1:\n",
    "        Q[current_state, action] = learning_rate * reward\n",
    "    else:\n",
    "        Q[current_state, action] = Q[current_state, action] + learning_rate * (\n",
    "            reward + (discount_factor * max_value) - Q[current_state, action]\n",
    "        )\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Experiment mangment"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "#### 3. Define the parameters "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "## for run time ##\n",
    "N_features=5\n",
    "N_data=1\n",
    "## for run time ##\n",
    "\n",
    "#Experiment: \n",
    "experiment='test'\n",
    "number_of_experiment=1\n",
    "\n",
    "# Dataset parameters #\n",
    "location = 'Datasets/adult'\n",
    "outputlocation='Datasets'\n",
    "file='adult' #adult #diabetic_data #no_show\n",
    "#np.random.seed(3)\n",
    "\n",
    "# Q learning parameter # \n",
    "learning_rate=0.005\n",
    "discount_factor = 0.01 #0\n",
    "epsilon = 0.1\n",
    "\n",
    "# Learner and episode parameters #\n",
    "learner_model = 'VQC' #DT #KNN #SVM\n",
    "episode_size=100\n",
    "internal_trashold=0\n",
    "external_trashold=0\n",
    "filename= file +'_int.csv'\n",
    "\n",
    "#Experiments folder management: \n",
    "#if not os.path.exists('/Experiments'):\n",
    "#    os.makedirs('/Experiments') \n",
    "if not os.path.exists('Experiments/'+ str(experiment)):\n",
    "    os.makedirs('Experiments/'+ str(experiment))\n",
    "else:\n",
    "    shutil.rmtree('Experiments/'+ str(experiment))          #removes all the subdirectories!\n",
    "    os.makedirs('Experiments/'+ str(experiment))\n",
    "#writer = pd.ExcelWriter('Experiments/'+ str(experiment) + '/df.xlsx') \n",
    "\n",
    "\n",
    "\n",
    "text_file = open('Experiments/'+ str(experiment) +'/parameters.txt', \"w\")\n",
    "text_file.write('experiment: ' + str(experiment)+ '\\n')\n",
    "text_file.write('number of experiments: ' + str(number_of_experiment)+ '\\n')\n",
    "text_file.write('file: ' + str(file)+ '\\n')\n",
    "text_file.write('learner model: ' + str(learner_model)+ '\\n')\n",
    "text_file.write('episode size: ' + str(episode_size)+ '\\n')\n",
    "#text_file.write('numbers of epocs: ' + str(epocs)+ '\\n')\n",
    "text_file.write('internal trashold: ' + str(internal_trashold)+ '\\n')\n",
    "text_file.write('external trashold: ' + str(external_trashold)+ '\\n')\n",
    " \n",
    "text_file.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "#### 4. Run all experiments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Experiments 0 start\n",
      "number of columns: 5 (exclude class column)\n",
      "Number of episodes: 2443.0\n",
      "initial state number: 5 (the last dummy column we have created)\n"
     ]
    },
    {
     "ename": "KeyError",
     "evalue": "\"None of [Index([36], dtype='int64')] are in the [columns]\"",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[14], line 83\u001b[0m\n\u001b[1;32m     81\u001b[0m X_train_episode, X_test_episode \u001b[38;5;241m=\u001b[39mupdate_X_train_X_test(episode_columns,X_train_main_episode, X_test_main_episode)\n\u001b[1;32m     82\u001b[0m \u001b[38;5;66;03m# Update the accuracy of the current columns\u001b[39;00m\n\u001b[0;32m---> 83\u001b[0m episode_error\u001b[38;5;241m=\u001b[39m \u001b[43mLearner\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX_train_episode\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mX_test_episode\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my_train_episode\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my_test_episode\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     84\u001b[0m \u001b[38;5;66;03m# Update reward\u001b[39;00m\n\u001b[1;32m     85\u001b[0m episode_reward\u001b[38;5;241m=\u001b[39mepisode_last_error\u001b[38;5;241m-\u001b[39mepisode_error\n",
      "Cell \u001b[0;32mIn[11], line 79\u001b[0m, in \u001b[0;36mLearner\u001b[0;34m(X_train, X_test, y_train, y_test)\u001b[0m\n\u001b[1;32m     77\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m _ \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(num_it):\n\u001b[1;32m     78\u001b[0m     batch_index \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39mrandom\u001b[38;5;241m.\u001b[39mrandint(\u001b[38;5;241m0\u001b[39m, \u001b[38;5;28mlen\u001b[39m(X_train), (batch_size,))\n\u001b[0;32m---> 79\u001b[0m     X_batch \u001b[38;5;241m=\u001b[39m \u001b[43mX_train\u001b[49m\u001b[43m[\u001b[49m\u001b[43mbatch_index\u001b[49m\u001b[43m]\u001b[49m\n\u001b[1;32m     80\u001b[0m     Y_batch \u001b[38;5;241m=\u001b[39m y_train[batch_index]\n\u001b[1;32m     81\u001b[0m     weights, bias, _, _ \u001b[38;5;241m=\u001b[39m opt\u001b[38;5;241m.\u001b[39mstep(cost, weights, bias, X_batch, Y_batch)\n",
      "File \u001b[0;32m~/.venv/lib/python3.13/site-packages/pandas/core/frame.py:4108\u001b[0m, in \u001b[0;36mDataFrame.__getitem__\u001b[0;34m(self, key)\u001b[0m\n\u001b[1;32m   4106\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m is_iterator(key):\n\u001b[1;32m   4107\u001b[0m         key \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mlist\u001b[39m(key)\n\u001b[0;32m-> 4108\u001b[0m     indexer \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcolumns\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_get_indexer_strict\u001b[49m\u001b[43m(\u001b[49m\u001b[43mkey\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mcolumns\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m[\u001b[38;5;241m1\u001b[39m]\n\u001b[1;32m   4110\u001b[0m \u001b[38;5;66;03m# take() does not accept boolean indexers\u001b[39;00m\n\u001b[1;32m   4111\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mgetattr\u001b[39m(indexer, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mdtype\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28;01mNone\u001b[39;00m) \u001b[38;5;241m==\u001b[39m \u001b[38;5;28mbool\u001b[39m:\n",
      "File \u001b[0;32m~/.venv/lib/python3.13/site-packages/pandas/core/indexes/base.py:6200\u001b[0m, in \u001b[0;36mIndex._get_indexer_strict\u001b[0;34m(self, key, axis_name)\u001b[0m\n\u001b[1;32m   6197\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m   6198\u001b[0m     keyarr, indexer, new_indexer \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_reindex_non_unique(keyarr)\n\u001b[0;32m-> 6200\u001b[0m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_raise_if_missing\u001b[49m\u001b[43m(\u001b[49m\u001b[43mkeyarr\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mindexer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43maxis_name\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   6202\u001b[0m keyarr \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtake(indexer)\n\u001b[1;32m   6203\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(key, Index):\n\u001b[1;32m   6204\u001b[0m     \u001b[38;5;66;03m# GH 42790 - Preserve name from an Index\u001b[39;00m\n",
      "File \u001b[0;32m~/.venv/lib/python3.13/site-packages/pandas/core/indexes/base.py:6249\u001b[0m, in \u001b[0;36mIndex._raise_if_missing\u001b[0;34m(self, key, indexer, axis_name)\u001b[0m\n\u001b[1;32m   6247\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m nmissing:\n\u001b[1;32m   6248\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m nmissing \u001b[38;5;241m==\u001b[39m \u001b[38;5;28mlen\u001b[39m(indexer):\n\u001b[0;32m-> 6249\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mKeyError\u001b[39;00m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mNone of [\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mkey\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m] are in the [\u001b[39m\u001b[38;5;132;01m{\u001b[39;00maxis_name\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m]\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m   6251\u001b[0m     not_found \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mlist\u001b[39m(ensure_index(key)[missing_mask\u001b[38;5;241m.\u001b[39mnonzero()[\u001b[38;5;241m0\u001b[39m]]\u001b[38;5;241m.\u001b[39munique())\n\u001b[1;32m   6252\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mKeyError\u001b[39;00m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mnot_found\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m not in index\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "\u001b[0;31mKeyError\u001b[0m: \"None of [Index([36], dtype='int64')] are in the [columns]\""
     ]
    }
   ],
   "source": [
    "for e in range (number_of_experiment):\n",
    "    if not os.path.exists('Experiments/'+ str(experiment)+ '/'+ str(e)):\n",
    "        os.makedirs('Experiments/'+ str(experiment)+ '/'+ str(e))\n",
    "    else:\n",
    "        shutil.rmtree('Experiments/'+ str(experiment)+ '/'+ str(e))          #removes all the subdirectories!\n",
    "        os.makedirs('Experiments/'+ str(experiment)+ '/'+ str(e))\n",
    "    print ('Experiments ' + str(e) + ' start')\n",
    "##########################Experiment setup##########################\n",
    "    # Read the data\n",
    "    data = pd.read_csv(location + '/' + filename, index_col=0)\n",
    "    \n",
    "##### for run time - start #####\n",
    "    import timeit\n",
    "    start = timeit.default_timer()\n",
    "    size= int(N_data* len(data.index))\n",
    "    data = data.sample(n=size)\n",
    "    data=data.iloc[:,-N_features-1:]\n",
    "##### for run time - end #####\n",
    "    \n",
    "    #Set the number of iterations:\n",
    "    interations=10*len(data.index)/episode_size\n",
    "    # Set the number of columns exclude the class column\n",
    "    number_of_columns=data.shape[1]-1 \n",
    "    print (\"number of columns: \"+ str(number_of_columns) +\" (exclude class column)\" ) \n",
    "    # Set the number of episodes \n",
    "    # episodes_number=epocs*len(data.index)/episode_size\n",
    "    episodes_number=interations\n",
    "    print (\"Number of episodes: \"+ str(episodes_number) ) \n",
    "    # Initialize matrix Q as a 1 values matrix:\n",
    "    #Q = np.matrix(np.ones([number_of_columns+1,number_of_columns+1])) # we will use the last dummy columns as initial state s\n",
    "    Q = np.matrix(np.ones([number_of_columns+1,number_of_columns+1])) # we will use the last dummy columns as initial state s\n",
    "    # Set initial_state to be the last dummy column we have created\n",
    "    initial_state=number_of_columns\n",
    "    # define data frame to save episode policies results\n",
    "    df = pd.DataFrame(columns=('episode','episode_columns','policy_columns','policy_accuracy_train','policy_accuracy_test'))\n",
    "    print (\"initial state number: \"+ str(initial_state) + \" (the last dummy column we have created)\") \n",
    "\n",
    "    ##########################  episode  ##########################  \n",
    "    for i in range (int(episodes_number)):\n",
    "    ########## Begining of episode  ############\n",
    "        # Initiate lists for available_act, episode_columns and and the policy mode & episode_error\n",
    "        episode_available_act=list(np.arange(number_of_columns))\n",
    "        episode_columns=[]\n",
    "        policy=0\n",
    "        episode_error=0\n",
    "        # Initiate the error to 0.5\n",
    "        episode_last_error=0.5\n",
    "        # Initiate current_state to be initial_state\n",
    "        episode_current_state=initial_state\n",
    "        # Create the episode data \n",
    "        episode= get_data(episode_size, policy=0, mode='train')\n",
    "        # Separate the episode data into features and label\n",
    "        X_episode,y_episode=data_separate(episode)\n",
    "        # Split the data into train and test \n",
    "        X_train_main_episode, X_test_main_episode, y_train_episode, y_test_episode = data_split(X_episode,y_episode)\n",
    "        if i<episodes_number*0.25:\n",
    "            epsilon=0.9\n",
    "            learning_rate=0.09\n",
    "        elif i<episodes_number*0.5:\n",
    "            epsilon=0.5\n",
    "            learning_rate=0.05\n",
    "        elif i<episodes_number*0.75:\n",
    "            epsilon=0.3\n",
    "            learning_rate=0.01\n",
    "        else:\n",
    "            epsilon=0.1\n",
    "            learning_rate=0.005\n",
    "        ########## Q learning start ############\n",
    "\n",
    "        while len(episode_available_act)>0:\n",
    "            # Get exploration or explotation flag \n",
    "            exploration=exploration_explotation(epsilon)\n",
    "            # Get available actions in the current state\n",
    "            episode_available_act = available_actions(number_of_columns,episode_columns,initial_state,episode_current_state,internal_trashold,exploration)\n",
    "            if len(episode_available_act)>0:\n",
    "                # Sample next action to be performed\n",
    "                episode_action = sample_next_action(episode_current_state, Q, episode_available_act, exploration)\n",
    "                # Update the episode_columns\n",
    "                episode_columns=update_columns(episode_action,episode_columns)\n",
    "                # Update the dataset to include all episode columns + current selected action (column)\n",
    "                X_train_episode, X_test_episode =update_X_train_X_test(episode_columns,X_train_main_episode, X_test_main_episode)\n",
    "                # Update the accuracy of the current columns\n",
    "                \n",
    "print(f\"Debug - Episode Columns: {episode_columns}\")\n",
    "print(f\"Debug - Available Train Columns: {list(X_train_episode.columns)}\")\n",
    "print(f\"Debug - Available Test Columns: {list(X_test_episode.columns)}\")\n",
    "\n",
    "\n",
    "print(f\"Debug - Episode Columns (Names): {episode_columns}\")\n",
    "print(f\"Debug - Available Train Columns: {list(X_train_episode.columns)}\")\n",
    "print(f\"Debug - Available Test Columns: {list(X_test_episode.columns)}\")\n",
    "\n",
    "episode_error= Learner(X_train_episode, X_test_episode, y_train_episode, y_test_episode)\n",
    "\n",
    "\n",
    "                # Update reward\n",
    "                episode_reward=episode_last_error-episode_error\n",
    "                # Update Q matrix\n",
    "                q_update(episode_current_state,episode_action,learning_rate, episode_reward)\n",
    "                # Update parameters for next round \n",
    "#                 if episode_current_state==initial_state:\n",
    "#                     beta=abs(episode_reward-Q[episode_current_state,episode_action])\n",
    "#                     epsilon=final_epsilon+(beta*(1-final_epsilon))\n",
    "                #    learning_rate=final_learning_rate+(beta*(1-final_learning_rate))\n",
    "                episode_current_state=episode_action\n",
    "                episode_last_error=episode_error\n",
    "                 \n",
    "        ########## Q learning End ############\n",
    "\n",
    "        #Save Q matrix: \n",
    "        if (i%100 ==0):\n",
    "            Q_save=pd.DataFrame(Q)\n",
    "            Q_save.to_csv('Experiments/'+ str(experiment)+ '/'+ str(e)+ '/Q.'+ str(i+1) + '.csv') \n",
    "\n",
    "        # Calculate policy \n",
    "        policy_available_actions=list(np.arange(number_of_columns))\n",
    "        policy_columns=[]\n",
    "        policy_current_state=initial_state\n",
    "        while len(policy_available_actions)>0:\n",
    "            # Get available actions in the current state\n",
    "            policy_available_actions = available_actions(number_of_columns,policy_columns,initial_state,policy_current_state, external_trashold, exploration=0)\n",
    "            # # Sample next action to be performed\n",
    "            if len(policy_available_actions)>0:\n",
    "                policy_select_action = sample_next_action(policy_current_state, Q, policy_available_actions, exploration=0)\n",
    "                # Update the episode_columns\n",
    "                policy_columns=update_columns(policy_select_action,policy_columns)\n",
    "                policy_current_state=policy_select_action\n",
    "        # Calculate policy_accuracy    \n",
    "        if len(policy_columns)>0:\n",
    "            ##for training dataset##\n",
    "            policy_data=get_data(episode_size,policy=1,mode='train')\n",
    "            X_policy,y_policy=data_separate(policy_data)\n",
    "            X_train_main_policy, X_test_main_policy, y_train_policy, y_test_policy = data_split(X,y)\n",
    "            X_train_policy, X_test_policy =update_X_train_X_test(policy_columns, X_train_main_policy, X_test_main_policy)\n",
    "            policy_error=Learner(X_train_policy, X_test_policy,y_train_policy, y_test_policy)\n",
    "            policy_accuracy_train=1-policy_error\n",
    "            ##for testing dataset##\n",
    "            policy_data=get_data(episode_size,policy=1,mode='test')\n",
    "            X_policy,y_policy=data_separate(policy_data)\n",
    "            X_train_main_policy, X_test_main_policy, y_train_policy, y_test_policy = data_split(X,y)\n",
    "            X_train_policy, X_test_policy =update_X_train_X_test(policy_columns, X_train_main_policy, X_test_main_policy)\n",
    "            policy_error=Learner(X_train_policy, X_test_policy,y_train_policy, y_test_policy)\n",
    "            policy_accuracy_test=1-policy_error \n",
    "        else:\n",
    "            policy_accuracy_train=0 \n",
    "            policy_accuracy_test=0\n",
    "        #df=df.append({'episode':str(i+1), 'episode_columns':str(episode_columns),'policy_columns':str(policy_columns),'policy_accuracy_train':policy_accuracy_train,'policy_accuracy_test':policy_accuracy_test}, ignore_index=True)\n",
    "        #new_row = pd.DataFrame([{'episode': str(i+1),\n",
    "        #                  'episode_columns': str(episode_columns),\n",
    "        #                  'policy_columns': str(policy_columns),\n",
    "        #                  'policy_accuracy_train': policy_accuracy_train,\n",
    "        #                  'policy_accuracy_test': policy_accuracy_test}])\n",
    "        #df = pd.concat([df, new_row], ignore_index=True)\n",
    "        df.loc[len(df)] = {\n",
    "            'episode': str(i+1),\n",
    "            'episode_columns': str(episode_columns),\n",
    "            'policy_columns': str(policy_columns),\n",
    "            'policy_accuracy_train': policy_accuracy_train,\n",
    "            'policy_accuracy_test': policy_accuracy_test\n",
    "        }\n",
    "\n",
    "        #Prints\n",
    "        print (\"episode \"+ str(i+1) +\" start\") \n",
    "        print (\"episode columns: \"+ str(episode_columns) + \" epsilon: \" + str(epsilon) + \" learning rate: \" + str(learning_rate) + \" error: \" +str(episode_error))\n",
    "        print (\"episode policy:\" + str(policy_columns) + \" train accuracy: \" + str(policy_accuracy_train)  + \" test accuracy: \" +str(policy_accuracy_test)) \n",
    "        print (\"episode \"+ str(i+1) +\" end\") \n",
    "    ########## End of episode  ############\n",
    "    #df.to_excel(writer, 'Experiment' + str(e))\n",
    "    df.to_excel(writer, sheet_name='Experiment' + str(e))\n",
    "    df_plot=df[['episode','policy_accuracy_train','policy_accuracy_test']]\n",
    "    plot=df_plot.plot()\n",
    "    fig = plot.get_figure()\n",
    "    fig.savefig('Experiments/'+ str(experiment) + '/plot_experiment_' + str(e) +'.png')\n",
    "    \n",
    "#writer.save()\n",
    "with pd.ExcelWriter('Experiments/'+ str(experiment) + '/df.xlsx') as writer:\n",
    "    df.to_excel(writer, sheet_name='Experiment' + str(e))\n",
    "\n",
    "## for run time ##\n",
    "stop = timeit.default_timer()\n",
    "print (stop - start)\n",
    "## for run time ##"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (venv)",
   "language": "python",
   "name": "venv"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
