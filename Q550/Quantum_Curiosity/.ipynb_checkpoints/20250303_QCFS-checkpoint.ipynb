{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    },
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "#### 1. Importing all the libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 309,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "import shutil\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from matplotlib.collections import LineCollection\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import pennylane as qml\n",
    "\n",
    "from sklearn import tree, metrics\n",
    "from sklearn.model_selection import train_test_split, cross_val_score\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.ensemble import AdaBoostClassifier, GradientBoostingClassifier, ExtraTreesClassifier\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "#### 2. Define the Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 310,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "# Function that create the episode data - sample randomaly\n",
    "def get_data(episode_size,policy,mode):\n",
    "    global dataset\n",
    "    if mode=='train':\n",
    "        if policy==0:\n",
    "             dataset=data.sample(n=episode_size)\n",
    "        else:\n",
    "            dataset=data\n",
    "    else:\n",
    "        dataset = pd.read_csv(location + '/' + file +'_test_int.csv', index_col=0)\n",
    "    return dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 311,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "# Function that separate the episode data into features and label\n",
    "def data_separate (dataset):\n",
    "    global X\n",
    "    global y    \n",
    "    X = dataset.iloc[:,0:dataset.shape[1]-1]  # all rows, all the features and no labels\n",
    "    y = dataset.iloc[:, -1]  # all rows, label only\n",
    "    return X,y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 312,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "# Function that split the episode data into train and test\n",
    "def data_split(X,y):\n",
    "    global X_train_main\n",
    "    global X_test_main   \n",
    "    global y_train\n",
    "    global y_test  \n",
    "    from sklearn.model_selection import train_test_split\n",
    "    X_train_main, X_test_main, y_train, y_test = train_test_split(X,y,test_size=0.2,random_state=4)\n",
    "    return X_train_main, X_test_main, y_train, y_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 313,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "# Function that chooses exploration or explotation method\n",
    "def exploration_explotation(epsilon):\n",
    "    global exploration \n",
    "    if np.random.rand() < epsilon:  \n",
    "        exploration=1\n",
    "    else:\n",
    "        exploration=0    \n",
    "    return exploration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 314,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "# Function that returns all available actions in the state given as an argument: \n",
    "def available_actions(number_of_columns,columns,initial_state,current_state,trashold, exploration):\n",
    "    global exclude\n",
    "    global all_columns\n",
    "#    exclude=[]\n",
    "    all_columns=np.arange(number_of_columns+1)\n",
    "    # remove columns that have been already selected\n",
    "    exclude=columns.copy()\n",
    "    # remove the initial_state and the current_state\n",
    "    exclude.extend([initial_state, current_state])\n",
    "    available_act = list(set(all_columns)-set(exclude))\n",
    "    # remove actions that have negetiv Q value\n",
    "    if exploration==0:\n",
    "        index = np.where(Q[current_state,available_act] > trashold)[1]\n",
    "        available_act= [available_act[i] for i in index.tolist()]\n",
    "    return available_act"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 315,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "def sample_next_action(current_state, Q, available_act, exploration):\n",
    "    global available_act_q_value\n",
    "    available_act_q_value = [float(q) for q in np.array(Q[current_state, available_act]).reshape(-1)]\n",
    "    \n",
    "    if exploration == 1: \n",
    "        # Random selection\n",
    "        next_action = int(np.random.choice(available_act, 1).item())\n",
    "    else: \n",
    "        # Greedy selection according to max value\n",
    "        maxQ = max(available_act_q_value)\n",
    "        count = available_act_q_value.count(maxQ)\n",
    "        \n",
    "        if count > 1:\n",
    "            max_columns = [i for i in range(len(available_act_q_value)) if available_act_q_value[i] == maxQ]\n",
    "            i = int(np.random.choice(max_columns, 1).item())\n",
    "        else:\n",
    "            i = available_act_q_value.index(maxQ)\n",
    "        \n",
    "        next_action = available_act[i]  \n",
    "    \n",
    "    return next_action\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 316,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "# function that update a list with all selected columns in the episode\n",
    "def update_columns(action, columns):\n",
    "    update_columns=columns\n",
    "    update_columns.append(action)\n",
    "    return update_columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 317,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "# function that update the X_train and X_test according to the current episode columns list \n",
    "def update_X_train_X_test(columns,X_train_main, X_test_main):\n",
    "    X_train=X_train_main.iloc[:,columns]\n",
    "    X_test=X_test_main.iloc[:,columns]\n",
    "    X_train=pd.DataFrame(X_train)\n",
    "    X_test=pd.DataFrame(X_test)\n",
    "    return X_train, X_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 318,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "# Function that run the learner and get the error to the current episode columns list\n",
    "def Learner(X_train, X_test,y_train, y_test):\n",
    "    global learner\n",
    "    global y_pred\n",
    "    if learner_model == 'DT':\n",
    "        learner = tree.DecisionTreeClassifier()\n",
    "        learner = learner.fit(X_train, y_train)\n",
    "        y_pred = learner.predict(X_test)\n",
    "    elif learner_model == 'KNN':\n",
    "        learner = KNeighborsClassifier(metric='hamming',n_neighbors=5)\n",
    "        learner = learner.fit(X_train, y_train)\n",
    "        y_pred = learner.predict(X_test)        \n",
    "    elif learner_model == 'SVM':\n",
    "        learner = SVC()\n",
    "        learner = learner.fit(X_train, y_train)\n",
    "        y_pred = learner.predict(X_test)        \n",
    "    elif learner_model == 'NB':\n",
    "        learner = MultinomialNB()\n",
    "        learner = learner.fit(X_train, y_train)\n",
    "        y_pred = learner.predict(X_test)\n",
    "    elif learner_model == 'AB':\n",
    "        learner = AdaBoostClassifier()\n",
    "        learner = learner.fit(X_train, y_train)\n",
    "        y_pred = learner.predict(X_test)  \n",
    "    elif learner_model == 'GB':\n",
    "        learner = GradientBoostingClassifier()\n",
    "        learner = learner.fit(X_train, y_train)\n",
    "        y_pred = learner.predict(X_test)  \n",
    "    elif learner_model == 'VQC':\n",
    "        learner = QuantumLearner()\n",
    "        learner = learner.fit(X_train, y_train)\n",
    "        y_pred = learner.predict(X_test)  \n",
    "    elif learner_model == 'ANN':\n",
    "        learner = ClassicalLearner()\n",
    "        learner = learner.fit(X_train, y_train)\n",
    "        y_pred = learner.predict(X_test)  \n",
    "    accuracy=metrics.accuracy_score(y_test, y_pred)\n",
    "    error=1-accuracy\n",
    "    return error\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 319,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "def q_update(current_state, action, learning_rate, reward):\n",
    "    # next_state = current action\n",
    "    max_index = np.where(Q[action,] == np.max(Q[action,]))[0]  # Use [0] instead of [1] for 1D arrays\n",
    "    \n",
    "    if max_index.shape[0] > 1:\n",
    "        # Resolve tie by selecting one randomly\n",
    "        max_index = int(np.random.choice(max_index, size=1).item())\n",
    "    else:\n",
    "        max_index = int(max_index[0])  # Convert the first element to a scalar\n",
    "\n",
    "    max_value = Q[action, max_index]\n",
    "\n",
    "    # Update the Q matrix\n",
    "    if Q[current_state, action] == 1:\n",
    "        Q[current_state, action] = learning_rate * reward\n",
    "    else:\n",
    "        Q[current_state, action] = Q[current_state, action] + learning_rate * (\n",
    "            reward + (discount_factor * max_value) - Q[current_state, action]\n",
    "        )\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Experiment mangment"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "#### 3. Define the parameters "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 320,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "## for run time ##\n",
    "N_features=5\n",
    "N_data=1\n",
    "## for run time ##\n",
    "\n",
    "#Experiment: \n",
    "experiment='test'\n",
    "number_of_experiment=1\n",
    "\n",
    "# Dataset parameters #\n",
    "location = 'Datasets/adult'\n",
    "outputlocation='Datasets'\n",
    "file='adult' #adult #diabetic_data #no_show\n",
    "#np.random.seed(3)\n",
    "\n",
    "# Q learning parameter # \n",
    "learning_rate=0.005\n",
    "discount_factor = 0.01 #0\n",
    "epsilon = 0.1\n",
    "\n",
    "# Learner and episode parameters #\n",
    "learner_model = 'VQC' #DT #KNN #SVM\n",
    "episode_size=100\n",
    "internal_trashold=0\n",
    "external_trashold=0\n",
    "filename= file +'_int.csv'\n",
    "\n",
    "#Experiments folder management: \n",
    "#if not os.path.exists('/Experiments'):\n",
    "#    os.makedirs('/Experiments') \n",
    "if not os.path.exists('Experiments/'+ str(experiment)):\n",
    "    os.makedirs('Experiments/'+ str(experiment))\n",
    "else:\n",
    "    shutil.rmtree('Experiments/'+ str(experiment))          #removes all the subdirectories!\n",
    "    os.makedirs('Experiments/'+ str(experiment))\n",
    "#writer = pd.ExcelWriter('Experiments/'+ str(experiment) + '/df.xlsx') \n",
    "\n",
    "\n",
    "\n",
    "text_file = open('Experiments/'+ str(experiment) +'/parameters.txt', \"w\")\n",
    "text_file.write('experiment: ' + str(experiment)+ '\\n')\n",
    "text_file.write('number of experiments: ' + str(number_of_experiment)+ '\\n')\n",
    "text_file.write('file: ' + str(file)+ '\\n')\n",
    "text_file.write('learner model: ' + str(learner_model)+ '\\n')\n",
    "text_file.write('episode size: ' + str(episode_size)+ '\\n')\n",
    "#text_file.write('numbers of epocs: ' + str(epocs)+ '\\n')\n",
    "text_file.write('internal trashold: ' + str(internal_trashold)+ '\\n')\n",
    "text_file.write('external trashold: ' + str(external_trashold)+ '\\n')\n",
    " \n",
    "text_file.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "#### 4. Run all experiments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 321,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Experiments 0 start\n",
      "number of columns: 5 (exclude class column)\n",
      "Number of episodes: 2443.0\n",
      "initial state number: 5 (the last dummy column we have created)\n",
      "episode 1 start\n",
      "episode columns: [1, np.int64(3), 0, 4, 2] epsilon: 0.9 learning rate: 0.09 error: 0.30000000000000004\n",
      "episode policy:[np.int64(3), np.int64(1), np.int64(2), np.int64(0)] train accuracy: 0.7887842816209578 test accuracy: 0.7016574585635359\n",
      "episode 1 end\n",
      "episode 2 start\n",
      "episode columns: [0, 2, 4, 3, 1] epsilon: 0.9 learning rate: 0.09 error: 0.30000000000000004\n",
      "episode policy:[np.int64(3), np.int64(2), np.int64(1), np.int64(0)] train accuracy: 0.7902169463774048 test accuracy: 0.6887661141804788\n",
      "episode 2 end\n",
      "episode 3 start\n",
      "episode columns: [2, 3, 0, 4, 1] epsilon: 0.9 learning rate: 0.09 error: 0.25\n",
      "episode policy:[np.int64(4), np.int64(0), np.int64(1), np.int64(2)] train accuracy: 0.804952926729431 test accuracy: 0.7366482504604052\n",
      "episode 3 end\n",
      "episode 4 start\n",
      "episode columns: [3, np.int64(2), 0, 4, 1] epsilon: 0.9 learning rate: 0.09 error: 0.0\n",
      "episode policy:[np.int64(4), np.int64(0), np.int64(3), np.int64(1), np.int64(2)] train accuracy: 0.7939009414654113 test accuracy: 0.7532228360957642\n",
      "episode 4 end\n",
      "episode 5 start\n",
      "episode columns: [1, 3, 2, 4, np.int64(0)] epsilon: 0.9 learning rate: 0.09 error: 0.25\n",
      "episode policy:[np.int64(4), np.int64(1), np.int64(2)] train accuracy: 0.7971756037658616 test accuracy: 0.7532228360957642\n",
      "episode 5 end\n",
      "episode 6 start\n",
      "episode columns: [3, 0, 4, 1, np.int64(2)] epsilon: 0.9 learning rate: 0.09 error: 0.25\n",
      "episode policy:[np.int64(4), np.int64(1), np.int64(0), np.int64(3)] train accuracy: 0.6923864101514531 test accuracy: 0.7532228360957642\n",
      "episode 6 end\n",
      "episode 7 start\n",
      "episode columns: [2, 4, 1, 0, 3] epsilon: 0.9 learning rate: 0.09 error: 0.25\n",
      "episode policy:[np.int64(4), np.int64(1), np.int64(3), np.int64(0)] train accuracy: 0.7758902988129349 test accuracy: 0.7311233885819521\n",
      "episode 7 end\n",
      "episode 8 start\n",
      "episode columns: [1, 2, 4, 0, 3] epsilon: 0.9 learning rate: 0.09 error: 0.44999999999999996\n",
      "episode policy:[np.int64(4), np.int64(1), np.int64(3), np.int64(0)] train accuracy: 0.7871469504707327 test accuracy: 0.7532228360957642\n",
      "episode 8 end\n",
      "episode 9 start\n",
      "episode columns: [1, 4, 3, 0, 2] epsilon: 0.9 learning rate: 0.09 error: 0.15000000000000002\n",
      "episode policy:[np.int64(4), np.int64(1), np.int64(3), np.int64(0)] train accuracy: 0.7496930004093327 test accuracy: 0.7532228360957642\n",
      "episode 9 end\n",
      "episode 10 start\n",
      "episode columns: [2, 1, 3, 4, 0] epsilon: 0.9 learning rate: 0.09 error: 0.25\n",
      "episode policy:[np.int64(4), np.int64(1), np.int64(3), np.int64(0)] train accuracy: 0.7808022922636103 test accuracy: 0.7532228360957642\n",
      "episode 10 end\n",
      "episode 11 start\n",
      "episode columns: [0, 1, np.int64(3), 2, 4] epsilon: 0.9 learning rate: 0.09 error: 0.19999999999999996\n",
      "episode policy:[np.int64(4), np.int64(1), np.int64(3), np.int64(0)] train accuracy: 0.7771182971756038 test accuracy: 0.716390423572744\n",
      "episode 11 end\n",
      "episode 12 start\n",
      "episode columns: [1, 0, 3, 2, 4] epsilon: 0.9 learning rate: 0.09 error: 0.30000000000000004\n",
      "episode policy:[np.int64(4), np.int64(1), np.int64(3), np.int64(2)] train accuracy: 0.770159639787147 test accuracy: 0.7532228360957642\n",
      "episode 12 end\n",
      "episode 13 start\n",
      "episode columns: [3, 1, 0, 4, 2] epsilon: 0.9 learning rate: 0.09 error: 0.050000000000000044\n",
      "episode policy:[np.int64(4), np.int64(1), np.int64(3), np.int64(2)] train accuracy: 0.7844862873516169 test accuracy: 0.7384898710865562\n",
      "episode 13 end\n",
      "episode 14 start\n",
      "episode columns: [0, 2, 1, 3, 4] epsilon: 0.9 learning rate: 0.09 error: 0.15000000000000002\n",
      "episode policy:[np.int64(4), np.int64(1)] train accuracy: 0.7891936144085141 test accuracy: 0.7532228360957642\n",
      "episode 14 end\n",
      "episode 15 start\n",
      "episode columns: [3, 4, 1, 2, 0] epsilon: 0.9 learning rate: 0.09 error: 0.19999999999999996\n",
      "episode policy:[np.int64(4), np.int64(1)] train accuracy: 0.7908309455587392 test accuracy: 0.7532228360957642\n",
      "episode 15 end\n",
      "episode 16 start\n",
      "episode columns: [0, 1, 4, 3, 2] epsilon: 0.9 learning rate: 0.09 error: 0.15000000000000002\n",
      "episode policy:[np.int64(4), np.int64(1)] train accuracy: 0.780392959476054 test accuracy: 0.7532228360957642\n",
      "episode 16 end\n",
      "episode 17 start\n",
      "episode columns: [np.int64(4), 3, 0, 1, 2] epsilon: 0.9 learning rate: 0.09 error: 0.19999999999999996\n",
      "episode policy:[np.int64(1)] train accuracy: 0.7853049529267294 test accuracy: 0.7532228360957642\n",
      "episode 17 end\n",
      "episode 18 start\n",
      "episode columns: [4, 2, 1, 3, 0] epsilon: 0.9 learning rate: 0.09 error: 0.15000000000000002\n",
      "episode policy:[np.int64(1)] train accuracy: 0.7842816209578387 test accuracy: 0.7532228360957642\n",
      "episode 18 end\n",
      "episode 19 start\n",
      "episode columns: [1, 4, 3, 2, 0] epsilon: 0.9 learning rate: 0.09 error: 0.15000000000000002\n",
      "episode policy:[np.int64(1)] train accuracy: 0.7777322963569382 test accuracy: 0.7532228360957642\n",
      "episode 19 end\n",
      "episode 20 start\n",
      "episode columns: [4, 1, 0, 3, 2] epsilon: 0.9 learning rate: 0.09 error: 0.30000000000000004\n",
      "episode policy:[np.int64(1)] train accuracy: 0.7851002865329513 test accuracy: 0.7532228360957642\n",
      "episode 20 end\n",
      "episode 21 start\n",
      "episode columns: [np.int64(1), 2, 3, 0, 4] epsilon: 0.9 learning rate: 0.09 error: 0.15000000000000002\n",
      "episode policy:[np.int64(1)] train accuracy: 0.7865329512893983 test accuracy: 0.7532228360957642\n",
      "episode 21 end\n",
      "episode 22 start\n",
      "episode columns: [2, 3, 0, np.int64(1), 4] epsilon: 0.9 learning rate: 0.09 error: 0.44999999999999996\n",
      "episode policy:[np.int64(1)] train accuracy: 0.7857142857142857 test accuracy: 0.7532228360957642\n",
      "episode 22 end\n",
      "episode 23 start\n",
      "episode columns: [4, 1, 2, 3, 0] epsilon: 0.9 learning rate: 0.09 error: 0.050000000000000044\n",
      "episode policy:[np.int64(1)] train accuracy: 0.7812116250511666 test accuracy: 0.7532228360957642\n",
      "episode 23 end\n",
      "episode 24 start\n",
      "episode columns: [3, 1, 4, 0] epsilon: 0.9 learning rate: 0.09 error: 0.30000000000000004\n",
      "episode policy:[np.int64(1), np.int64(4), np.int64(2), np.int64(3), np.int64(0)] train accuracy: 0.7871469504707327 test accuracy: 0.7532228360957642\n",
      "episode 24 end\n",
      "episode 25 start\n",
      "episode columns: [1, np.int64(4), 0] epsilon: 0.9 learning rate: 0.09 error: 0.25\n",
      "episode policy:[np.int64(1), np.int64(4), np.int64(2), np.int64(3), np.int64(0)] train accuracy: 0.7787556283258289 test accuracy: 0.7532228360957642\n",
      "episode 25 end\n",
      "episode 26 start\n",
      "episode columns: [2, np.int64(3), 0, 1, 4] epsilon: 0.9 learning rate: 0.09 error: 0.15000000000000002\n",
      "episode policy:[np.int64(1), np.int64(4), np.int64(2), np.int64(3), np.int64(0)] train accuracy: 0.7828489562013917 test accuracy: 0.7440147329650092\n",
      "episode 26 end\n",
      "episode 27 start\n",
      "episode columns: [1, 3, 0, 4, 2] epsilon: 0.9 learning rate: 0.09 error: 0.19999999999999996\n",
      "episode policy:[np.int64(1), np.int64(4), np.int64(2), np.int64(3)] train accuracy: 0.7752762996316005 test accuracy: 0.7532228360957642\n",
      "episode 27 end\n",
      "episode 28 start\n",
      "episode columns: [0, 1, 4, 2, 3] epsilon: 0.9 learning rate: 0.09 error: 0.15000000000000002\n",
      "episode policy:[np.int64(1), np.int64(4), np.int64(2), np.int64(3)] train accuracy: 0.7492836676217765 test accuracy: 0.7532228360957642\n",
      "episode 28 end\n",
      "episode 29 start\n",
      "episode columns: [3, 1, 0, 4, np.int64(2)] epsilon: 0.9 learning rate: 0.09 error: 0.25\n",
      "episode policy:[np.int64(1), np.int64(0)] train accuracy: 0.7808022922636103 test accuracy: 0.7532228360957642\n",
      "episode 29 end\n",
      "episode 30 start\n",
      "episode columns: [0, 4, np.int64(1), 3, 2] epsilon: 0.9 learning rate: 0.09 error: 0.25\n",
      "episode policy:[np.int64(1), np.int64(0)] train accuracy: 0.7867376176831764 test accuracy: 0.7476979742173112\n",
      "episode 30 end\n",
      "episode 31 start\n",
      "episode columns: [3, 2, 1, 4] epsilon: 0.9 learning rate: 0.09 error: 0.25\n",
      "episode policy:[np.int64(1), np.int64(0)] train accuracy: 0.7873516168645108 test accuracy: 0.7532228360957642\n",
      "episode 31 end\n",
      "episode 32 start\n",
      "episode columns: [3, 2, 1, 4, 0] epsilon: 0.9 learning rate: 0.09 error: 0.30000000000000004\n",
      "episode policy:[np.int64(1), np.int64(4), np.int64(2), np.int64(3)] train accuracy: 0.7926729431027425 test accuracy: 0.7311233885819521\n",
      "episode 32 end\n",
      "episode 33 start\n",
      "episode columns: [3, 4, 0, 1, 2] epsilon: 0.9 learning rate: 0.09 error: 0.30000000000000004\n",
      "episode policy:[np.int64(1), np.int64(4), np.int64(2), np.int64(3)] train accuracy: 0.7994269340974212 test accuracy: 0.7071823204419889\n",
      "episode 33 end\n",
      "episode 34 start\n",
      "episode columns: [4, 0, np.int64(1)] epsilon: 0.9 learning rate: 0.09 error: 0.25\n",
      "episode policy:[np.int64(1), np.int64(4), np.int64(2), np.int64(3)] train accuracy: 0.7918542775276299 test accuracy: 0.7532228360957642\n",
      "episode 34 end\n",
      "episode 35 start\n",
      "episode columns: [3, 2, 1, 0, 4] epsilon: 0.9 learning rate: 0.09 error: 0.19999999999999996\n",
      "episode policy:[np.int64(1), np.int64(4), np.int64(2), np.int64(3)] train accuracy: 0.7810069586573885 test accuracy: 0.7532228360957642\n",
      "episode 35 end\n",
      "episode 36 start\n",
      "episode columns: [4, 2, 0, 3, 1] epsilon: 0.9 learning rate: 0.09 error: 0.30000000000000004\n",
      "episode policy:[np.int64(1), np.int64(4), np.int64(2), np.int64(3)] train accuracy: 0.7941056078591895 test accuracy: 0.7532228360957642\n",
      "episode 36 end\n",
      "episode 37 start\n",
      "episode columns: [0, 2, 3, 1, 4] epsilon: 0.9 learning rate: 0.09 error: 0.09999999999999998\n",
      "episode policy:[np.int64(1), np.int64(0), np.int64(2), np.int64(3)] train accuracy: 0.7906262791649611 test accuracy: 0.7532228360957642\n",
      "episode 37 end\n",
      "episode 38 start\n",
      "episode columns: [np.int64(1), 3, 4, 2, 0] epsilon: 0.9 learning rate: 0.09 error: 0.15000000000000002\n",
      "episode policy:[np.int64(1), np.int64(0), np.int64(2), np.int64(3)] train accuracy: 0.787556283258289 test accuracy: 0.7532228360957642\n",
      "episode 38 end\n",
      "episode 39 start\n",
      "episode columns: [np.int64(1), np.int64(0), 4, 3, 2] epsilon: 0.9 learning rate: 0.09 error: 0.19999999999999996\n",
      "episode policy:[np.int64(1), np.int64(0), np.int64(2), np.int64(3)] train accuracy: 0.7953336062218583 test accuracy: 0.7532228360957642\n",
      "episode 39 end\n",
      "episode 40 start\n",
      "episode columns: [2, 4, 3, np.int64(1), 0] epsilon: 0.9 learning rate: 0.09 error: 0.15000000000000002\n",
      "episode policy:[np.int64(1), np.int64(0), np.int64(2), np.int64(3)] train accuracy: 0.7887842816209578 test accuracy: 0.7458563535911602\n",
      "episode 40 end\n",
      "episode 41 start\n",
      "episode columns: [3, 1, 4, np.int64(2), 0] epsilon: 0.9 learning rate: 0.09 error: 0.19999999999999996\n",
      "episode policy:[np.int64(1), np.int64(0), np.int64(2), np.int64(3)] train accuracy: 0.7893982808022922 test accuracy: 0.7329650092081031\n",
      "episode 41 end\n",
      "episode 42 start\n",
      "episode columns: [4, 1, 2, np.int64(3), 0] epsilon: 0.9 learning rate: 0.09 error: 0.15000000000000002\n",
      "episode policy:[np.int64(1), np.int64(0), np.int64(2), np.int64(3)] train accuracy: 0.7951289398280802 test accuracy: 0.7532228360957642\n",
      "episode 42 end\n",
      "episode 43 start\n",
      "episode columns: [0, 3, 4, 2, 1] epsilon: 0.9 learning rate: 0.09 error: 0.15000000000000002\n",
      "episode policy:[np.int64(1), np.int64(0), np.int64(2), np.int64(3)] train accuracy: 0.7887842816209578 test accuracy: 0.7532228360957642\n",
      "episode 43 end\n",
      "episode 44 start\n",
      "episode columns: [2, 1, 3, 0, 4] epsilon: 0.9 learning rate: 0.09 error: 0.30000000000000004\n",
      "episode policy:[np.int64(1), np.int64(0), np.int64(4), np.int64(2), np.int64(3)] train accuracy: 0.7971756037658616 test accuracy: 0.7532228360957642\n",
      "episode 44 end\n",
      "episode 45 start\n",
      "episode columns: [0, 1, 2, 3, 4] epsilon: 0.9 learning rate: 0.09 error: 0.15000000000000002\n",
      "episode policy:[np.int64(1), np.int64(0), np.int64(4), np.int64(2), np.int64(3)] train accuracy: 0.7961522717969709 test accuracy: 0.7182320441988951\n",
      "episode 45 end\n",
      "episode 46 start\n",
      "episode columns: [np.int64(1), 3, 4, 0, 2] epsilon: 0.9 learning rate: 0.09 error: 0.30000000000000004\n",
      "episode policy:[np.int64(1), np.int64(0), np.int64(2), np.int64(3)] train accuracy: 0.7881702824396234 test accuracy: 0.7476979742173112\n",
      "episode 46 end\n",
      "episode 47 start\n",
      "episode columns: [1, 0, np.int64(2), 4, 3] epsilon: 0.9 learning rate: 0.09 error: 0.19999999999999996\n",
      "episode policy:[np.int64(1), np.int64(0), np.int64(4), np.int64(2), np.int64(3)] train accuracy: 0.7515349979533361 test accuracy: 0.7311233885819521\n",
      "episode 47 end\n",
      "episode 48 start\n",
      "episode columns: [4, np.int64(1), 0, 3, 2] epsilon: 0.9 learning rate: 0.09 error: 0.15000000000000002\n",
      "episode policy:[np.int64(1), np.int64(0), np.int64(4), np.int64(2), np.int64(3)] train accuracy: 0.7902169463774048 test accuracy: 0.7532228360957642\n",
      "episode 48 end\n",
      "episode 49 start\n",
      "episode columns: [1, 3, 0, 4, 2] epsilon: 0.9 learning rate: 0.09 error: 0.25\n",
      "episode policy:[np.int64(1), np.int64(0), np.int64(4), np.int64(2), np.int64(3)] train accuracy: 0.7869422840769545 test accuracy: 0.7532228360957642\n",
      "episode 49 end\n",
      "episode 50 start\n",
      "episode columns: [4, 0, 1, 3, 2] epsilon: 0.9 learning rate: 0.09 error: 0.19999999999999996\n",
      "episode policy:[np.int64(1), np.int64(0), np.int64(4), np.int64(2), np.int64(3)] train accuracy: 0.8067949242734344 test accuracy: 0.7366482504604052\n",
      "episode 50 end\n",
      "episode 51 start\n",
      "episode columns: [2, 3, 4, 1, 0] epsilon: 0.9 learning rate: 0.09 error: 0.30000000000000004\n",
      "episode policy:[np.int64(1), np.int64(0), np.int64(4), np.int64(2), np.int64(3)] train accuracy: 0.8029062627916496 test accuracy: 0.7532228360957642\n",
      "episode 51 end\n",
      "episode 52 start\n",
      "episode columns: [3, 2, 0, 1, 4] epsilon: 0.9 learning rate: 0.09 error: 0.15000000000000002\n",
      "episode policy:[np.int64(1), np.int64(0), np.int64(4), np.int64(2), np.int64(3)] train accuracy: 0.7957429390094146 test accuracy: 0.7034990791896869\n",
      "episode 52 end\n",
      "episode 53 start\n",
      "episode columns: [1, np.int64(0), 2, 4] epsilon: 0.9 learning rate: 0.09 error: 0.09999999999999998\n",
      "episode policy:[np.int64(1), np.int64(0), np.int64(2), np.int64(3)] train accuracy: 0.8010642652476463 test accuracy: 0.7403314917127072\n",
      "episode 53 end\n",
      "episode 54 start\n",
      "episode columns: [0, 3, 2, 1, 4] epsilon: 0.9 learning rate: 0.09 error: 0.09999999999999998\n",
      "episode policy:[np.int64(1), np.int64(0), np.int64(2), np.int64(3)] train accuracy: 0.7871469504707327 test accuracy: 0.712707182320442\n",
      "episode 54 end\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[321], line 83\u001b[0m\n\u001b[1;32m     81\u001b[0m X_train_episode, X_test_episode \u001b[38;5;241m=\u001b[39mupdate_X_train_X_test(episode_columns,X_train_main_episode, X_test_main_episode)\n\u001b[1;32m     82\u001b[0m \u001b[38;5;66;03m# Update the accuracy of the current columns\u001b[39;00m\n\u001b[0;32m---> 83\u001b[0m episode_error\u001b[38;5;241m=\u001b[39m \u001b[43mLearner\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX_train_episode\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mX_test_episode\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my_train_episode\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my_test_episode\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     84\u001b[0m \u001b[38;5;66;03m# Update reward\u001b[39;00m\n\u001b[1;32m     85\u001b[0m episode_reward\u001b[38;5;241m=\u001b[39mepisode_last_error\u001b[38;5;241m-\u001b[39mepisode_error\n",
      "Cell \u001b[0;32mIn[318], line 35\u001b[0m, in \u001b[0;36mLearner\u001b[0;34m(X_train, X_test, y_train, y_test)\u001b[0m\n\u001b[1;32m     33\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m learner_model \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mANN\u001b[39m\u001b[38;5;124m'\u001b[39m:\n\u001b[1;32m     34\u001b[0m     learner \u001b[38;5;241m=\u001b[39m ClassicalLearner()\n\u001b[0;32m---> 35\u001b[0m     learner \u001b[38;5;241m=\u001b[39m \u001b[43mlearner\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfit\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX_train\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my_train\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     36\u001b[0m     y_pred \u001b[38;5;241m=\u001b[39m learner\u001b[38;5;241m.\u001b[39mpredict(X_test)  \n\u001b[1;32m     37\u001b[0m accuracy\u001b[38;5;241m=\u001b[39mmetrics\u001b[38;5;241m.\u001b[39maccuracy_score(y_test, y_pred)\n",
      "Cell \u001b[0;32mIn[282], line 26\u001b[0m, in \u001b[0;36mClassicalLearner.fit\u001b[0;34m(self, X_train, y_train, num_it, lr)\u001b[0m\n\u001b[1;32m     24\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m epoch \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(num_it):\n\u001b[1;32m     25\u001b[0m     optimizer\u001b[38;5;241m.\u001b[39mzero_grad()\n\u001b[0;32m---> 26\u001b[0m     y_pred \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mforward\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtensor\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX_train\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mvalues\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdtype\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfloat64\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241m.\u001b[39mreshape(\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m, \u001b[38;5;241m1\u001b[39m)\n\u001b[1;32m     27\u001b[0m     loss \u001b[38;5;241m=\u001b[39m nn\u001b[38;5;241m.\u001b[39mBCELoss()(y_pred, y_train)\n\u001b[1;32m     28\u001b[0m     loss\u001b[38;5;241m.\u001b[39mbackward()\n",
      "Cell \u001b[0;32mIn[282], line 14\u001b[0m, in \u001b[0;36mClassicalLearner.forward\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m     12\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, x):\n\u001b[1;32m     13\u001b[0m     x \u001b[38;5;241m=\u001b[39m x\u001b[38;5;241m.\u001b[39mto(torch\u001b[38;5;241m.\u001b[39mfloat64)\n\u001b[0;32m---> 14\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m layer \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mlayers\u001b[49m\u001b[43m[\u001b[49m\u001b[43m:\u001b[49m\u001b[38;5;241;43m-\u001b[39;49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m]\u001b[49m:\n\u001b[1;32m     15\u001b[0m         x \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mrelu(layer(x))\n\u001b[1;32m     16\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39msigmoid(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlayers[\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m](x))\n",
      "File \u001b[0;32m~/.venv/lib/python3.13/site-packages/torch/nn/modules/container.py:332\u001b[0m, in \u001b[0;36mModuleList.__getitem__\u001b[0;34m(self, idx)\u001b[0m\n\u001b[1;32m    329\u001b[0m \u001b[38;5;129m@_copy_to_script_wrapper\u001b[39m\n\u001b[1;32m    330\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21m__getitem__\u001b[39m(\u001b[38;5;28mself\u001b[39m, idx: Union[\u001b[38;5;28mint\u001b[39m, \u001b[38;5;28mslice\u001b[39m]) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Union[Module, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mModuleList\u001b[39m\u001b[38;5;124m\"\u001b[39m]:\n\u001b[1;32m    331\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(idx, \u001b[38;5;28mslice\u001b[39m):\n\u001b[0;32m--> 332\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[38;5;18;43m__class__\u001b[39;49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mlist\u001b[39;49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_modules\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mvalues\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\u001b[43m[\u001b[49m\u001b[43midx\u001b[49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    333\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    334\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_modules[\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_get_abs_string_index(idx)]\n",
      "File \u001b[0;32m~/.venv/lib/python3.13/site-packages/torch/nn/modules/container.py:308\u001b[0m, in \u001b[0;36mModuleList.__init__\u001b[0;34m(self, modules)\u001b[0m\n\u001b[1;32m    307\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21m__init__\u001b[39m(\u001b[38;5;28mself\u001b[39m, modules: Optional[Iterable[Module]] \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m--> 308\u001b[0m     \u001b[38;5;28;43msuper\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[38;5;21;43m__init__\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    309\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m modules \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    310\u001b[0m         \u001b[38;5;28mself\u001b[39m \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m modules\n",
      "File \u001b[0;32m~/.venv/lib/python3.13/site-packages/torch/nn/modules/module.py:478\u001b[0m, in \u001b[0;36mModule.__init__\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m    476\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21m__init__\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    477\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Initialize internal Module state, shared by both nn.Module and ScriptModule.\"\"\"\u001b[39;00m\n\u001b[0;32m--> 478\u001b[0m     \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_C\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_log_api_usage_once\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mpython.nn_module\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m    480\u001b[0m     \u001b[38;5;66;03m# Backward compatibility: no args used to be allowed when call_super_init=False\u001b[39;00m\n\u001b[1;32m    481\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcall_super_init \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mFalse\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mbool\u001b[39m(kwargs):\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "for e in range (number_of_experiment):\n",
    "    if not os.path.exists('Experiments/'+ str(experiment)+ '/'+ str(e)):\n",
    "        os.makedirs('Experiments/'+ str(experiment)+ '/'+ str(e))\n",
    "    else:\n",
    "        shutil.rmtree('Experiments/'+ str(experiment)+ '/'+ str(e))          #removes all the subdirectories!\n",
    "        os.makedirs('Experiments/'+ str(experiment)+ '/'+ str(e))\n",
    "    print ('Experiments ' + str(e) + ' start')\n",
    "##########################Experiment setup##########################\n",
    "    # Read the data\n",
    "    data = pd.read_csv(location + '/' + filename, index_col=0)\n",
    "    \n",
    "##### for run time - start #####\n",
    "    import timeit\n",
    "    start = timeit.default_timer()\n",
    "    size= int(N_data* len(data.index))\n",
    "    data = data.sample(n=size)\n",
    "    data=data.iloc[:,-N_features-1:]\n",
    "##### for run time - end #####\n",
    "    \n",
    "    #Set the number of iterations:\n",
    "    interations=10*len(data.index)/episode_size\n",
    "    # Set the number of columns exclude the class column\n",
    "    number_of_columns=data.shape[1]-1 \n",
    "    print (\"number of columns: \"+ str(number_of_columns) +\" (exclude class column)\" ) \n",
    "    # Set the number of episodes \n",
    "    # episodes_number=epocs*len(data.index)/episode_size\n",
    "    episodes_number=interations\n",
    "    print (\"Number of episodes: \"+ str(episodes_number) ) \n",
    "    # Initialize matrix Q as a 1 values matrix:\n",
    "    #Q = np.matrix(np.ones([number_of_columns+1,number_of_columns+1])) # we will use the last dummy columns as initial state s\n",
    "    Q = np.matrix(np.ones([number_of_columns+1,number_of_columns+1])) # we will use the last dummy columns as initial state s\n",
    "    # Set initial_state to be the last dummy column we have created\n",
    "    initial_state=number_of_columns\n",
    "    # define data frame to save episode policies results\n",
    "    df = pd.DataFrame(columns=('episode','episode_columns','policy_columns','policy_accuracy_train','policy_accuracy_test'))\n",
    "    print (\"initial state number: \"+ str(initial_state) + \" (the last dummy column we have created)\") \n",
    "\n",
    "    ##########################  episode  ##########################  \n",
    "    for i in range (int(episodes_number)):\n",
    "    ########## Begining of episode  ############\n",
    "        # Initiate lists for available_act, episode_columns and and the policy mode & episode_error\n",
    "        episode_available_act=list(np.arange(number_of_columns))\n",
    "        episode_columns=[]\n",
    "        policy=0\n",
    "        episode_error=0\n",
    "        # Initiate the error to 0.5\n",
    "        episode_last_error=0.5\n",
    "        # Initiate current_state to be initial_state\n",
    "        episode_current_state=initial_state\n",
    "        # Create the episode data \n",
    "        episode= get_data(episode_size, policy=0, mode='train')\n",
    "        # Separate the episode data into features and label\n",
    "        X_episode,y_episode=data_separate(episode)\n",
    "        # Split the data into train and test \n",
    "        X_train_main_episode, X_test_main_episode, y_train_episode, y_test_episode = data_split(X_episode,y_episode)\n",
    "        if i<episodes_number*0.25:\n",
    "            epsilon=0.9\n",
    "            learning_rate=0.09\n",
    "        elif i<episodes_number*0.5:\n",
    "            epsilon=0.5\n",
    "            learning_rate=0.05\n",
    "        elif i<episodes_number*0.75:\n",
    "            epsilon=0.3\n",
    "            learning_rate=0.01\n",
    "        else:\n",
    "            epsilon=0.1\n",
    "            learning_rate=0.005\n",
    "        ########## Q learning start ############\n",
    "\n",
    "        while len(episode_available_act)>0:\n",
    "            # Get exploration or explotation flag \n",
    "            exploration=exploration_explotation(epsilon)\n",
    "            # Get available actions in the current state\n",
    "            episode_available_act = available_actions(number_of_columns,episode_columns,initial_state,episode_current_state,internal_trashold,exploration)\n",
    "            if len(episode_available_act)>0:\n",
    "                # Sample next action to be performed\n",
    "                episode_action = sample_next_action(episode_current_state, Q, episode_available_act, exploration)\n",
    "                # Update the episode_columns\n",
    "                episode_columns=update_columns(episode_action,episode_columns)\n",
    "                # Update the dataset to include all episode columns + current selected action (column)\n",
    "                X_train_episode, X_test_episode =update_X_train_X_test(episode_columns,X_train_main_episode, X_test_main_episode)\n",
    "                # Update the accuracy of the current columns\n",
    "                episode_error= Learner(X_train_episode, X_test_episode, y_train_episode, y_test_episode)\n",
    "                # Update reward\n",
    "                episode_reward=episode_last_error-episode_error\n",
    "                # Update Q matrix\n",
    "                q_update(episode_current_state,episode_action,learning_rate, episode_reward)\n",
    "                # Update parameters for next round \n",
    "#                 if episode_current_state==initial_state:\n",
    "#                     beta=abs(episode_reward-Q[episode_current_state,episode_action])\n",
    "#                     epsilon=final_epsilon+(beta*(1-final_epsilon))\n",
    "                #    learning_rate=final_learning_rate+(beta*(1-final_learning_rate))\n",
    "                episode_current_state=episode_action\n",
    "                episode_last_error=episode_error\n",
    "                 \n",
    "        ########## Q learning End ############\n",
    "\n",
    "        #Save Q matrix: \n",
    "        if (i%100 ==0):\n",
    "            Q_save=pd.DataFrame(Q)\n",
    "            Q_save.to_csv('Experiments/'+ str(experiment)+ '/'+ str(e)+ '/Q.'+ str(i+1) + '.csv') \n",
    "\n",
    "        # Calculate policy \n",
    "        policy_available_actions=list(np.arange(number_of_columns))\n",
    "        policy_columns=[]\n",
    "        policy_current_state=initial_state\n",
    "        while len(policy_available_actions)>0:\n",
    "            # Get available actions in the current state\n",
    "            policy_available_actions = available_actions(number_of_columns,policy_columns,initial_state,policy_current_state, external_trashold, exploration=0)\n",
    "            # # Sample next action to be performed\n",
    "            if len(policy_available_actions)>0:\n",
    "                policy_select_action = sample_next_action(policy_current_state, Q, policy_available_actions, exploration=0)\n",
    "                # Update the episode_columns\n",
    "                policy_columns=update_columns(policy_select_action,policy_columns)\n",
    "                policy_current_state=policy_select_action\n",
    "        # Calculate policy_accuracy    \n",
    "        if len(policy_columns)>0:\n",
    "            ##for training dataset##\n",
    "            policy_data=get_data(episode_size,policy=1,mode='train')\n",
    "            X_policy,y_policy=data_separate(policy_data)\n",
    "            X_train_main_policy, X_test_main_policy, y_train_policy, y_test_policy = data_split(X,y)\n",
    "            X_train_policy, X_test_policy =update_X_train_X_test(policy_columns, X_train_main_policy, X_test_main_policy)\n",
    "            policy_error=Learner(X_train_policy, X_test_policy,y_train_policy, y_test_policy)\n",
    "            policy_accuracy_train=1-policy_error\n",
    "            ##for testing dataset##\n",
    "            policy_data=get_data(episode_size,policy=1,mode='test')\n",
    "            X_policy,y_policy=data_separate(policy_data)\n",
    "            X_train_main_policy, X_test_main_policy, y_train_policy, y_test_policy = data_split(X,y)\n",
    "            X_train_policy, X_test_policy =update_X_train_X_test(policy_columns, X_train_main_policy, X_test_main_policy)\n",
    "            policy_error=Learner(X_train_policy, X_test_policy,y_train_policy, y_test_policy)\n",
    "            policy_accuracy_test=1-policy_error \n",
    "        else:\n",
    "            policy_accuracy_train=0 \n",
    "            policy_accuracy_test=0\n",
    "        #df=df.append({'episode':str(i+1), 'episode_columns':str(episode_columns),'policy_columns':str(policy_columns),'policy_accuracy_train':policy_accuracy_train,'policy_accuracy_test':policy_accuracy_test}, ignore_index=True)\n",
    "        #new_row = pd.DataFrame([{'episode': str(i+1),\n",
    "        #                  'episode_columns': str(episode_columns),\n",
    "        #                  'policy_columns': str(policy_columns),\n",
    "        #                  'policy_accuracy_train': policy_accuracy_train,\n",
    "        #                  'policy_accuracy_test': policy_accuracy_test}])\n",
    "        #df = pd.concat([df, new_row], ignore_index=True)\n",
    "        df.loc[len(df)] = {\n",
    "            'episode': str(i+1),\n",
    "            'episode_columns': str(episode_columns),\n",
    "            'policy_columns': str(policy_columns),\n",
    "            'policy_accuracy_train': policy_accuracy_train,\n",
    "            'policy_accuracy_test': policy_accuracy_test\n",
    "        }\n",
    "\n",
    "        #Prints\n",
    "        print (\"episode \"+ str(i+1) +\" start\") \n",
    "        print (\"episode columns: \"+ str(episode_columns) + \" epsilon: \" + str(epsilon) + \" learning rate: \" + str(learning_rate) + \" error: \" +str(episode_error))\n",
    "        print (\"episode policy:\" + str(policy_columns) + \" train accuracy: \" + str(policy_accuracy_train)  + \" test accuracy: \" +str(policy_accuracy_test)) \n",
    "        print (\"episode \"+ str(i+1) +\" end\") \n",
    "    ########## End of episode  ############\n",
    "    #df.to_excel(writer, 'Experiment' + str(e))\n",
    "    df.to_excel(writer, sheet_name='Experiment' + str(e))\n",
    "    df_plot=df[['episode','policy_accuracy_train','policy_accuracy_test']]\n",
    "    plot=df_plot.plot()\n",
    "    fig = plot.get_figure()\n",
    "    fig.savefig('Experiments/'+ str(experiment) + '/plot_experiment_' + str(e) +'.png')\n",
    "    \n",
    "#writer.save()\n",
    "with pd.ExcelWriter('Experiments/'+ str(experiment) + '/df.xlsx') as writer:\n",
    "    df.to_excel(writer, sheet_name='Experiment' + str(e))\n",
    "\n",
    "## for run time ##\n",
    "stop = timeit.default_timer()\n",
    "print (stop - start)\n",
    "## for run time ##"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 212,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Quantum Learner based on Benedetti et al.\n",
    "class QuantumLearner:\n",
    "    def __init__(self, num_layers=2):\n",
    "        self.num_layers = num_layers\n",
    "        self.weights = None\n",
    "        self.bias = None\n",
    "        self.num_qubits = None\n",
    "        self.dev = None\n",
    "        self.opt = qml.optimize.AdamOptimizer(0.05)\n",
    "\n",
    "    def _initialize_circuit(self, num_features):\n",
    "        # Update the number of qubits to match the current feature count.\n",
    "        self.num_qubits = num_features\n",
    "        self.dev = qml.device(\"default.qubit\", wires=list(range(self.num_qubits)))\n",
    "\n",
    "        @qml.qnode(self.dev, interface=\"autograd\")\n",
    "        def circuit(weights, x):\n",
    "            self.feature_encoding(x)\n",
    "            for W in weights:\n",
    "                self.variational_layer(W)\n",
    "            return qml.expval(qml.PauliZ(0))\n",
    "        \n",
    "        self.circuit = circuit\n",
    "\n",
    "    def feature_encoding(self, x):\n",
    "        for i in range(self.num_qubits):\n",
    "            qml.RY(np.pi * x[i], wires=i)\n",
    "        for i in range(self.num_qubits - 1):\n",
    "            qml.CZ(wires=[i, i + 1])\n",
    "\n",
    "    def variational_layer(self, W):\n",
    "        for i in range(self.num_qubits):\n",
    "            qml.Rot(W[i, 0], W[i, 1], W[i, 2], wires=i)\n",
    "        for i in range(self.num_qubits - 1):\n",
    "            qml.CNOT(wires=[i, i + 1])\n",
    "        if self.num_qubits > 1:\n",
    "            qml.CNOT(wires=[self.num_qubits - 1, 0])\n",
    "\n",
    "    def variational_classifier(self, weights, bias, x):\n",
    "        return self.circuit(weights, x) + bias\n",
    "\n",
    "    def cost(self, weights, bias, X, Y):\n",
    "        # X is expected to be a NumPy array.\n",
    "        predictions = qml.numpy.array([self.variational_classifier(weights, bias, x) for x in X])\n",
    "        return qml.numpy.mean((qml.numpy.array(Y) - predictions) ** 2)\n",
    "\n",
    "    def fit(self, X_train, y_train, num_it=100, batch_size=48, warm_start=False):\n",
    "        # Convert inputs to NumPy arrays if needed.\n",
    "        if hasattr(X_train, \"values\"):\n",
    "            X_train = X_train.values.astype(np.float64)\n",
    "        else:\n",
    "            X_train = np.array(X_train, dtype=np.float64)\n",
    "        if hasattr(y_train, \"values\"):\n",
    "            y_train = y_train.values\n",
    "        else:\n",
    "            y_train = np.array(y_train)\n",
    "        \n",
    "        current_features = X_train.shape[1]\n",
    "        \n",
    "        # If not warm starting or if the number of features has changed, reinitialize the circuit and parameters.\n",
    "        if not warm_start or (self.num_qubits is None) or (self.num_qubits != current_features):\n",
    "            self._initialize_circuit(current_features)\n",
    "            np.random.seed(0)\n",
    "            self.weights = qml.numpy.tensor(\n",
    "                0.01 * np.random.randn(self.num_layers, self.num_qubits, 3),\n",
    "                requires_grad=True,\n",
    "            )\n",
    "            self.bias = qml.numpy.tensor(0.0, requires_grad=True)\n",
    "\n",
    "        batch_size = min(batch_size, len(X_train))\n",
    "        for it in range(num_it):\n",
    "            batch_index = np.random.choice(len(X_train), batch_size, replace=False)\n",
    "            X_batch = X_train[batch_index]\n",
    "            Y_batch = y_train[batch_index]\n",
    "            self.weights, self.bias = self.opt.step(\n",
    "                lambda w, b: self.cost(w, b, X_batch, Y_batch), self.weights, self.bias\n",
    "            )\n",
    "        return self\n",
    "\n",
    "    def predict(self, X_test):\n",
    "        if hasattr(X_test, \"values\"):\n",
    "            X_test = X_test.values.astype(np.float64)\n",
    "        else:\n",
    "            X_test = np.array(X_test, dtype=np.float64)\n",
    "        return np.array([\n",
    "            float(qml.numpy.sign(self.variational_classifier(self.weights, self.bias, x)))\n",
    "            for x in X_test\n",
    "        ])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 227,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Classical Learner based on a simple ANN\n",
    "class ClassicalLearner(nn.Module):\n",
    "    def __init__(self, num_layers=2, hidden_size=5):\n",
    "        super().__init__()\n",
    "        self.layers = None\n",
    "        self.sigmoid = nn.Sigmoid()\n",
    "\n",
    "    def initialize_layers(self, input_size, num_layers, hidden_size=5):\n",
    "        layers = [input_size] + [hidden_size] * (num_layers - 1) + [1]\n",
    "        self.layers = nn.ModuleList([nn.Linear(layers[i], layers[i+1], dtype=torch.float64) for i in range(len(layers) - 1)])\n",
    "    \n",
    "    def forward(self, x):\n",
    "        x = x.to(torch.float64)\n",
    "        for layer in self.layers[:-1]:\n",
    "            x = torch.relu(layer(x))\n",
    "        return self.sigmoid(self.layers[-1](x))\n",
    "    \n",
    "    def fit(self, X_train, y_train, num_it=50, lr=0.01):\n",
    "        input_size = X_train.shape[1]\n",
    "        self.initialize_layers(input_size, num_layers=2)\n",
    "        optimizer = optim.Adam(self.parameters(), lr=lr)\n",
    "        y_train = torch.tensor(y_train.values, dtype=torch.float64).reshape(-1, 1)\n",
    "        \n",
    "        for epoch in range(num_it):\n",
    "            optimizer.zero_grad()\n",
    "            y_pred = self.forward(torch.tensor(X_train.values, dtype=torch.float64)).reshape(-1, 1)\n",
    "            loss = nn.BCELoss()(y_pred, y_train)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "        \n",
    "        return self\n",
    "    \n",
    "    def predict(self, X_test):\n",
    "        with torch.no_grad():\n",
    "            X_test_tensor = torch.tensor(X_test.values, dtype=torch.float64)\n",
    "            y_pred = self.forward(X_test_tensor).reshape(-1, 1)\n",
    "            return (y_pred.numpy().flatten() > 0.5).astype(int)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (venv)",
   "language": "python",
   "name": "venv"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
