{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "e1a75755-a676-474f-9eea-926866ff044f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training [2,2,2]...\n",
      "Training [2,3,2]...\n",
      "Training [2,3,2,2]...\n",
      "Training [2,3,3,2]...\n",
      "Training Quantum Model...\n",
      "[2,2,2] Accuracy: 0.6200\n",
      "[2,3,2] Accuracy: 0.6200\n",
      "[2,3,2,2] Accuracy: 0.6200\n",
      "[2,3,3,2] Accuracy: 0.6200\n",
      "Quantum VQC Accuracy: 0.7500\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pennylane as qml\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "# === Generate Binary Circle Dataset ===\n",
    "np.random.seed(42)\n",
    "N = 500  # Number of data points\n",
    "X = np.random.uniform(-1, 1, size=(N, 2))\n",
    "y = (X[:, 0]**2 + X[:, 1]**2 <= 0.49).astype(int)  # Circle radius r=0.7 -> r^2 = 0.49\n",
    "\n",
    "# Split dataset\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# === Quantum Classifier Setup ===\n",
    "n_qubits = 2\n",
    "dev = qml.device(\"default.qubit\", wires=n_qubits)\n",
    "\n",
    "@qml.qnode(dev, interface=\"torch\")\n",
    "def quantum_circuit(params, x):\n",
    "    \"\"\"Variational Quantum Classifier (VQC)\"\"\"\n",
    "    qml.Hadamard(wires=0)\n",
    "    qml.Hadamard(wires=1)\n",
    "    \n",
    "    # Feature Encoding\n",
    "    qml.RZ(x[0] * np.pi, wires=0)\n",
    "    qml.RZ(x[1] * np.pi, wires=1)\n",
    "    \n",
    "    # Variational Layer\n",
    "    qml.RY(params[0], wires=0)\n",
    "    qml.RY(params[1], wires=1)\n",
    "    qml.CNOT(wires=[0, 1])\n",
    "    \n",
    "    return qml.expval(qml.PauliZ(0))\n",
    "\n",
    "class QuantumClassifier(nn.Module):\n",
    "    \"\"\"Quantum Classifier (VQC)\"\"\"\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.params = nn.Parameter(torch.rand(2, dtype=torch.float64, requires_grad=True))  # Ensure float64\n",
    "\n",
    "    def forward(self, x):\n",
    "        \"\"\"Fix: Apply to batch, reshape output, and rescale from [-1,1] to [0,1]\"\"\"\n",
    "        x = x.to(torch.float64)  # Convert input to float64\n",
    "        raw_output = torch.vmap(lambda x_single: quantum_circuit(self.params, x_single))(x).reshape(-1, 1)\n",
    "        return (raw_output + 1) / 2  # Rescale to [0,1]\n",
    "\n",
    "# === Classical ANN Classifiers (Different Architectures) ===\n",
    "class ClassicalANN(nn.Module):\n",
    "    \"\"\"Feedforward Neural Network\"\"\"\n",
    "    def __init__(self, layers):\n",
    "        super().__init__()\n",
    "        self.layers = nn.ModuleList()\n",
    "        for i in range(len(layers) - 1):\n",
    "            self.layers.append(nn.Linear(layers[i], layers[i+1], dtype=torch.float64))  # Fix: Use float64\n",
    "        self.sigmoid = nn.Sigmoid()\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = x.to(torch.float64)  # Fix: Convert input to float64\n",
    "        for layer in self.layers[:-1]:\n",
    "            x = torch.relu(layer(x))\n",
    "        x = self.sigmoid(self.layers[-1](x))\n",
    "        return x\n",
    "\n",
    "# Create ANN models based on the table\n",
    "ann_models = {\n",
    "    \"[2,2,2]\": ClassicalANN([2, 2, 2, 1]),\n",
    "    \"[2,3,2]\": ClassicalANN([2, 3, 2, 1]),\n",
    "    \"[2,3,2,2]\": ClassicalANN([2, 3, 2, 2, 1]),\n",
    "    \"[2,3,3,2]\": ClassicalANN([2, 3, 3, 2, 1]),\n",
    "}\n",
    "\n",
    "# Create Quantum Model\n",
    "quantum_model = QuantumClassifier()\n",
    "\n",
    "# === Training Setup ===\n",
    "loss_fn = nn.BCELoss()\n",
    "optimizer_q = optim.Adam(quantum_model.parameters(), lr=0.1)\n",
    "\n",
    "def train_model(model, optimizer, X_train, y_train, epochs=20):\n",
    "    \"\"\"Fix: Ensure training tensors are float64\"\"\"\n",
    "    X_train_torch = torch.tensor(X_train, dtype=torch.float64)  # Convert to float64\n",
    "    y_train_torch = torch.tensor(y_train, dtype=torch.float64).reshape(-1, 1)  # Convert to float64\n",
    "\n",
    "    for epoch in range(epochs):\n",
    "        optimizer.zero_grad()\n",
    "        y_pred = model(X_train_torch).reshape(-1, 1)  # Fix: Ensure output shape matches target\n",
    "        loss = loss_fn(y_pred, y_train_torch)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "# Train classical models\n",
    "for name, model in ann_models.items():\n",
    "    optimizer = optim.Adam(model.parameters(), lr=0.01)\n",
    "    print(f\"Training {name}...\")\n",
    "    train_model(model, optimizer, X_train, y_train, epochs=20)\n",
    "\n",
    "# Train Quantum Model\n",
    "print(\"Training Quantum Model...\")\n",
    "train_model(quantum_model, optimizer_q, X_train, y_train, epochs=20)\n",
    "\n",
    "# === Evaluation ===\n",
    "def evaluate_model(model, X_test, y_test):\n",
    "    \"\"\"Fix: Convert input to float64 before passing to model\"\"\"\n",
    "    with torch.no_grad():\n",
    "        X_test_torch = torch.tensor(X_test, dtype=torch.float64)  # Convert to float64\n",
    "        y_pred = model(X_test_torch).reshape(-1, 1)  # Fix: Reshape for comparison\n",
    "        y_pred = (y_pred.numpy().flatten() > 0.5).astype(int)\n",
    "    return accuracy_score(y_test, y_pred)\n",
    "\n",
    "# Evaluate all models\n",
    "results = {}\n",
    "for name, model in ann_models.items():\n",
    "    acc = evaluate_model(model, X_test, y_test)\n",
    "    results[name] = acc\n",
    "\n",
    "acc_q = evaluate_model(quantum_model, X_test, y_test)\n",
    "results[\"Quantum VQC\"] = acc_q\n",
    "\n",
    "# === Print Results ===\n",
    "for model, acc in results.items():\n",
    "    print(f\"{model} Accuracy: {acc:.4f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "8867aca3-7056-41ec-8166-36ece16f51c7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=== Model Performance Comparison ===\n",
      "+-------------+------------+\n",
      "| Model       |   Accuracy |\n",
      "+=============+============+\n",
      "| [2,2,2]     |       0.62 |\n",
      "+-------------+------------+\n",
      "| [2,3,2]     |       0.62 |\n",
      "+-------------+------------+\n",
      "| [2,3,2,2]   |       0.62 |\n",
      "+-------------+------------+\n",
      "| [2,3,3,2]   |       0.62 |\n",
      "+-------------+------------+\n",
      "| Quantum VQC |       0.75 |\n",
      "+-------------+------------+\n"
     ]
    }
   ],
   "source": [
    "from tabulate import tabulate  # Install with: pip install tabulate\n",
    "\n",
    "# === Print Results in a Table ===\n",
    "table_data = [[model, f\"{acc:.4f}\"] for model, acc in results.items()]\n",
    "headers = [\"Model\", \"Accuracy\"]\n",
    "\n",
    "print(\"\\n=== Model Performance Comparison ===\")\n",
    "print(tabulate(table_data, headers=headers, tablefmt=\"grid\"))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "3cdf8ab6-82bd-4d46-b319-9fa33b7cae83",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iter:     1 | Cost: 2.3009054 | Accuracy: 0.3657928 \n",
      "Iter:     2 | Cost: 2.0157333 | Accuracy: 0.3657928 \n",
      "Iter:     3 | Cost: 1.6928554 | Accuracy: 0.3657928 \n",
      "Iter:     4 | Cost: 1.4394783 | Accuracy: 0.3657928 \n",
      "Iter:     5 | Cost: 1.2992964 | Accuracy: 0.5205993 \n",
      "Iter:     6 | Cost: 1.2592430 | Accuracy: 0.6167291 \n",
      "Iter:     7 | Cost: 1.2860673 | Accuracy: 0.6167291 \n",
      "Iter:     8 | Cost: 1.3340117 | Accuracy: 0.6167291 \n",
      "Iter:     9 | Cost: 1.3157367 | Accuracy: 0.6167291 \n",
      "Iter:    10 | Cost: 1.2278186 | Accuracy: 0.6167291 \n",
      "Iter:    11 | Cost: 1.1070846 | Accuracy: 0.6167291 \n",
      "Iter:    12 | Cost: 1.0319251 | Accuracy: 0.6167291 \n",
      "Iter:    13 | Cost: 0.9932298 | Accuracy: 0.6167291 \n",
      "Iter:    14 | Cost: 0.9745684 | Accuracy: 0.6167291 \n",
      "Iter:    15 | Cost: 0.9807690 | Accuracy: 0.7802747 \n",
      "Iter:    16 | Cost: 0.9487227 | Accuracy: 0.7802747 \n",
      "Iter:    17 | Cost: 0.9169272 | Accuracy: 0.7802747 \n",
      "Iter:    18 | Cost: 0.8685417 | Accuracy: 0.7802747 \n",
      "Iter:    19 | Cost: 0.8121101 | Accuracy: 0.7802747 \n",
      "Iter:    20 | Cost: 0.7734251 | Accuracy: 0.7802747 \n",
      "Iter:    21 | Cost: 0.7578252 | Accuracy: 0.7802747 \n",
      "Iter:    22 | Cost: 0.7533380 | Accuracy: 0.7802747 \n",
      "Iter:    23 | Cost: 0.7540889 | Accuracy: 0.7727840 \n",
      "Iter:    24 | Cost: 0.7547410 | Accuracy: 0.7727840 \n",
      "Iter:    25 | Cost: 0.7418572 | Accuracy: 0.7727840 \n",
      "Iter:    26 | Cost: 0.7216730 | Accuracy: 0.7727840 \n",
      "Iter:    27 | Cost: 0.7054738 | Accuracy: 0.7840200 \n",
      "Iter:    28 | Cost: 0.6913697 | Accuracy: 0.7840200 \n",
      "Iter:    29 | Cost: 0.6826785 | Accuracy: 0.7840200 \n",
      "Iter:    30 | Cost: 0.6772999 | Accuracy: 0.7840200 \n",
      "Iter:    31 | Cost: 0.6758436 | Accuracy: 0.7840200 \n",
      "Iter:    32 | Cost: 0.6825518 | Accuracy: 0.7840200 \n",
      "Iter:    33 | Cost: 0.6856851 | Accuracy: 0.7840200 \n",
      "Iter:    34 | Cost: 0.6896622 | Accuracy: 0.7840200 \n",
      "Iter:    35 | Cost: 0.6943790 | Accuracy: 0.7840200 \n",
      "Iter:    36 | Cost: 0.7012850 | Accuracy: 0.7840200 \n",
      "Iter:    37 | Cost: 0.7067733 | Accuracy: 0.7840200 \n",
      "Iter:    38 | Cost: 0.7103192 | Accuracy: 0.7840200 \n",
      "Iter:    39 | Cost: 0.7161690 | Accuracy: 0.7840200 \n",
      "Iter:    40 | Cost: 0.7176914 | Accuracy: 0.7840200 \n",
      "Iter:    41 | Cost: 0.7134635 | Accuracy: 0.7840200 \n",
      "Iter:    42 | Cost: 0.7095454 | Accuracy: 0.7840200 \n",
      "Iter:    43 | Cost: 0.7075034 | Accuracy: 0.7840200 \n",
      "Iter:    44 | Cost: 0.7016297 | Accuracy: 0.7840200 \n",
      "Iter:    45 | Cost: 0.6959733 | Accuracy: 0.7840200 \n",
      "Iter:    46 | Cost: 0.6963289 | Accuracy: 0.7840200 \n",
      "Iter:    47 | Cost: 0.6943661 | Accuracy: 0.7840200 \n",
      "Iter:    48 | Cost: 0.6855695 | Accuracy: 0.7840200 \n",
      "Iter:    49 | Cost: 0.6820337 | Accuracy: 0.7840200 \n",
      "Iter:    50 | Cost: 0.6739566 | Accuracy: 0.7840200 \n",
      "Iter:    51 | Cost: 0.6727428 | Accuracy: 0.7840200 \n",
      "Iter:    52 | Cost: 0.6757914 | Accuracy: 0.7840200 \n",
      "Iter:    53 | Cost: 0.6806343 | Accuracy: 0.7840200 \n",
      "Iter:    54 | Cost: 0.6841203 | Accuracy: 0.7840200 \n",
      "Iter:    55 | Cost: 0.6862026 | Accuracy: 0.7840200 \n",
      "Iter:    56 | Cost: 0.6912584 | Accuracy: 0.7840200 \n",
      "Iter:    57 | Cost: 0.6992686 | Accuracy: 0.7852684 \n",
      "Iter:    58 | Cost: 0.7072007 | Accuracy: 0.7852684 \n",
      "Iter:    59 | Cost: 0.7130674 | Accuracy: 0.7852684 \n",
      "Iter:    60 | Cost: 0.7230222 | Accuracy: 0.7852684 \n",
      "Iter:    61 | Cost: 0.7195306 | Accuracy: 0.7852684 \n",
      "Iter:    62 | Cost: 0.7081696 | Accuracy: 0.7852684 \n",
      "Iter:    63 | Cost: 0.7017707 | Accuracy: 0.7852684 \n",
      "Iter:    64 | Cost: 0.6935658 | Accuracy: 0.7840200 \n",
      "Iter:    65 | Cost: 0.6855921 | Accuracy: 0.7840200 \n",
      "Iter:    66 | Cost: 0.6818968 | Accuracy: 0.7840200 \n",
      "Iter:    67 | Cost: 0.6849336 | Accuracy: 0.7840200 \n",
      "Iter:    68 | Cost: 0.6827583 | Accuracy: 0.7840200 \n",
      "Iter:    69 | Cost: 0.6800306 | Accuracy: 0.7840200 \n",
      "Iter:    70 | Cost: 0.6809625 | Accuracy: 0.7840200 \n",
      "Total training time: 259.55 seconds\n",
      "Quantum Model Performance:\n",
      "Accuracy: 0.7888888888888889\n",
      "Precision: 0.7666666666666667\n",
      "Recall: 0.6571428571428571\n",
      "F1 Score: 0.7712374581939799\n",
      "Classical Model Performance:\n",
      "Accuracy: 0.7555555555555555\n",
      "Precision: 0.696969696969697\n",
      "Recall: 0.6571428571428571\n",
      "F1 Score: 0.7400210084033614\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import pennylane as qml\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score\n",
    "import time\n",
    "import math\n",
    "\n",
    "num_qubits = 4\n",
    "num_layers = 2\n",
    "dev = qml.device(\"default.qubit\", wires=num_qubits)\n",
    "\n",
    "# Quantum circuit functions\n",
    "def statepreparation(x):\n",
    "    qml.BasisEmbedding(x, wires=range(num_qubits))\n",
    "\n",
    "def layer(W):\n",
    "    for i in range(num_qubits):\n",
    "        qml.Rot(W[i, 0], W[i, 1], W[i, 2], wires=i)\n",
    "    for i in range(num_qubits - 1):\n",
    "        qml.CNOT(wires=[i, i + 1])\n",
    "    qml.CNOT(wires=[num_qubits - 1, 0])\n",
    "\n",
    "@qml.qnode(dev, interface=\"autograd\")\n",
    "def circuit(weights, x):\n",
    "    statepreparation(x)\n",
    "    for W in weights:\n",
    "        layer(W)\n",
    "    return qml.expval(qml.PauliZ(0))\n",
    "\n",
    "def variational_classifier(weights, bias, x):\n",
    "    return circuit(weights, x) + bias\n",
    "\n",
    "def square_loss(labels, predictions):\n",
    "    labels = qml.numpy.array(labels)  # Ensure PennyLane tensor\n",
    "    predictions = qml.numpy.array(predictions)\n",
    "    return qml.numpy.mean((labels - predictions) ** 2)\n",
    "\n",
    "def accuracy(labels, predictions):\n",
    "    labels = qml.numpy.array(labels)\n",
    "    predictions = qml.numpy.array(predictions)\n",
    "    return qml.numpy.mean(qml.numpy.abs(labels - predictions) < 1e-5)\n",
    "\n",
    "def cost(weights, bias, X, Y):\n",
    "    predictions = qml.numpy.array([variational_classifier(weights, bias, x) for x in X])\n",
    "    return square_loss(Y, predictions)\n",
    "\n",
    "# Preparing dataset\n",
    "df_train = pd.read_csv('train.csv')\n",
    "df_train['Pclass'] = df_train['Pclass'].astype(str)\n",
    "df_train = pd.concat([df_train, pd.get_dummies(df_train[['Pclass', 'Sex', 'Embarked']])], axis=1)\n",
    "df_train['Age'] = df_train['Age'].fillna(df_train['Age'].median())\n",
    "df_train['is_child'] = df_train['Age'].map(lambda x: 1 if x < 12 else 0)\n",
    "cols_model = ['is_child', 'Pclass_1', 'Pclass_2', 'Sex_female']\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    df_train[cols_model], df_train['Survived'], test_size=0.10, random_state=42, stratify=df_train['Survived']\n",
    ")\n",
    "\n",
    "X_train = np.array(X_train.values)\n",
    "Y_train = np.array(y_train.values * 2 - np.ones(len(y_train)))\n",
    "X_test = np.array(X_test.values)\n",
    "Y_test = np.array(y_test.values * 2 - np.ones(len(y_test)))\n",
    "\n",
    "# Initialize quantum model parameters\n",
    "np.random.seed(0)\n",
    "weights_init = qml.numpy.tensor(0.01 * np.random.randn(num_layers, num_qubits, 3), requires_grad=True)\n",
    "bias_init = qml.numpy.tensor(0.0, requires_grad=True)\n",
    "\n",
    "opt = qml.optimize.AdamOptimizer(0.125)\n",
    "num_it = 70\n",
    "batch_size = math.floor(len(X_train) / num_it)\n",
    "\n",
    "# Training process\n",
    "start_time = time.time()\n",
    "weights = weights_init\n",
    "bias = bias_init\n",
    "\n",
    "for it in range(num_it):\n",
    "    batch_index = np.random.randint(0, len(X_train), (batch_size,))\n",
    "    X_batch = X_train[batch_index]\n",
    "    Y_batch = Y_train[batch_index]\n",
    "    \n",
    "    # Ensure correct types inside the optimizer step\n",
    "    weights, bias = opt.step(lambda w, b: cost(w, b, X_batch, Y_batch), weights, bias)\n",
    "\n",
    "    # Convert back to PennyLane tensors\n",
    "    weights = qml.numpy.tensor(weights, requires_grad=True)\n",
    "    bias = qml.numpy.tensor(bias, requires_grad=True)\n",
    "    \n",
    "    predictions = qml.numpy.array([qml.numpy.sign(variational_classifier(weights, bias, x)) for x in X_train])\n",
    "    acc = accuracy(Y_train, predictions)\n",
    "\n",
    "    print(\"Iter: {:5d} | Cost: {:0.7f} | Accuracy: {:0.7f} \".format(it + 1, cost(weights, bias, X_train, Y_train), acc))\n",
    "\n",
    "elapsed_time = time.time() - start_time\n",
    "print(\"Total training time: {:.2f} seconds\".format(elapsed_time))\n",
    "\n",
    "# Evaluation\n",
    "predictions = qml.numpy.array([qml.numpy.sign(variational_classifier(weights, bias, x)) for x in X_test])\n",
    "\n",
    "# Classical ANN Model\n",
    "class ClassicalANN(nn.Module):\n",
    "    def __init__(self, layers):\n",
    "        super().__init__()\n",
    "        self.layers = nn.ModuleList()\n",
    "        for i in range(len(layers) - 1):\n",
    "            self.layers.append(nn.Linear(layers[i], layers[i+1], dtype=torch.float64))\n",
    "        self.sigmoid = nn.Sigmoid()\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = x.to(torch.float64)\n",
    "        for layer in self.layers[:-1]:\n",
    "            x = torch.relu(layer(x))\n",
    "        return self.sigmoid(self.layers[-1](x))\n",
    "\n",
    "# Train Classical Model\n",
    "classical_model = ClassicalANN([len(cols_model), 10, 5, 1])\n",
    "optimizer_classical = optim.Adam(classical_model.parameters(), lr=0.01)\n",
    "\n",
    "def train_classical(model, optimizer, X_train, y_train, epochs=50):\n",
    "    y_train = torch.tensor(y_train.tolist(), dtype=torch.float64).reshape(-1, 1)  # Ensure correct shape\n",
    "    for epoch in range(epochs):\n",
    "        optimizer.zero_grad()\n",
    "        y_pred = model(torch.tensor(X_train.tolist(), dtype=torch.float64)).reshape(-1, 1)  # Ensure same shape\n",
    "        loss = nn.BCELoss()(y_pred, y_train)  # No more shape mismatch\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "train_classical(classical_model, optimizer_classical, X_train, y_train, epochs=50)\n",
    "\n",
    "# Compare models\n",
    "print(\"Quantum Model Performance:\")\n",
    "print(\"Accuracy:\", accuracy_score(Y_test, predictions))\n",
    "print(\"Precision:\", precision_score(Y_test, predictions))\n",
    "print(\"Recall:\", recall_score(Y_test, predictions))\n",
    "print(\"F1 Score:\", f1_score(Y_test, predictions, average='macro'))\n",
    "\n",
    "with torch.no_grad():\n",
    "    X_test_numeric = np.array(X_test, dtype=np.float64)  # Ensure numeric dtype\n",
    "    y_pred_classical = classical_model(torch.tensor(X_test_numeric, dtype=torch.float64)).reshape(-1, 1)\n",
    "    y_pred_classical = (y_pred_classical.numpy().flatten() > 0.5).astype(int)\n",
    "\n",
    "print(\"Classical Model Performance:\")\n",
    "print(\"Accuracy:\", accuracy_score(y_test, y_pred_classical))\n",
    "print(\"Precision:\", precision_score(y_test, y_pred_classical))\n",
    "print(\"Recall:\", recall_score(y_test, y_pred_classical))\n",
    "print(\"F1 Score:\", f1_score(y_test, y_pred_classical, average='macro'))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "91c475ab-eacd-45b9-aa96-76f7f1a23110",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Quantum Model with RZ Feature Encoding...\n",
      "Quantum Model (RZ Encoding) Accuracy: 0.3889\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import pennylane as qml\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score\n",
    "import math\n",
    "\n",
    "# === Load Titanic Dataset ===\n",
    "df_train = pd.read_csv('train.csv')\n",
    "\n",
    "# Convert categorical features using one-hot encoding\n",
    "df_train['Pclass'] = df_train['Pclass'].astype(str)\n",
    "df_train = pd.concat([df_train, pd.get_dummies(df_train[['Pclass', 'Sex', 'Embarked']])], axis=1)\n",
    "\n",
    "# Fill missing age values & create child feature\n",
    "df_train['Age'] = df_train['Age'].fillna(df_train['Age'].median())\n",
    "df_train['is_child'] = df_train['Age'].map(lambda x: 1 if x < 12 else 0)\n",
    "\n",
    "# Select model features\n",
    "cols_model = ['is_child', 'Pclass_1', 'Pclass_2', 'Sex_female']\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    df_train[cols_model], df_train['Survived'], test_size=0.10, random_state=42, stratify=df_train['Survived']\n",
    ")\n",
    "\n",
    "# Normalize features to [0,1]\n",
    "X_train = np.array(X_train.values, dtype=np.float64)\n",
    "X_test = np.array(X_test.values, dtype=np.float64)\n",
    "X_train = (X_train - X_train.min()) / (X_train.max() - X_train.min())  # Normalize to [0,1]\n",
    "X_test = (X_test - X_test.min()) / (X_test.max() - X_test.min())  # Normalize to [0,1]\n",
    "\n",
    "# Convert labels to {-1,1} for quantum training\n",
    "Y_train = np.array(y_train.values * 2 - np.ones(len(y_train)))\n",
    "Y_test = np.array(y_test.values * 2 - np.ones(len(y_test)))\n",
    "\n",
    "# === Quantum Classifier Setup ===\n",
    "num_qubits = len(cols_model)  # Number of qubits matches number of features\n",
    "dev = qml.device(\"default.qubit\", wires=num_qubits)\n",
    "\n",
    "@qml.qnode(dev, interface=\"torch\")\n",
    "def quantum_circuit(params, x):\n",
    "    \"\"\"Variational Quantum Classifier with RZ Feature Encoding\"\"\"\n",
    "    # Feature Encoding\n",
    "    for i in range(num_qubits):\n",
    "        qml.RZ(x[i] * np.pi, wires=i)  # Encode features as RZ rotations\n",
    "    \n",
    "    # Variational Layer\n",
    "    for i in range(num_qubits):\n",
    "        qml.RY(params[i], wires=i)\n",
    "    for i in range(num_qubits - 1):\n",
    "        qml.CNOT(wires=[i, i + 1])\n",
    "    qml.CNOT(wires=[num_qubits - 1, 0])  # Wrap-around entanglement\n",
    "\n",
    "    return qml.expval(qml.PauliZ(0))\n",
    "\n",
    "class QuantumClassifier(nn.Module):\n",
    "    \"\"\"Quantum Variational Classifier\"\"\"\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.params = nn.Parameter(torch.rand(num_qubits, dtype=torch.float64, requires_grad=True))\n",
    "\n",
    "    def forward(self, x):\n",
    "        \"\"\"Apply quantum circuit and normalize output to [0,1]\"\"\"\n",
    "        x = x.to(torch.float64)\n",
    "        raw_output = torch.vmap(lambda x_single: quantum_circuit(self.params, x_single))(x).reshape(-1, 1)\n",
    "        return (raw_output + 1) / 2  # Map from [-1,1] to [0,1]\n",
    "\n",
    "# === Training Setup ===\n",
    "loss_fn = nn.BCELoss()\n",
    "optimizer_q = optim.Adam(QuantumClassifier().parameters(), lr=0.1)\n",
    "\n",
    "def train_model(model, optimizer, X_train, y_train, epochs=20):\n",
    "    \"\"\"Train Quantum Model\"\"\"\n",
    "    X_train_torch = torch.tensor(X_train, dtype=torch.float64)\n",
    "    y_train_torch = torch.tensor(y_train, dtype=torch.float64).reshape(-1, 1)\n",
    "\n",
    "    for epoch in range(epochs):\n",
    "        optimizer.zero_grad()\n",
    "        y_pred = model(X_train_torch).reshape(-1, 1)\n",
    "        loss = loss_fn(y_pred, y_train_torch)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "# Train Quantum Model\n",
    "quantum_model = QuantumClassifier()\n",
    "print(\"Training Quantum Model with RZ Feature Encoding...\")\n",
    "train_model(quantum_model, optimizer_q, X_train, y_train, epochs=20)\n",
    "\n",
    "# === Evaluation ===\n",
    "def evaluate_model(model, X_test, y_test):\n",
    "    with torch.no_grad():\n",
    "        X_test_torch = torch.tensor(X_test, dtype=torch.float64)\n",
    "        y_pred = model(X_test_torch).reshape(-1, 1)\n",
    "        y_pred = (y_pred.numpy().flatten() > 0.5).astype(int)\n",
    "    return accuracy_score(y_test, y_pred)\n",
    "\n",
    "acc_q = evaluate_model(quantum_model, X_test, y_test)\n",
    "print(f\"Quantum Model (RZ Encoding) Accuracy: {acc_q:.4f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "ea52610d-747f-4f35-aed6-45b93f8b4894",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Quantum Model with Improved Encoding...\n",
      "Quantum Model (Fixed Encoding) Accuracy: 0.6111\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import pennylane as qml\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "# === Load Titanic Dataset ===\n",
    "df_train = pd.read_csv('train.csv')\n",
    "\n",
    "# Convert categorical features using one-hot encoding\n",
    "df_train['Pclass'] = df_train['Pclass'].astype(str)\n",
    "df_train = pd.concat([df_train, pd.get_dummies(df_train[['Pclass', 'Sex', 'Embarked']])], axis=1)\n",
    "\n",
    "# Fill missing age values & create child feature\n",
    "df_train['Age'] = df_train['Age'].fillna(df_train['Age'].median())\n",
    "df_train['is_child'] = df_train['Age'].map(lambda x: 1 if x < 12 else 0)\n",
    "\n",
    "# Select model features\n",
    "cols_model = ['is_child', 'Pclass_1', 'Pclass_2', 'Sex_female']\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    df_train[cols_model], df_train['Survived'], test_size=0.10, random_state=42, stratify=df_train['Survived']\n",
    ")\n",
    "\n",
    "# Normalize across features\n",
    "X_train = np.array(X_train.values, dtype=np.float64)\n",
    "X_test = np.array(X_test.values, dtype=np.float64)\n",
    "X_train = (X_train - np.mean(X_train, axis=0)) / np.std(X_train, axis=0)\n",
    "X_test = (X_test - np.mean(X_test, axis=0)) / np.std(X_test, axis=0)\n",
    "\n",
    "# Convert labels to {0,1} for quantum training\n",
    "Y_train = np.array(y_train.values)\n",
    "Y_test = np.array(y_test.values)\n",
    "\n",
    "# === Quantum Classifier Setup ===\n",
    "num_qubits = len(cols_model)\n",
    "dev = qml.device(\"default.qubit\", wires=num_qubits)\n",
    "\n",
    "@qml.qnode(dev, interface=\"torch\")\n",
    "def quantum_circuit(params, x):\n",
    "    \"\"\"Variational Quantum Classifier with RZ and RX Encoding\"\"\"\n",
    "    # Feature Encoding\n",
    "    for i in range(num_qubits):\n",
    "        qml.RZ(x[i] * np.pi, wires=i)\n",
    "        qml.RX(x[i] * np.pi, wires=i)  # Adding amplitude encoding\n",
    "\n",
    "    # Variational Layers (More Layers for Expressiveness)\n",
    "    for _ in range(2):  # Increase circuit depth\n",
    "        for i in range(num_qubits):\n",
    "            qml.RY(params[i], wires=i)\n",
    "        for i in range(num_qubits - 1):\n",
    "            qml.CNOT(wires=[i, i + 1])\n",
    "        qml.CNOT(wires=[num_qubits - 1, 0])  # Wrap-around entanglement\n",
    "\n",
    "    return qml.expval(qml.PauliZ(0))\n",
    "\n",
    "class QuantumClassifier(nn.Module):\n",
    "    \"\"\"Quantum Variational Classifier\"\"\"\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.params = nn.Parameter(torch.rand(num_qubits, dtype=torch.float64, requires_grad=True))\n",
    "        self.bias = nn.Parameter(torch.tensor(0.0, dtype=torch.float64, requires_grad=True))\n",
    "\n",
    "    def forward(self, x):\n",
    "        \"\"\"Apply quantum circuit and normalize output\"\"\"\n",
    "        x = x.to(torch.float64)\n",
    "        raw_output = torch.vmap(lambda x_single: quantum_circuit(self.params, x_single))(x).reshape(-1, 1)\n",
    "        return torch.sigmoid(raw_output + self.bias)  # Apply sigmoid activation with bias\n",
    "\n",
    "# Training Setup\n",
    "loss_fn = nn.BCELoss()\n",
    "quantum_model = QuantumClassifier()\n",
    "optimizer_q = optim.Adam(quantum_model.parameters(), lr=0.01)  # Reduce LR\n",
    "\n",
    "def train_model(model, optimizer, X_train, y_train, epochs=30):  # More epochs\n",
    "    X_train_torch = torch.tensor(X_train, dtype=torch.float64)\n",
    "    y_train_torch = torch.tensor(y_train, dtype=torch.float64).reshape(-1, 1)\n",
    "\n",
    "    for epoch in range(epochs):\n",
    "        optimizer.zero_grad()\n",
    "        y_pred = model(X_train_torch).reshape(-1, 1)\n",
    "        loss = loss_fn(y_pred, y_train_torch)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "# Train Quantum Model\n",
    "print(\"Training Quantum Model with Improved Encoding...\")\n",
    "train_model(quantum_model, optimizer_q, X_train, y_train, epochs=30)\n",
    "\n",
    "# Evaluation\n",
    "acc_q = evaluate_model(quantum_model, X_test, y_test)\n",
    "print(f\"Quantum Model (Fixed Encoding) Accuracy: {acc_q:.4f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bdd516c9-4064-4316-bf63-bd08a911a0fb",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (venv)",
   "language": "python",
   "name": "venv"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
