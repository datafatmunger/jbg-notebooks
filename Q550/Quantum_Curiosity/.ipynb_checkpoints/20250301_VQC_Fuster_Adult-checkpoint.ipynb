{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "9eff16e2-8186-41cb-8129-91132103e173",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Iteration 0: Parameter Check\n",
      "Weights Mean: 0.002989, Std: 0.010904\n",
      "Bias: 0.000000\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'Z_matrix' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[1], line 123\u001b[0m\n\u001b[1;32m    120\u001b[0m debug_parameters(weights, bias, it)\n\u001b[1;32m    122\u001b[0m \u001b[38;5;66;03m# Optimization step\u001b[39;00m\n\u001b[0;32m--> 123\u001b[0m weights, bias \u001b[38;5;241m=\u001b[39m \u001b[43mopt\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mstep\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43;01mlambda\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mw\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mb\u001b[49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mcost\u001b[49m\u001b[43m(\u001b[49m\u001b[43mw\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mb\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mX_batch\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mY_batch\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mweights\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbias\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    125\u001b[0m \u001b[38;5;66;03m# Compute training accuracy\u001b[39;00m\n\u001b[1;32m    126\u001b[0m predictions \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39marray([qml\u001b[38;5;241m.\u001b[39mnumpy\u001b[38;5;241m.\u001b[39msign(variational_classifier(weights, bias, x)) \u001b[38;5;28;01mfor\u001b[39;00m x \u001b[38;5;129;01min\u001b[39;00m X_train_q])\n",
      "File \u001b[0;32m~/.venv/lib/python3.13/site-packages/pennylane/optimize/gradient_descent.py:93\u001b[0m, in \u001b[0;36mGradientDescentOptimizer.step\u001b[0;34m(self, objective_fn, grad_fn, *args, **kwargs)\u001b[0m\n\u001b[1;32m     75\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mstep\u001b[39m(\u001b[38;5;28mself\u001b[39m, objective_fn, \u001b[38;5;241m*\u001b[39margs, grad_fn\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[1;32m     76\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Update trainable arguments with one step of the optimizer.\u001b[39;00m\n\u001b[1;32m     77\u001b[0m \n\u001b[1;32m     78\u001b[0m \u001b[38;5;124;03m    Args:\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     90\u001b[0m \u001b[38;5;124;03m        If single arg is provided, list [array] is replaced by array.\u001b[39;00m\n\u001b[1;32m     91\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[0;32m---> 93\u001b[0m     g, _ \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcompute_grad\u001b[49m\u001b[43m(\u001b[49m\u001b[43mobjective_fn\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mgrad_fn\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mgrad_fn\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     94\u001b[0m     new_args \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mapply_grad(g, args)\n\u001b[1;32m     96\u001b[0m     \u001b[38;5;66;03m# unwrap from list if one argument, cleaner return\u001b[39;00m\n",
      "File \u001b[0;32m~/.venv/lib/python3.13/site-packages/pennylane/optimize/gradient_descent.py:122\u001b[0m, in \u001b[0;36mGradientDescentOptimizer.compute_grad\u001b[0;34m(objective_fn, args, kwargs, grad_fn)\u001b[0m\n\u001b[1;32m    104\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124mr\u001b[39m\u001b[38;5;124;03m\"\"\"Compute the gradient of the objective function at the given point and return it along with\u001b[39;00m\n\u001b[1;32m    105\u001b[0m \u001b[38;5;124;03mthe objective function forward pass (if available).\u001b[39;00m\n\u001b[1;32m    106\u001b[0m \n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    119\u001b[0m \u001b[38;5;124;03m    will not be evaluated and instead ``None`` will be returned.\u001b[39;00m\n\u001b[1;32m    120\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m    121\u001b[0m g \u001b[38;5;241m=\u001b[39m get_gradient(objective_fn) \u001b[38;5;28;01mif\u001b[39;00m grad_fn \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;28;01melse\u001b[39;00m grad_fn\n\u001b[0;32m--> 122\u001b[0m grad \u001b[38;5;241m=\u001b[39m \u001b[43mg\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    123\u001b[0m forward \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mgetattr\u001b[39m(g, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mforward\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28;01mNone\u001b[39;00m)\n\u001b[1;32m    125\u001b[0m num_trainable_args \u001b[38;5;241m=\u001b[39m \u001b[38;5;28msum\u001b[39m(\u001b[38;5;28mgetattr\u001b[39m(arg, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mrequires_grad\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28;01mFalse\u001b[39;00m) \u001b[38;5;28;01mfor\u001b[39;00m arg \u001b[38;5;129;01min\u001b[39;00m args)\n",
      "File \u001b[0;32m~/.venv/lib/python3.13/site-packages/pennylane/_grad.py:224\u001b[0m, in \u001b[0;36mgrad.__call__\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m    221\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_fun(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[1;32m    222\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m ()\n\u001b[0;32m--> 224\u001b[0m grad_value, ans \u001b[38;5;241m=\u001b[39m \u001b[43mgrad_fn\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# pylint: disable=not-callable\u001b[39;00m\n\u001b[1;32m    225\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward \u001b[38;5;241m=\u001b[39m ans\n\u001b[1;32m    227\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m grad_value\n",
      "File \u001b[0;32m~/.venv/lib/python3.13/site-packages/autograd/wrap_util.py:20\u001b[0m, in \u001b[0;36munary_to_nary.<locals>.nary_operator.<locals>.nary_f\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     18\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m     19\u001b[0m     x \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mtuple\u001b[39m(args[i] \u001b[38;5;28;01mfor\u001b[39;00m i \u001b[38;5;129;01min\u001b[39;00m argnum)\n\u001b[0;32m---> 20\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43munary_operator\u001b[49m\u001b[43m(\u001b[49m\u001b[43munary_f\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mnary_op_args\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mnary_op_kwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/.venv/lib/python3.13/site-packages/pennylane/_grad.py:242\u001b[0m, in \u001b[0;36mgrad._grad_with_forward\u001b[0;34m(fun, x)\u001b[0m\n\u001b[1;32m    236\u001b[0m \u001b[38;5;129m@staticmethod\u001b[39m\n\u001b[1;32m    237\u001b[0m \u001b[38;5;129m@unary_to_nary\u001b[39m\n\u001b[1;32m    238\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21m_grad_with_forward\u001b[39m(fun, x):\n\u001b[1;32m    239\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"This function is a replica of ``autograd.grad``, with the only\u001b[39;00m\n\u001b[1;32m    240\u001b[0m \u001b[38;5;124;03m    difference being that it returns both the gradient *and* the forward pass\u001b[39;00m\n\u001b[1;32m    241\u001b[0m \u001b[38;5;124;03m    value.\"\"\"\u001b[39;00m\n\u001b[0;32m--> 242\u001b[0m     vjp, ans \u001b[38;5;241m=\u001b[39m \u001b[43m_make_vjp\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfun\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mx\u001b[49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# pylint: disable=redefined-outer-name\u001b[39;00m\n\u001b[1;32m    244\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m vspace(ans)\u001b[38;5;241m.\u001b[39msize \u001b[38;5;241m!=\u001b[39m \u001b[38;5;241m1\u001b[39m:\n\u001b[1;32m    245\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mTypeError\u001b[39;00m(\n\u001b[1;32m    246\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mGrad only applies to real scalar-output functions. \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    247\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mTry jacobian, elementwise_grad or holomorphic_grad.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    248\u001b[0m         )\n",
      "File \u001b[0;32m~/.venv/lib/python3.13/site-packages/autograd/core.py:10\u001b[0m, in \u001b[0;36mmake_vjp\u001b[0;34m(fun, x)\u001b[0m\n\u001b[1;32m      8\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mmake_vjp\u001b[39m(fun, x):\n\u001b[1;32m      9\u001b[0m     start_node \u001b[38;5;241m=\u001b[39m VJPNode\u001b[38;5;241m.\u001b[39mnew_root()\n\u001b[0;32m---> 10\u001b[0m     end_value, end_node \u001b[38;5;241m=\u001b[39m  \u001b[43mtrace\u001b[49m\u001b[43m(\u001b[49m\u001b[43mstart_node\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mfun\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mx\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     11\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m end_node \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m     12\u001b[0m         \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mvjp\u001b[39m(g): \u001b[38;5;28;01mreturn\u001b[39;00m vspace(x)\u001b[38;5;241m.\u001b[39mzeros()\n",
      "File \u001b[0;32m~/.venv/lib/python3.13/site-packages/autograd/tracer.py:10\u001b[0m, in \u001b[0;36mtrace\u001b[0;34m(start_node, fun, x)\u001b[0m\n\u001b[1;32m      8\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m trace_stack\u001b[38;5;241m.\u001b[39mnew_trace() \u001b[38;5;28;01mas\u001b[39;00m t:\n\u001b[1;32m      9\u001b[0m     start_box \u001b[38;5;241m=\u001b[39m new_box(x, t, start_node)\n\u001b[0;32m---> 10\u001b[0m     end_box \u001b[38;5;241m=\u001b[39m \u001b[43mfun\u001b[49m\u001b[43m(\u001b[49m\u001b[43mstart_box\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     11\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m isbox(end_box) \u001b[38;5;129;01mand\u001b[39;00m end_box\u001b[38;5;241m.\u001b[39m_trace \u001b[38;5;241m==\u001b[39m start_box\u001b[38;5;241m.\u001b[39m_trace:\n\u001b[1;32m     12\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m end_box\u001b[38;5;241m.\u001b[39m_value, end_box\u001b[38;5;241m.\u001b[39m_node\n",
      "File \u001b[0;32m~/.venv/lib/python3.13/site-packages/autograd/wrap_util.py:15\u001b[0m, in \u001b[0;36munary_to_nary.<locals>.nary_operator.<locals>.nary_f.<locals>.unary_f\u001b[0;34m(x)\u001b[0m\n\u001b[1;32m     13\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m     14\u001b[0m     subargs \u001b[38;5;241m=\u001b[39m subvals(args, \u001b[38;5;28mzip\u001b[39m(argnum, x))\n\u001b[0;32m---> 15\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfun\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43msubargs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[0;32mIn[1], line 123\u001b[0m, in \u001b[0;36m<lambda>\u001b[0;34m(w, b)\u001b[0m\n\u001b[1;32m    120\u001b[0m debug_parameters(weights, bias, it)\n\u001b[1;32m    122\u001b[0m \u001b[38;5;66;03m# Optimization step\u001b[39;00m\n\u001b[0;32m--> 123\u001b[0m weights, bias \u001b[38;5;241m=\u001b[39m opt\u001b[38;5;241m.\u001b[39mstep(\u001b[38;5;28;01mlambda\u001b[39;00m w, b: \u001b[43mcost\u001b[49m\u001b[43m(\u001b[49m\u001b[43mw\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mb\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mX_batch\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mY_batch\u001b[49m\u001b[43m)\u001b[49m, weights, bias)\n\u001b[1;32m    125\u001b[0m \u001b[38;5;66;03m# Compute training accuracy\u001b[39;00m\n\u001b[1;32m    126\u001b[0m predictions \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39marray([qml\u001b[38;5;241m.\u001b[39mnumpy\u001b[38;5;241m.\u001b[39msign(variational_classifier(weights, bias, x)) \u001b[38;5;28;01mfor\u001b[39;00m x \u001b[38;5;129;01min\u001b[39;00m X_train_q])\n",
      "Cell \u001b[0;32mIn[1], line 87\u001b[0m, in \u001b[0;36mcost\u001b[0;34m(weights, bias, X, Y)\u001b[0m\n\u001b[1;32m     86\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mcost\u001b[39m(weights, bias, X, Y):\n\u001b[0;32m---> 87\u001b[0m     predictions \u001b[38;5;241m=\u001b[39m qml\u001b[38;5;241m.\u001b[39mnumpy\u001b[38;5;241m.\u001b[39marray([\u001b[43mvariational_classifier\u001b[49m\u001b[43m(\u001b[49m\u001b[43mweights\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbias\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mx\u001b[49m\u001b[43m)\u001b[49m \u001b[38;5;28;01mfor\u001b[39;00m x \u001b[38;5;129;01min\u001b[39;00m X])\n\u001b[1;32m     88\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m square_loss(Y, predictions)\n",
      "Cell \u001b[0;32mIn[1], line 77\u001b[0m, in \u001b[0;36mvariational_classifier\u001b[0;34m(weights, bias, x)\u001b[0m\n\u001b[1;32m     76\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mvariational_classifier\u001b[39m(weights, bias, x):\n\u001b[0;32m---> 77\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mcircuit\u001b[49m\u001b[43m(\u001b[49m\u001b[43mweights\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mx\u001b[49m\u001b[43m)\u001b[49m \u001b[38;5;241m+\u001b[39m bias\n",
      "File \u001b[0;32m~/.venv/lib/python3.13/site-packages/pennylane/workflow/qnode.py:905\u001b[0m, in \u001b[0;36mQNode.__call__\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m    903\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m qml\u001b[38;5;241m.\u001b[39mcapture\u001b[38;5;241m.\u001b[39menabled():\n\u001b[1;32m    904\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m capture_qnode(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m--> 905\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_impl_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/.venv/lib/python3.13/site-packages/pennylane/workflow/qnode.py:868\u001b[0m, in \u001b[0;36mQNode._impl_call\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m    865\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21m_impl_call\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m qml\u001b[38;5;241m.\u001b[39mtyping\u001b[38;5;241m.\u001b[39mResult:\n\u001b[1;32m    866\u001b[0m \n\u001b[1;32m    867\u001b[0m     \u001b[38;5;66;03m# construct the tape\u001b[39;00m\n\u001b[0;32m--> 868\u001b[0m     tape \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mconstruct\u001b[49m\u001b[43m(\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    870\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39minterface \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mauto\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[1;32m    871\u001b[0m         interface \u001b[38;5;241m=\u001b[39m qml\u001b[38;5;241m.\u001b[39mmath\u001b[38;5;241m.\u001b[39mget_interface(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;28mlist\u001b[39m(kwargs\u001b[38;5;241m.\u001b[39mvalues()))\n",
      "File \u001b[0;32m~/.venv/lib/python3.13/site-packages/pennylane/logging/decorators.py:61\u001b[0m, in \u001b[0;36mlog_string_debug_func.<locals>.wrapper_entry\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     54\u001b[0m     s_caller \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m::L\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m.\u001b[39mjoin(\n\u001b[1;32m     55\u001b[0m         [\u001b[38;5;28mstr\u001b[39m(i) \u001b[38;5;28;01mfor\u001b[39;00m i \u001b[38;5;129;01min\u001b[39;00m inspect\u001b[38;5;241m.\u001b[39mgetouterframes(inspect\u001b[38;5;241m.\u001b[39mcurrentframe(), \u001b[38;5;241m2\u001b[39m)[\u001b[38;5;241m1\u001b[39m][\u001b[38;5;241m1\u001b[39m:\u001b[38;5;241m3\u001b[39m]]\n\u001b[1;32m     56\u001b[0m     )\n\u001b[1;32m     57\u001b[0m     lgr\u001b[38;5;241m.\u001b[39mdebug(\n\u001b[1;32m     58\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mCalling \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mf_string\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m from \u001b[39m\u001b[38;5;132;01m{\u001b[39;00ms_caller\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m,\n\u001b[1;32m     59\u001b[0m         \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39m_debug_log_kwargs,\n\u001b[1;32m     60\u001b[0m     )\n\u001b[0;32m---> 61\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/.venv/lib/python3.13/site-packages/pennylane/workflow/qnode.py:854\u001b[0m, in \u001b[0;36mQNode.construct\u001b[0;34m(self, args, kwargs)\u001b[0m\n\u001b[1;32m    852\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m pldb_device_manager(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdevice):\n\u001b[1;32m    853\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m qml\u001b[38;5;241m.\u001b[39mqueuing\u001b[38;5;241m.\u001b[39mAnnotatedQueue() \u001b[38;5;28;01mas\u001b[39;00m q:\n\u001b[0;32m--> 854\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_qfunc_output \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    856\u001b[0m tape \u001b[38;5;241m=\u001b[39m QuantumScript\u001b[38;5;241m.\u001b[39mfrom_queue(q, shots)\n\u001b[1;32m    858\u001b[0m params \u001b[38;5;241m=\u001b[39m tape\u001b[38;5;241m.\u001b[39mget_parameters(trainable_only\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m)\n",
      "Cell \u001b[0;32mIn[1], line 74\u001b[0m, in \u001b[0;36mcircuit\u001b[0;34m(weights, x)\u001b[0m\n\u001b[1;32m     72\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m W \u001b[38;5;129;01min\u001b[39;00m weights:\n\u001b[1;32m     73\u001b[0m     layer(W)\n\u001b[0;32m---> 74\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m qml\u001b[38;5;241m.\u001b[39mexpval(qml\u001b[38;5;241m.\u001b[39mHermitian(\u001b[43mZ_matrix\u001b[49m, wires\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mrange\u001b[39m(num_qubits)))\n",
      "\u001b[0;31mNameError\u001b[0m: name 'Z_matrix' is not defined"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import pennylane as qml\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "import time\n",
    "import math\n",
    "\n",
    "# === Load and Preprocess the Adult Dataset ===\n",
    "df_adult = pd.read_csv('Datasets/adult/adult_test_int.csv')\n",
    "\n",
    "# Drop the unnecessary index column\n",
    "df_adult = df_adult.drop(columns=[\"Unnamed: 0\"])\n",
    "\n",
    "# Select numerical features for the Quantum Model\n",
    "selected_features = [\"age\", \"capital.gain\", \"capital.loss\", \"hours.per.week\"]\n",
    "\n",
    "# Standardize numerical features\n",
    "scaler = StandardScaler()\n",
    "df_adult[selected_features] = scaler.fit_transform(df_adult[selected_features])\n",
    "\n",
    "# One-hot encode categorical features for Classical Model\n",
    "categorical_cols = [\"workclass\", \"education\", \"marital.status\", \"occupation\",\n",
    "                    \"relationship\", \"race\", \"sex\", \"native.country\"]\n",
    "df_adult = pd.get_dummies(df_adult, columns=categorical_cols, dtype=int)\n",
    "\n",
    "# Define Quantum Model input (only selected numerical features)\n",
    "X_quantum = df_adult[selected_features].values\n",
    "\n",
    "# Define Classical Model input (all features except target)\n",
    "X_classical = df_adult.drop(columns=[\"over50K\"]).values\n",
    "\n",
    "# Define Target Variables\n",
    "y = df_adult[\"over50K\"].values\n",
    "y_quantum = y * 2 - 1  # Convert to {-1,1} for quantum classifier\n",
    "\n",
    "# Split dataset into training and testing sets\n",
    "X_train_q, X_test_q, y_train_q, y_test_q = train_test_split(X_quantum, y_quantum, test_size=0.10, random_state=42, stratify=y)\n",
    "X_train_c, X_test_c, y_train_c, y_test_c = train_test_split(X_classical, y, test_size=0.10, random_state=42, stratify=y)\n",
    "\n",
    "# === Quantum Model Setup ===\n",
    "num_qubits = 4  # Using 4 selected numerical features\n",
    "num_layers = 3\n",
    "dev = qml.device(\"default.qubit\", wires=num_qubits)\n",
    "\n",
    "# Quantum Circuit\n",
    "def statepreparation(x):\n",
    "    qml.Hadamard(wires=0)\n",
    "    qml.Hadamard(wires=1)\n",
    "    qml.Hadamard(wires=2)\n",
    "    qml.Hadamard(wires=3)\n",
    "\n",
    "    qml.RZ(x[0] * np.pi, wires=0)\n",
    "    qml.RZ(x[1] * np.pi, wires=1)\n",
    "    qml.RZ(x[2] * np.pi, wires=2)\n",
    "    qml.RZ(x[3] * np.pi, wires=3)\n",
    "\n",
    "def layer(W):\n",
    "    for i in range(num_qubits):\n",
    "        qml.Rot(W[i, 0], W[i, 1], W[i, 2], wires=i)\n",
    "    for i in range(num_qubits - 1):\n",
    "        qml.CNOT(wires=[i, i + 1])\n",
    "    qml.CNOT(wires=[num_qubits - 1, 0])\n",
    "\n",
    "@qml.qnode(dev, interface=\"autograd\")\n",
    "def circuit(weights, x):\n",
    "    statepreparation(x)\n",
    "    for W in weights:\n",
    "        layer(W)\n",
    "    return qml.expval(qml.PauliZ(0))  # Measure Z on the first qubit\n",
    "\n",
    "def variational_classifier(weights, bias, x):\n",
    "    return circuit(weights, x) + bias\n",
    "\n",
    "# Loss and Accuracy Functions\n",
    "def square_loss(labels, predictions):\n",
    "    return qml.numpy.mean((qml.numpy.array(labels) - qml.numpy.array(predictions)) ** 2)\n",
    "\n",
    "def accuracy(labels, predictions):\n",
    "    return qml.numpy.mean(qml.numpy.abs(qml.numpy.array(labels) - qml.numpy.array(predictions)) < 1e-5)\n",
    "\n",
    "def cost(weights, bias, X, Y):\n",
    "    predictions = qml.numpy.array([variational_classifier(weights, bias, x) for x in X])\n",
    "    return square_loss(Y, predictions)\n",
    "\n",
    "# Initialize Quantum Model Parameters\n",
    "np.random.seed(0)\n",
    "weights_init = qml.numpy.tensor(0.01 * np.random.randn(num_layers, num_qubits, 3), requires_grad=True)\n",
    "bias_init = qml.numpy.tensor(0.0, requires_grad=True)\n",
    "\n",
    "opt = qml.optimize.AdamOptimizer(0.05)\n",
    "num_it = 10\n",
    "batch_size = min(48, len(X_train_q))  # Ensure batch_size does not exceed dataset size\n",
    "\n",
    "# === Debugging Helpers ===\n",
    "def debug_parameters(weights, bias, iteration):\n",
    "    print(f\"\\nIteration {iteration}: Parameter Check\")\n",
    "    print(f\"Weights Mean: {qml.numpy.mean(weights):.6f}, Std: {qml.numpy.std(weights):.6f}\")\n",
    "    print(f\"Bias: {bias:.6f}\")\n",
    "\n",
    "def debug_predictions(predictions, iteration):\n",
    "    unique_values = np.unique(predictions)\n",
    "    print(f\"Iteration {iteration}: Unique Prediction Values: {unique_values}\")\n",
    "    if len(unique_values) == 1:\n",
    "        print(\"WARNING: Model is outputting the same value for all inputs!\")\n",
    "\n",
    "# === Train Quantum Model (Debugging Version) ===\n",
    "start_time = time.time()\n",
    "weights, bias = weights_init, bias_init\n",
    "\n",
    "for it in range(num_it):\n",
    "    batch_index = np.random.choice(len(X_train_q), batch_size, replace=False)\n",
    "    X_batch, Y_batch = X_train_q[batch_index].astype(np.float64), y_train_q[batch_index]\n",
    "\n",
    "    # Track parameter values before update\n",
    "    debug_parameters(weights, bias, it)\n",
    "\n",
    "    # Optimization step\n",
    "    weights, bias = opt.step(lambda w, b: cost(w, b, X_batch, Y_batch), weights, bias)\n",
    "\n",
    "    # Compute training accuracy\n",
    "    predictions = np.array([qml.numpy.sign(variational_classifier(weights, bias, x)) for x in X_train_q])\n",
    "    acc = accuracy(y_train_q, predictions)\n",
    "\n",
    "    # Check if predictions are stuck at a single value\n",
    "    debug_predictions(predictions, it)\n",
    "\n",
    "    print(f\"Iter: {it+1:5d} | Cost: {cost(weights, bias, X_train_q, y_train_q):0.7f} | Accuracy: {acc:0.7f}\")\n",
    "\n",
    "print(f\"Total training time: {time.time() - start_time:.2f} seconds\")\n",
    "\n",
    "# === Quantum Model Evaluation (Debugging Version) ===\n",
    "predictions_q = np.array([\n",
    "    float(qml.numpy.sign(variational_classifier(weights, bias, x))) \n",
    "    for x in X_test_q\n",
    "])\n",
    "\n",
    "debug_predictions(predictions_q, \"Final Test Set\")\n",
    "\n",
    "print(\"\\nQuantum Model Performance:\")\n",
    "print(f\"Accuracy: {accuracy_score(y_test_q, predictions_q):.4f}\")\n",
    "print(f\"Precision: {precision_score(y_test_q, predictions_q, zero_division=1):.4f}\")\n",
    "print(f\"Recall: {recall_score(y_test_q, predictions_q, zero_division=1):.4f}\")\n",
    "print(f\"F1 Score: {f1_score(y_test_q, predictions_q, average='macro'):.4f}\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "3c1c4237-d9e0-4cca-a182-34f058a3d0c9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Classical Model Performance:\n",
      "Accuracy: 0.7978\n",
      "Precision: 0.7586\n",
      "Recall: 0.3143\n",
      "F1 Score: 0.6604\n"
     ]
    }
   ],
   "source": [
    "# Select same features for Classical Model\n",
    "selected_features = [\"age\", \"capital.gain\", \"capital.loss\", \"hours.per.week\"]\n",
    "X_classical = df_adult[selected_features].values  # Now same as Quantum model\n",
    "\n",
    "# Update Train/Test Split\n",
    "X_train_c, X_test_c, y_train_c, y_test_c = train_test_split(X_classical, y, test_size=0.10, random_state=42, stratify=y)\n",
    "\n",
    "# Define Classical ANN Model\n",
    "class ClassicalANN(nn.Module):\n",
    "    def __init__(self, layers):\n",
    "        super().__init__()\n",
    "        self.layers = nn.ModuleList([nn.Linear(layers[i], layers[i+1], dtype=torch.float64) for i in range(len(layers) - 1)])\n",
    "        self.sigmoid = nn.Sigmoid()\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = x.to(torch.float64)\n",
    "        for layer in self.layers[:-1]:\n",
    "            x = torch.relu(layer(x))\n",
    "        return self.sigmoid(self.layers[-1](x))\n",
    "\n",
    "# Train Classical Model with the same 4 features\n",
    "classical_model = ClassicalANN([4, 5, 1])  # Input layer now has 4 neurons\n",
    "optimizer_classical = optim.Adam(classical_model.parameters(), lr=0.01)\n",
    "\n",
    "def train_classical(model, optimizer, X_train, y_train, epochs=50):\n",
    "    y_train = torch.tensor(y_train.tolist(), dtype=torch.float64).reshape(-1, 1)\n",
    "    for epoch in range(epochs):\n",
    "        optimizer.zero_grad()\n",
    "        y_pred = model(torch.tensor(X_train.tolist(), dtype=torch.float64)).reshape(-1, 1)\n",
    "        loss = nn.BCELoss()(y_pred, y_train)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "train_classical(classical_model, optimizer_classical, X_train_c, y_train_c, epochs=50)\n",
    "\n",
    "# Evaluate Classical Model\n",
    "with torch.no_grad():\n",
    "    X_test_c_numeric = np.array(X_test_c, dtype=np.float64)\n",
    "    y_pred_classical = classical_model(torch.tensor(X_test_c_numeric, dtype=torch.float64)).reshape(-1, 1)\n",
    "    y_pred_classical = (y_pred_classical.numpy().flatten() > 0.5).astype(int)\n",
    "\n",
    "print(\"\\nClassical Model Performance:\")\n",
    "print(f\"Accuracy: {accuracy_score(y_test_c, y_pred_classical):.4f}\")\n",
    "print(f\"Precision: {precision_score(y_test_c, y_pred_classical):.4f}\")\n",
    "print(f\"Recall: {recall_score(y_test_c, y_pred_classical):.4f}\")\n",
    "print(f\"F1 Score: {f1_score(y_test_c, y_pred_classical, average='macro'):.4f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e588777c-22b4-4ca4-8e95-900f901e8813",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (venv)",
   "language": "python",
   "name": "venv"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
