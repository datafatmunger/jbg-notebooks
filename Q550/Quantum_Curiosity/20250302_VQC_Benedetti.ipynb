{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "465c64f0-fb2b-4bc7-b28a-766701680ee7",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import pennylane as qml\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "import time\n",
    "\n",
    "# === Load and Preprocess Data ===\n",
    "# df_adult = pd.read_csv('Datasets/adult/adult_test_int.csv')\n",
    "# df_adult = df_adult.drop(columns=[\"Unnamed: 0\"])\n",
    "df_adult = pd.read_csv('Datasets/No_show_test_int.csv')\n",
    "df_adult = df_adult.drop(columns=[\"Unnamed: 0\"])\n",
    "\n",
    "num_it = 50\n",
    "num_layers = 3\n",
    "#selected_features = [\"age\", \"capital.gain\", \"capital.loss\", \"hours.per.week\", \"education\"]\n",
    "#y = df_adult[\"over50K\"].values\n",
    "selected_features = [\"Age\", \"Gender\", \"Sms_Reminder\", \"AwaitingTime\"]\n",
    "y = df_adult[\"Show_Up\"].values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "9eff16e2-8186-41cb-8129-91132103e173",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iter:     1 | Cost: 1.5783359 | Accuracy: 0.3014444\n",
      "Iter:     2 | Cost: 1.4705731 | Accuracy: 0.3014444\n",
      "Iter:     3 | Cost: 1.3547277 | Accuracy: 0.3014444\n",
      "Iter:     4 | Cost: 1.2377902 | Accuracy: 0.3650370\n",
      "Iter:     5 | Cost: 1.1272282 | Accuracy: 0.4286296\n",
      "Iter:     6 | Cost: 1.0310845 | Accuracy: 0.4609630\n",
      "Iter:     7 | Cost: 0.9542702 | Accuracy: 0.5624444\n",
      "Iter:     8 | Cost: 0.9048373 | Accuracy: 0.6292593\n",
      "Iter:     9 | Cost: 0.8750897 | Accuracy: 0.6931852\n",
      "Iter:    10 | Cost: 0.8643566 | Accuracy: 0.6985556\n",
      "Iter:    11 | Cost: 0.8654881 | Accuracy: 0.6985556\n",
      "Iter:    12 | Cost: 0.8737837 | Accuracy: 0.6985556\n",
      "Iter:    13 | Cost: 0.8836415 | Accuracy: 0.6985556\n",
      "Iter:    14 | Cost: 0.8887784 | Accuracy: 0.6985556\n",
      "Iter:    15 | Cost: 0.8919498 | Accuracy: 0.6985556\n",
      "Iter:    16 | Cost: 0.8931842 | Accuracy: 0.6985556\n",
      "Iter:    17 | Cost: 0.8920793 | Accuracy: 0.6985556\n",
      "Iter:    18 | Cost: 0.8879608 | Accuracy: 0.6985556\n",
      "Iter:    19 | Cost: 0.8818256 | Accuracy: 0.6985556\n",
      "Iter:    20 | Cost: 0.8747003 | Accuracy: 0.6985556\n",
      "Iter:    21 | Cost: 0.8672897 | Accuracy: 0.6985556\n",
      "Iter:    22 | Cost: 0.8612759 | Accuracy: 0.6985556\n",
      "Iter:    23 | Cost: 0.8553492 | Accuracy: 0.6985556\n",
      "Iter:    24 | Cost: 0.8512624 | Accuracy: 0.6985556\n",
      "Iter:    25 | Cost: 0.8503188 | Accuracy: 0.6985556\n",
      "Iter:    26 | Cost: 0.8525906 | Accuracy: 0.6960741\n",
      "Iter:    27 | Cost: 0.8555415 | Accuracy: 0.6890000\n",
      "Iter:    28 | Cost: 0.8582658 | Accuracy: 0.6860370\n",
      "Iter:    29 | Cost: 0.8585325 | Accuracy: 0.6842593\n",
      "Iter:    30 | Cost: 0.8576745 | Accuracy: 0.6842963\n",
      "Iter:    31 | Cost: 0.8557962 | Accuracy: 0.6852963\n",
      "Iter:    32 | Cost: 0.8539561 | Accuracy: 0.6872222\n",
      "Iter:    33 | Cost: 0.8529874 | Accuracy: 0.6880741\n",
      "Iter:    34 | Cost: 0.8519303 | Accuracy: 0.6900000\n",
      "Iter:    35 | Cost: 0.8514792 | Accuracy: 0.6937407\n",
      "Iter:    36 | Cost: 0.8533440 | Accuracy: 0.6974815\n",
      "Iter:    37 | Cost: 0.8563794 | Accuracy: 0.6985556\n",
      "Iter:    38 | Cost: 0.8595422 | Accuracy: 0.6985556\n",
      "Iter:    39 | Cost: 0.8613330 | Accuracy: 0.6985556\n",
      "Iter:    40 | Cost: 0.8654068 | Accuracy: 0.6985556\n",
      "Iter:    41 | Cost: 0.8671479 | Accuracy: 0.6985556\n",
      "Iter:    42 | Cost: 0.8679687 | Accuracy: 0.6985556\n",
      "Iter:    43 | Cost: 0.8654417 | Accuracy: 0.6985556\n",
      "Iter:    44 | Cost: 0.8612648 | Accuracy: 0.6985556\n",
      "Iter:    45 | Cost: 0.8593986 | Accuracy: 0.6985556\n",
      "Iter:    46 | Cost: 0.8576946 | Accuracy: 0.6985556\n",
      "Iter:    47 | Cost: 0.8565945 | Accuracy: 0.6985556\n",
      "Iter:    48 | Cost: 0.8559466 | Accuracy: 0.6985556\n",
      "Iter:    49 | Cost: 0.8550612 | Accuracy: 0.6985556\n",
      "Iter:    50 | Cost: 0.8545302 | Accuracy: 0.6985556\n",
      "\n",
      "Quantum Model Performance:\n",
      "Accuracy: 0.6987\n",
      "Precision: 0.6987\n",
      "Recall: 1.0000\n",
      "F1 Score: 0.4113\n"
     ]
    }
   ],
   "source": [
    "scaler = StandardScaler()\n",
    "df_adult[selected_features] = scaler.fit_transform(df_adult[selected_features])\n",
    "\n",
    "X_quantum = df_adult[selected_features].values\n",
    "y_quantum = y * 2 - 1  # Convert to {-1,1} for quantum classifier\n",
    "\n",
    "X_train_q, X_test_q, y_train_q, y_test_q = train_test_split(\n",
    "    X_quantum, y_quantum, test_size=0.10, random_state=42, stratify=y\n",
    ")\n",
    "\n",
    "# === Quantum Model Setup ===\n",
    "num_qubits = len(selected_features)  # Update number of qubits to match feature count\n",
    "dev = qml.device(\"default.qubit\", wires=num_qubits)\n",
    "\n",
    "# Benedetti-style feature encoding\n",
    "def feature_encoding(x):\n",
    "    for i in range(num_qubits):\n",
    "        qml.RY(np.pi * x[i], wires=i)  # Use RY encoding instead of simple RZ\n",
    "    for i in range(num_qubits - 1):\n",
    "        qml.CZ(wires=[i, i + 1])  # Introduce entanglement\n",
    "\n",
    "# Variational Layer from Benedetti et al.\n",
    "def variational_layer(W):\n",
    "    for i in range(num_qubits):\n",
    "        qml.Rot(W[i, 0], W[i, 1], W[i, 2], wires=i)\n",
    "    for i in range(num_qubits - 1):\n",
    "        qml.CNOT(wires=[i, i + 1])  # Alternating CNOT layers\n",
    "    qml.CNOT(wires=[num_qubits - 1, 0])\n",
    "\n",
    "@qml.qnode(dev, interface=\"autograd\")\n",
    "def circuit(weights, x):\n",
    "    feature_encoding(x)\n",
    "    for W in weights:\n",
    "        variational_layer(W)\n",
    "    return qml.expval(qml.PauliZ(0))\n",
    "\n",
    "def variational_classifier(weights, bias, x):\n",
    "    return circuit(weights, x) + bias\n",
    "\n",
    "# === Training Setup ===\n",
    "def cost(weights, bias, X, Y):\n",
    "    predictions = qml.numpy.array([variational_classifier(weights, bias, x) for x in X])\n",
    "    return qml.numpy.mean((qml.numpy.array(Y) - predictions) ** 2)\n",
    "\n",
    "np.random.seed(0)\n",
    "weights_init = qml.numpy.tensor(0.01 * np.random.randn(num_layers, num_qubits, 3), requires_grad=True)\n",
    "bias_init = qml.numpy.tensor(0.0, requires_grad=True)\n",
    "\n",
    "opt = qml.optimize.AdamOptimizer(0.05)\n",
    "batch_size = min(48, len(X_train_q))\n",
    "\n",
    "# === Train Quantum Model ===\n",
    "weights, bias = weights_init, bias_init\n",
    "\n",
    "for it in range(num_it):\n",
    "    batch_index = np.random.choice(len(X_train_q), batch_size, replace=False)\n",
    "    X_batch, Y_batch = X_train_q[batch_index].astype(np.float64), y_train_q[batch_index]\n",
    "\n",
    "    weights, bias = opt.step(lambda w, b: cost(w, b, X_batch, Y_batch), weights, bias)\n",
    "\n",
    "    predictions = np.array([qml.numpy.sign(variational_classifier(weights, bias, x)) for x in X_train_q])\n",
    "    acc = np.mean(np.abs(y_train_q - predictions) < 1e-5)\n",
    "    \n",
    "    print(f\"Iter: {it+1:5d} | Cost: {cost(weights, bias, X_train_q, y_train_q):0.7f} | Accuracy: {acc:0.7f}\")\n",
    "\n",
    "# Recompute predictions using the trained quantum model\n",
    "predictions_q = np.array([\n",
    "    float(qml.numpy.sign(variational_classifier(weights, bias, x))) \n",
    "    for x in X_test_q\n",
    "])\n",
    "\n",
    "# Compute evaluation metrics\n",
    "accuracy = accuracy_score(y_test_q, predictions_q)\n",
    "precision = precision_score(y_test_q, predictions_q, zero_division=1)\n",
    "recall = recall_score(y_test_q, predictions_q, zero_division=1)\n",
    "f1 = f1_score(y_test_q, predictions_q, average='macro')\n",
    "\n",
    "# Print results\n",
    "print(\"\\nQuantum Model Performance:\")\n",
    "print(f\"Accuracy: {accuracy:.4f}\")\n",
    "print(f\"Precision: {precision:.4f}\")\n",
    "print(f\"Recall: {recall:.4f}\")\n",
    "print(f\"F1 Score: {f1:.4f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "3c1c4237-d9e0-4cca-a182-34f058a3d0c9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iter:     1 | Cost: 0.6483373 | Accuracy: 0.6985556\n",
      "Iter:     2 | Cost: 0.6454453 | Accuracy: 0.6985556\n",
      "Iter:     3 | Cost: 0.6429669 | Accuracy: 0.6985556\n",
      "Iter:     4 | Cost: 0.6408171 | Accuracy: 0.6985556\n",
      "Iter:     5 | Cost: 0.6389565 | Accuracy: 0.6985556\n",
      "Iter:     6 | Cost: 0.6373933 | Accuracy: 0.6985556\n",
      "Iter:     7 | Cost: 0.6359890 | Accuracy: 0.6985556\n",
      "Iter:     8 | Cost: 0.6346891 | Accuracy: 0.6985556\n",
      "Iter:     9 | Cost: 0.6334662 | Accuracy: 0.6985556\n",
      "Iter:    10 | Cost: 0.6322850 | Accuracy: 0.6985556\n",
      "Iter:    11 | Cost: 0.6311238 | Accuracy: 0.6985556\n",
      "Iter:    12 | Cost: 0.6299668 | Accuracy: 0.6985556\n",
      "Iter:    13 | Cost: 0.6288207 | Accuracy: 0.6985556\n",
      "Iter:    14 | Cost: 0.6276883 | Accuracy: 0.6985556\n",
      "Iter:    15 | Cost: 0.6264862 | Accuracy: 0.6985556\n",
      "Iter:    16 | Cost: 0.6252083 | Accuracy: 0.6985556\n",
      "Iter:    17 | Cost: 0.6239148 | Accuracy: 0.6985556\n",
      "Iter:    18 | Cost: 0.6225739 | Accuracy: 0.6985556\n",
      "Iter:    19 | Cost: 0.6212085 | Accuracy: 0.6985556\n",
      "Iter:    20 | Cost: 0.6198506 | Accuracy: 0.6985556\n",
      "Iter:    21 | Cost: 0.6185232 | Accuracy: 0.6985556\n",
      "Iter:    22 | Cost: 0.6172376 | Accuracy: 0.6985556\n",
      "Iter:    23 | Cost: 0.6160070 | Accuracy: 0.6985556\n",
      "Iter:    24 | Cost: 0.6148520 | Accuracy: 0.6985556\n",
      "Iter:    25 | Cost: 0.6137742 | Accuracy: 0.6985556\n",
      "Iter:    26 | Cost: 0.6127591 | Accuracy: 0.6985556\n",
      "Iter:    27 | Cost: 0.6118248 | Accuracy: 0.6985556\n",
      "Iter:    28 | Cost: 0.6110044 | Accuracy: 0.6985556\n",
      "Iter:    29 | Cost: 0.6102786 | Accuracy: 0.6985556\n",
      "Iter:    30 | Cost: 0.6096426 | Accuracy: 0.6985556\n",
      "Iter:    31 | Cost: 0.6091331 | Accuracy: 0.6985556\n",
      "Iter:    32 | Cost: 0.6087493 | Accuracy: 0.6985556\n",
      "Iter:    33 | Cost: 0.6084704 | Accuracy: 0.6985556\n",
      "Iter:    34 | Cost: 0.6082609 | Accuracy: 0.6985556\n",
      "Iter:    35 | Cost: 0.6080643 | Accuracy: 0.6985556\n",
      "Iter:    36 | Cost: 0.6078353 | Accuracy: 0.6985556\n",
      "Iter:    37 | Cost: 0.6075411 | Accuracy: 0.6985556\n",
      "Iter:    38 | Cost: 0.6071809 | Accuracy: 0.6985556\n",
      "Iter:    39 | Cost: 0.6067708 | Accuracy: 0.6985556\n",
      "Iter:    40 | Cost: 0.6063322 | Accuracy: 0.6985556\n",
      "Iter:    41 | Cost: 0.6058955 | Accuracy: 0.6985556\n",
      "Iter:    42 | Cost: 0.6054690 | Accuracy: 0.6985556\n",
      "Iter:    43 | Cost: 0.6050638 | Accuracy: 0.6985556\n",
      "Iter:    44 | Cost: 0.6046935 | Accuracy: 0.6985556\n",
      "Iter:    45 | Cost: 0.6043742 | Accuracy: 0.6985556\n",
      "Iter:    46 | Cost: 0.6041017 | Accuracy: 0.6985556\n",
      "Iter:    47 | Cost: 0.6038624 | Accuracy: 0.6985556\n",
      "Iter:    48 | Cost: 0.6036457 | Accuracy: 0.6985556\n",
      "Iter:    49 | Cost: 0.6034679 | Accuracy: 0.6985556\n",
      "Iter:    50 | Cost: 0.6033049 | Accuracy: 0.6985556\n",
      "\n",
      "Classical Model Performance:\n",
      "Accuracy: 0.6987\n",
      "Precision: 0.6987\n",
      "Recall: 1.0000\n",
      "F1 Score: 0.4113\n"
     ]
    }
   ],
   "source": [
    "# === Classical Model Setup ===\n",
    "\n",
    "# Select same features for Classical Model\n",
    "X_classical = df_adult[selected_features].values  # Matches Quantum Model\n",
    "\n",
    "# Update Train/Test Split\n",
    "X_train_c, X_test_c, y_train_c, y_test_c = train_test_split(\n",
    "    X_classical, y, test_size=0.10, random_state=42, stratify=y\n",
    ")\n",
    "\n",
    "# Define Classical ANN Model\n",
    "class ClassicalANN(nn.Module):\n",
    "    def __init__(self, input_size, num_layers, hidden_size=5):\n",
    "        super().__init__()\n",
    "        layers = [input_size] + [hidden_size] * (num_layers - 1) + [1]  # Dynamic layer sizes\n",
    "        self.layers = nn.ModuleList([nn.Linear(layers[i], layers[i+1], dtype=torch.float64) for i in range(len(layers) - 1)])\n",
    "        self.sigmoid = nn.Sigmoid()\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = x.to(torch.float64)\n",
    "        for layer in self.layers[:-1]:\n",
    "            x = torch.relu(layer(x))\n",
    "        return self.sigmoid(self.layers[-1](x))\n",
    "\n",
    "# Train Classical Model\n",
    "input_size = len(selected_features)  # Automatically adjust input size\n",
    "classical_model = ClassicalANN(input_size, num_layers)  # Uses num_layers for hidden depth\n",
    "optimizer_classical = optim.Adam(classical_model.parameters(), lr=0.01)\n",
    "\n",
    "def train_classical(model, optimizer, X_train, y_train, epochs=50):\n",
    "    y_train = torch.tensor(y_train.tolist(), dtype=torch.float64).reshape(-1, 1)\n",
    "    \n",
    "    for epoch in range(epochs):\n",
    "        optimizer.zero_grad()\n",
    "        y_pred = model(torch.tensor(X_train.tolist(), dtype=torch.float64)).reshape(-1, 1)\n",
    "        \n",
    "        # Compute loss\n",
    "        loss = nn.BCELoss()(y_pred, y_train)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        # Compute training accuracy\n",
    "        y_pred_binary = (y_pred.detach().numpy().flatten() > 0.5).astype(int)\n",
    "        acc = accuracy_score(y_train.numpy().flatten(), y_pred_binary)\n",
    "        \n",
    "        # Debug output\n",
    "        print(f\"Iter: {epoch+1:5d} | Cost: {loss.item():0.7f} | Accuracy: {acc:0.7f}\")\n",
    "\n",
    "# Train the Classical Model with Debugging Output\n",
    "train_classical(classical_model, optimizer_classical, X_train_c, y_train_c, epochs=num_it)\n",
    "\n",
    "# === Evaluate Classical Model ===\n",
    "with torch.no_grad():\n",
    "    X_test_c_numeric = np.array(X_test_c, dtype=np.float64)\n",
    "    y_pred_classical = classical_model(torch.tensor(X_test_c_numeric, dtype=torch.float64)).reshape(-1, 1)\n",
    "    y_pred_classical = (y_pred_classical.numpy().flatten() > 0.5).astype(int)\n",
    "\n",
    "print(\"\\nClassical Model Performance:\")\n",
    "print(f\"Accuracy: {accuracy_score(y_test_c, y_pred_classical):.4f}\")\n",
    "print(f\"Precision: {precision_score(y_test_c, y_pred_classical, zero_division=1):.4f}\")\n",
    "print(f\"Recall: {recall_score(y_test_c, y_pred_classical, zero_division=1):.4f}\")\n",
    "print(f\"F1 Score: {f1_score(y_test_c, y_pred_classical, average='macro'):.4f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "00d3bef5-3e6a-4958-b37f-2353feb63943",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (venv)",
   "language": "python",
   "name": "venv"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
