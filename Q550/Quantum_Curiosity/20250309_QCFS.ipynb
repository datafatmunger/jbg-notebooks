{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    },
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "#### 1. Importing all the libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "import shutil\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from matplotlib.collections import LineCollection\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import pennylane as qml\n",
    "\n",
    "from sklearn import tree, metrics\n",
    "from sklearn.model_selection import train_test_split, cross_val_score\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.ensemble import AdaBoostClassifier, GradientBoostingClassifier, ExtraTreesClassifier\n",
    "\n",
    "import pennylane as qml\n",
    "\n",
    "import time\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "#### 2. Define the Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "# Function that create the episode data - sample randomaly\n",
    "def get_data(episode_size,policy,mode):\n",
    "    global dataset\n",
    "    if mode=='train':\n",
    "        if policy==0:\n",
    "             dataset=data.sample(n=episode_size)\n",
    "        else:\n",
    "            dataset=data\n",
    "    else:\n",
    "        dataset = pd.read_csv(location + '/' + file +'_test_int.csv', index_col=0)\n",
    "    return dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "# Function that separate the episode data into features and label\n",
    "def data_separate (dataset):\n",
    "    global X\n",
    "    global y    \n",
    "    X = dataset.iloc[:,0:dataset.shape[1]-1]  # all rows, all the features and no labels\n",
    "    y = dataset.iloc[:, -1]  # all rows, label only\n",
    "    return X,y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "# Function that split the episode data into train and test\n",
    "def data_split(X,y):\n",
    "    global X_train_main\n",
    "    global X_test_main   \n",
    "    global y_train\n",
    "    global y_test  \n",
    "    from sklearn.model_selection import train_test_split\n",
    "    X_train_main, X_test_main, y_train, y_test = train_test_split(X,y,test_size=0.2,random_state=4)\n",
    "    return X_train_main, X_test_main, y_train, y_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "# Function that chooses exploration or explotation method\n",
    "def exploration_explotation(epsilon):\n",
    "    global exploration \n",
    "    if np.random.rand() < epsilon:  \n",
    "        exploration=1\n",
    "    else:\n",
    "        exploration=0    \n",
    "    return exploration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "# Function that returns all available actions in the state given as an argument: \n",
    "def available_actions(number_of_columns,columns,initial_state,current_state,trashold, exploration):\n",
    "    global exclude\n",
    "    global all_columns\n",
    "#    exclude=[]\n",
    "    all_columns=np.arange(number_of_columns+1)\n",
    "    # remove columns that have been already selected\n",
    "    exclude=columns.copy()\n",
    "    # remove the initial_state and the current_state\n",
    "    exclude.extend([initial_state, current_state])\n",
    "    available_act = list(set(all_columns)-set(exclude))\n",
    "    # remove actions that have negetiv Q value\n",
    "    if exploration==0:\n",
    "        index = np.where(Q[current_state,available_act] > trashold)[1]\n",
    "        available_act= [available_act[i] for i in index.tolist()]\n",
    "    return available_act"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "def sample_next_action(current_state, Q, available_act, exploration):\n",
    "    global available_act_q_value\n",
    "    available_act_q_value = [float(q) for q in np.array(Q[current_state, available_act]).reshape(-1)]\n",
    "    \n",
    "    if exploration == 1: \n",
    "        # Random selection\n",
    "        next_action = int(np.random.choice(available_act, 1).item())\n",
    "    else: \n",
    "        # Greedy selection according to max value\n",
    "        maxQ = max(available_act_q_value)\n",
    "        count = available_act_q_value.count(maxQ)\n",
    "        \n",
    "        if count > 1:\n",
    "            max_columns = [i for i in range(len(available_act_q_value)) if available_act_q_value[i] == maxQ]\n",
    "            i = int(np.random.choice(max_columns, 1).item())\n",
    "        else:\n",
    "            i = available_act_q_value.index(maxQ)\n",
    "        \n",
    "        next_action = available_act[i]  \n",
    "    \n",
    "    return next_action\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "# function that update a list with all selected columns in the episode\n",
    "def update_columns(action, columns):\n",
    "    update_columns=columns\n",
    "    update_columns.append(action)\n",
    "    return update_columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "# function that update the X_train and X_test according to the current episode columns list \n",
    "def update_X_train_X_test(columns,X_train_main, X_test_main):\n",
    "    X_train=X_train_main.iloc[:,columns]\n",
    "    X_test=X_test_main.iloc[:,columns]\n",
    "    X_train=pd.DataFrame(X_train)\n",
    "    X_test=pd.DataFrame(X_test)\n",
    "    return X_train, X_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "# Function that run the learner and get the error to the current episode columns list\n",
    "def Learner(X_train, X_test,y_train, y_test):\n",
    "    global learner\n",
    "    global y_pred\n",
    "    if learner_model == 'DT':\n",
    "        learner = tree.DecisionTreeClassifier()\n",
    "        learner = learner.fit(X_train, y_train)\n",
    "        y_pred = learner.predict(X_test)\n",
    "    elif learner_model == 'KNN':\n",
    "        learner = KNeighborsClassifier(metric='hamming',n_neighbors=5)\n",
    "        learner = learner.fit(X_train, y_train)\n",
    "        y_pred = learner.predict(X_test)        \n",
    "    elif learner_model == 'SVM':\n",
    "        learner = SVC()\n",
    "        learner = learner.fit(X_train, y_train)\n",
    "        y_pred = learner.predict(X_test)        \n",
    "    elif learner_model == 'NB':\n",
    "        learner = MultinomialNB()\n",
    "        learner = learner.fit(X_train, y_train)\n",
    "        y_pred = learner.predict(X_test)\n",
    "    elif learner_model == 'AB':\n",
    "        learner = AdaBoostClassifier()\n",
    "        learner = learner.fit(X_train, y_train)\n",
    "        y_pred = learner.predict(X_test)  \n",
    "    elif learner_model == 'GB':\n",
    "        learner = GradientBoostingClassifier()\n",
    "        learner = learner.fit(X_train, y_train)\n",
    "        y_pred = learner.predict(X_test)  \n",
    "    elif learner_model == 'VQC':\n",
    "        learner = QuantumLearner()\n",
    "        learner = learner.fit(X_train, y_train)\n",
    "        y_pred = learner.predict(X_test)  \n",
    "    elif learner_model == 'ANN':\n",
    "        learner = ClassicalLearner()\n",
    "        learner = learner.fit(X_train, y_train)\n",
    "        y_pred = learner.predict(X_test)  \n",
    "    accuracy=metrics.accuracy_score(y_test, y_pred)\n",
    "    error=1-accuracy\n",
    "    return error\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "def q_update(current_state, action, learning_rate, reward):\n",
    "    # next_state = current action\n",
    "    max_index = np.where(Q[action,] == np.max(Q[action,]))[0]  # Use [0] instead of [1] for 1D arrays\n",
    "    \n",
    "    if max_index.shape[0] > 1:\n",
    "        # Resolve tie by selecting one randomly\n",
    "        max_index = int(np.random.choice(max_index, size=1).item())\n",
    "    else:\n",
    "        max_index = int(max_index[0])  # Convert the first element to a scalar\n",
    "\n",
    "    max_value = Q[action, max_index]\n",
    "\n",
    "    # Update the Q matrix\n",
    "    if Q[current_state, action] == 1:\n",
    "        Q[current_state, action] = learning_rate * reward\n",
    "    else:\n",
    "        Q[current_state, action] = Q[current_state, action] + learning_rate * (\n",
    "            reward + (discount_factor * max_value) - Q[current_state, action]\n",
    "        )\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Experiment mangment"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "#### 3. Define the parameters "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "## for run time ##\n",
    "N_features=5\n",
    "N_data=1\n",
    "## for run time ##\n",
    "\n",
    "#Experiment: \n",
    "experiment='test'\n",
    "number_of_experiment=1\n",
    "\n",
    "# Dataset parameters #\n",
    "location = 'Datasets/adult'\n",
    "outputlocation='Datasets'\n",
    "file='adult' #adult #diabetic_data #no_show\n",
    "#np.random.seed(3)\n",
    "\n",
    "# Q learning parameter # \n",
    "learning_rate=0.005\n",
    "discount_factor = 0.01 #0\n",
    "epsilon = 0.1\n",
    "\n",
    "# Learner and episode parameters #\n",
    "learner_model = 'VQC' #DT #KNN #SVM\n",
    "episode_size=10\n",
    "internal_trashold=0\n",
    "external_trashold=0\n",
    "filename= file +'_int.csv'\n",
    "\n",
    "#Experiments folder management: \n",
    "#if not os.path.exists('/Experiments'):\n",
    "#    os.makedirs('/Experiments') \n",
    "if not os.path.exists('Experiments/'+ str(experiment)):\n",
    "    os.makedirs('Experiments/'+ str(experiment))\n",
    "else:\n",
    "    shutil.rmtree('Experiments/'+ str(experiment))          #removes all the subdirectories!\n",
    "    os.makedirs('Experiments/'+ str(experiment))\n",
    "#writer = pd.ExcelWriter('Experiments/'+ str(experiment) + '/df.xlsx') \n",
    "\n",
    "\n",
    "\n",
    "text_file = open('Experiments/'+ str(experiment) +'/parameters.txt', \"w\")\n",
    "text_file.write('experiment: ' + str(experiment)+ '\\n')\n",
    "text_file.write('number of experiments: ' + str(number_of_experiment)+ '\\n')\n",
    "text_file.write('file: ' + str(file)+ '\\n')\n",
    "text_file.write('learner model: ' + str(learner_model)+ '\\n')\n",
    "text_file.write('episode size: ' + str(episode_size)+ '\\n')\n",
    "#text_file.write('numbers of epocs: ' + str(epocs)+ '\\n')\n",
    "text_file.write('internal trashold: ' + str(internal_trashold)+ '\\n')\n",
    "text_file.write('external trashold: ' + str(external_trashold)+ '\\n')\n",
    " \n",
    "text_file.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Classical Learner based on a simple ANN\n",
    "class ClassicalLearner(nn.Module):\n",
    "    def __init__(self, num_layers=2, hidden_size=5):\n",
    "        super().__init__()\n",
    "        self.layers = None\n",
    "        self.sigmoid = nn.Sigmoid()\n",
    "\n",
    "    def initialize_layers(self, input_size, num_layers, hidden_size=5):\n",
    "        layers = [input_size] + [hidden_size] * (num_layers - 1) + [1]\n",
    "        self.layers = nn.ModuleList([nn.Linear(layers[i], layers[i+1], dtype=torch.float64) for i in range(len(layers) - 1)])\n",
    "    \n",
    "    def forward(self, x):\n",
    "        x = x.to(torch.float64)\n",
    "        for layer in self.layers[:-1]:\n",
    "            x = torch.relu(layer(x))\n",
    "        return self.sigmoid(self.layers[-1](x))\n",
    "    \n",
    "    def fit(self, X_train, y_train, num_it=50, lr=0.01):\n",
    "        input_size = X_train.shape[1]\n",
    "        self.initialize_layers(input_size, num_layers=2)\n",
    "        optimizer = optim.Adam(self.parameters(), lr=lr)\n",
    "        y_train = torch.tensor(y_train.values, dtype=torch.float64).reshape(-1, 1)\n",
    "        \n",
    "        for epoch in range(num_it):\n",
    "            optimizer.zero_grad()\n",
    "            y_pred = self.forward(torch.tensor(X_train.values, dtype=torch.float64)).reshape(-1, 1)\n",
    "            loss = nn.BCELoss()(y_pred, y_train)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "        \n",
    "        return self\n",
    "    \n",
    "    def predict(self, X_test):\n",
    "        with torch.no_grad():\n",
    "            X_test_tensor = torch.tensor(X_test.values, dtype=torch.float64)\n",
    "            y_pred = self.forward(X_test_tensor).reshape(-1, 1)\n",
    "            return (y_pred.numpy().flatten() > 0.5).astype(int)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Quantum Learner based on Benedetti et al.\n",
    "class QuantumLearner:\n",
    "    def __init__(self, num_layers=2):\n",
    "        self.num_layers = num_layers\n",
    "        self.weights = None\n",
    "        self.bias = None\n",
    "        self.num_qubits = None\n",
    "        self.dev = None\n",
    "        self.opt = qml.optimize.AdamOptimizer(0.05)\n",
    "\n",
    "    def _initialize_circuit(self, num_features):\n",
    "        # Update the number of qubits to match the current feature count.\n",
    "        self.num_qubits = num_features\n",
    "        #self.dev = qml.device('qiskit.aer', wires=list(range(self.num_qubits)))\n",
    "        self.dev = qml.device(\"default.qubit\", wires=list(range(self.num_qubits)))\n",
    "\n",
    "        @qml.qnode(self.dev, interface=\"autograd\")\n",
    "        def circuit(weights, x):\n",
    "            self.feature_encoding(x)\n",
    "            for W in weights:\n",
    "                self.variational_layer(W)\n",
    "            return qml.expval(qml.PauliZ(0))\n",
    "        \n",
    "        self.circuit = circuit\n",
    "\n",
    "    def feature_encoding(self, x):\n",
    "        for i in range(self.num_qubits):\n",
    "            qml.RY(np.pi * x[i], wires=i)\n",
    "        for i in range(self.num_qubits - 1):\n",
    "            qml.CZ(wires=[i, i + 1])\n",
    "\n",
    "    def variational_layer(self, W):\n",
    "        for i in range(self.num_qubits):\n",
    "            qml.Rot(W[i, 0], W[i, 1], W[i, 2], wires=i)\n",
    "        for i in range(self.num_qubits - 1):\n",
    "            qml.CNOT(wires=[i, i + 1])\n",
    "        if self.num_qubits > 1:\n",
    "            qml.CNOT(wires=[self.num_qubits - 1, 0])\n",
    "\n",
    "    def variational_classifier(self, weights, bias, x):\n",
    "        return self.circuit(weights, x) + bias\n",
    "\n",
    "    def cost(self, weights, bias, X, Y):\n",
    "        # X is expected to be a NumPy array.\n",
    "        predictions = qml.numpy.array([self.variational_classifier(weights, bias, x) for x in X])\n",
    "        return qml.numpy.mean((qml.numpy.array(Y) - predictions) ** 2)\n",
    "\n",
    "    def fit(self, X_train, y_train, num_it=10, batch_size=48, warm_start=False):\n",
    "        \"\"\"\n",
    "        Train the QuantumLearner with debug logging to track performance bottlenecks.\n",
    "        \"\"\"\n",
    "        print(\"ðŸš€ [QuantumLearner] Starting fit function...\")\n",
    "    \n",
    "        # Convert inputs to NumPy arrays if needed.\n",
    "        if hasattr(X_train, \"values\"):\n",
    "            X_train = X_train.values.astype(np.float64)\n",
    "        else:\n",
    "            X_train = np.array(X_train, dtype=np.float64)\n",
    "        if hasattr(y_train, \"values\"):\n",
    "            y_train = y_train.values\n",
    "        else:\n",
    "            y_train = np.array(y_train)\n",
    "    \n",
    "        print(f\"ðŸ“Š Dataset size: X_train={X_train.shape}, y_train={y_train.shape}\")\n",
    "    \n",
    "        current_features = X_train.shape[1]\n",
    "    \n",
    "        # If not warm starting or if the number of features has changed, reinitialize the circuit.\n",
    "        if not warm_start or (self.num_qubits is None) or (self.num_qubits != current_features):\n",
    "            print(\"ðŸ”„ Reinitializing circuit...\")\n",
    "            self._initialize_circuit(current_features)\n",
    "            np.random.seed(0)\n",
    "            self.weights = qml.numpy.tensor(\n",
    "                0.01 * np.random.randn(self.num_layers, self.num_qubits, 3),\n",
    "                requires_grad=True,\n",
    "            )\n",
    "            self.bias = qml.numpy.tensor(0.0, requires_grad=True)\n",
    "            print(f\"âœ… Initialized circuit with {self.num_qubits} qubits\")\n",
    "    \n",
    "        batch_size = min(batch_size, len(X_train))\n",
    "        print(f\"ðŸ“Œ Using batch size: {batch_size}\")\n",
    "    \n",
    "        # Track total training time\n",
    "        start_time = time.time()\n",
    "    \n",
    "        for it in range(num_it):\n",
    "            #print(f\"â³ Epoch {it+1}/{num_it}\")\n",
    "    \n",
    "            batch_start = time.time()\n",
    "            batch_index = np.random.choice(len(X_train), batch_size, replace=False)\n",
    "            X_batch = X_train[batch_index]\n",
    "            Y_batch = y_train[batch_index]\n",
    "            batch_time = time.time() - batch_start\n",
    "            #print(f\"ðŸ›  Batch selection completed in {batch_time:.2f} seconds\")\n",
    "    \n",
    "            # Time the optimization step\n",
    "            opt_start = time.time()\n",
    "            self.weights, self.bias = self.opt.step(\n",
    "                lambda w, b: self.cost(w, b, X_batch, Y_batch), self.weights, self.bias\n",
    "            )\n",
    "            opt_time = time.time() - opt_start\n",
    "            #print(f\"âœ… Optimization step completed in {opt_time:.2f} seconds\")\n",
    "    \n",
    "        total_time = time.time() - start_time\n",
    "        print(f\"ðŸš€ Training completed in {total_time:.2f} seconds\")\n",
    "    \n",
    "        return self\n",
    "\n",
    "\n",
    "    def predict(self, X_test):\n",
    "        if hasattr(X_test, \"values\"):\n",
    "            X_test = X_test.values.astype(np.float64)\n",
    "        else:\n",
    "            X_test = np.array(X_test, dtype=np.float64)\n",
    "        return np.array([\n",
    "            float(qml.numpy.sign(self.variational_classifier(self.weights, self.bias, x)))\n",
    "            for x in X_test\n",
    "        ])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "class QuantumFeatureSelection:\n",
    "    def __init__(self, num_features, learning_rate=0.1):\n",
    "        self.num_features = num_features  # Number of possible features\n",
    "        self.learning_rate = learning_rate  # Learning rate\n",
    "        #self.device = qml.device(\"lightning.gpu\", wires=self.num_features)\n",
    "        self.device = qml.device(\"default.qubit\", wires=self.num_features)\n",
    "        self.state = self.initialize_state()\n",
    "\n",
    "    def initialize_state(self):\n",
    "        \"\"\"Initialize the quantum state as an equal superposition over all features.\"\"\"\n",
    "        @qml.qnode(self.device)\n",
    "        def circuit():\n",
    "            for qubit in range(self.num_features):\n",
    "                qml.Hadamard(wires=qubit)  # Create equal superposition of all features\n",
    "            return qml.probs(wires=range(self.num_features))  # Get probability distribution\n",
    "        \n",
    "        state_vector = np.array(circuit())  # Convert to NumPy array\n",
    "        state_vector = state_vector[:self.num_features]  # Extract relevant part\n",
    "        state_vector /= state_vector.sum()  # Normalize to ensure probability sum is 1\n",
    "        return state_vector\n",
    "\n",
    "    def measure(self, available_features):\n",
    "        \"\"\"Measure the quantum state and return a valid feature (action).\"\"\"\n",
    "        probabilities = self.state  # Already a probability distribution\n",
    "    \n",
    "        if not available_features:\n",
    "            raise ValueError(\"No available features for selection.\")\n",
    "    \n",
    "        # Only consider the probabilities of available features\n",
    "        valid_probs = np.array([probabilities[i] for i in available_features], dtype=np.float64)\n",
    "    \n",
    "        print(f\"ðŸ”Ž Available features: {available_features}\")\n",
    "        print(f\"ðŸ§ Raw valid_probs before normalization: {valid_probs}\")\n",
    "    \n",
    "        # Normalize the probabilities\n",
    "        prob_sum = valid_probs.sum()\n",
    "        if prob_sum == 0:\n",
    "            print(\"âš ï¸ Warning: All probabilities are zero, using uniform distribution.\")\n",
    "            valid_probs = np.ones(len(available_features), dtype=np.float64) / len(available_features)\n",
    "        else:\n",
    "            valid_probs /= prob_sum\n",
    "    \n",
    "        print(f\"âœ… Valid probabilities after normalization: {valid_probs}\")\n",
    "    \n",
    "        # Ensure arrays have matching lengths\n",
    "        if len(valid_probs) != len(available_features):\n",
    "            raise ValueError(f\"Size mismatch: available_features={len(available_features)}, valid_probs={len(valid_probs)}\")\n",
    "    \n",
    "        action = np.random.choice(available_features, p=valid_probs)\n",
    "        return action\n",
    "\n",
    "    def unitary_update(self, action, reward):\n",
    "        \"\"\"Update the quantum state using a reinforcement learning-inspired unitary operation.\"\"\"\n",
    "        if len(self.state.shape) > 1:\n",
    "            self.state = self.state.flatten()\n",
    "    \n",
    "        # Ensure the state is float64\n",
    "        self.state = self.state.astype(np.float64)\n",
    "    \n",
    "        U = np.eye(self.num_features, dtype=np.float64)  # Identity matrix as float64\n",
    "    \n",
    "        if reward > 0:\n",
    "            U[action, action] += self.learning_rate  # Reinforce good actions\n",
    "        else:\n",
    "            U[action, action] -= self.learning_rate  # Reduce weight for bad actions\n",
    "    \n",
    "        # Apply transformation in probability space\n",
    "        new_state = U @ self.state  # Matrix-vector multiplication\n",
    "        new_state = np.abs(new_state)  # Ensure non-negative values\n",
    "    \n",
    "        # Avoid division by zero in normalization\n",
    "        state_sum = np.sum(new_state, dtype=np.float64)\n",
    "        if state_sum == 0:\n",
    "            print(\"Warning: Normalization issue in unitary_update. Resetting state.\")\n",
    "            new_state = np.full(self.num_features, 1 / self.num_features, dtype=np.float64)\n",
    "        else:\n",
    "            new_state /= state_sum  # Normalize to maintain probabilities\n",
    "    \n",
    "        self.state = new_state.astype(np.float64)  # Ensure final state is float64\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "#### 4. Run all experiments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Experiments 0 start\n",
      "number of columns: 5 (exclude class column)\n",
      "Number of episodes: 24430.0\n",
      "initial state number: 5 (the last dummy column we have created)\n",
      "\n",
      "ðŸŸ¢ Available features: [np.int64(0), np.int64(1), np.int64(2), np.int64(3), np.int64(4)]\n",
      "ðŸ”Ž Available features: [np.int64(0), np.int64(1), np.int64(2), np.int64(3), np.int64(4)]\n",
      "ðŸ§ Raw valid_probs before normalization: [0.2 0.2 0.2 0.2 0.2]\n",
      "âœ… Valid probabilities after normalization: [0.2 0.2 0.2 0.2 0.2]\n",
      "ðŸŽ¯ Selected feature: 4\n",
      "ðŸ“Œ Updated selected features: [np.int64(4)]\n",
      "ðŸ“Š Training set shape: (8, 1), Test set shape: (2, 1)\n",
      "ðŸš€ [QuantumLearner] Starting fit function...\n",
      "ðŸ“Š Dataset size: X_train=(8, 1), y_train=(8,)\n",
      "ðŸ”„ Reinitializing circuit...\n",
      "âœ… Initialized circuit with 1 qubits\n",
      "ðŸ“Œ Using batch size: 8\n",
      "ðŸš€ Training completed in 0.15 seconds\n",
      "ðŸ“‰ Model error after selection: 0.0\n",
      "ðŸ† Computed Reward: 0.5\n",
      "Quantum state before update: [0.2 0.2 0.2 0.2 0.2]\n",
      "ðŸ”„ Updated Quantum State: [0.19607843 0.19607843 0.19607843 0.19607843 0.21568627]\n",
      "\n",
      "ðŸŸ¢ Available features: [np.int64(0), np.int64(1), np.int64(2), np.int64(3), np.int64(4)]\n",
      "ðŸ”Ž Available features: [np.int64(0), np.int64(1), np.int64(2), np.int64(3)]\n",
      "ðŸ§ Raw valid_probs before normalization: [0.19607843 0.19607843 0.19607843 0.19607843]\n",
      "âœ… Valid probabilities after normalization: [0.25 0.25 0.25 0.25]\n",
      "ðŸŽ¯ Selected feature: 1\n",
      "ðŸ“Œ Updated selected features: [np.int64(4), np.int64(1)]\n",
      "ðŸ“Š Training set shape: (8, 2), Test set shape: (2, 2)\n",
      "ðŸš€ [QuantumLearner] Starting fit function...\n",
      "ðŸ“Š Dataset size: X_train=(8, 2), y_train=(8,)\n",
      "ðŸ”„ Reinitializing circuit...\n",
      "âœ… Initialized circuit with 2 qubits\n",
      "ðŸ“Œ Using batch size: 8\n",
      "ðŸš€ Training completed in 0.43 seconds\n",
      "ðŸ“‰ Model error after selection: 0.0\n",
      "ðŸ† Computed Reward: 0.0\n",
      "Quantum state before update: [0.19607843 0.19607843 0.19607843 0.19607843 0.21568627]\n",
      "ðŸ”„ Updated Quantum State: [0.2  0.18 0.2  0.2  0.22]\n",
      "\n",
      "ðŸŸ¢ Available features: [np.int64(0), np.int64(1), np.int64(2), np.int64(3)]\n",
      "ðŸ”Ž Available features: [np.int64(0), np.int64(2), np.int64(3)]\n",
      "ðŸ§ Raw valid_probs before normalization: [0.2 0.2 0.2]\n",
      "âœ… Valid probabilities after normalization: [0.33333333 0.33333333 0.33333333]\n",
      "ðŸŽ¯ Selected feature: 0\n",
      "ðŸ“Œ Updated selected features: [np.int64(4), np.int64(1), np.int64(0)]\n",
      "ðŸ“Š Training set shape: (8, 3), Test set shape: (2, 3)\n",
      "ðŸš€ [QuantumLearner] Starting fit function...\n",
      "ðŸ“Š Dataset size: X_train=(8, 3), y_train=(8,)\n",
      "ðŸ”„ Reinitializing circuit...\n",
      "âœ… Initialized circuit with 3 qubits\n",
      "ðŸ“Œ Using batch size: 8\n",
      "ðŸš€ Training completed in 0.54 seconds\n",
      "ðŸ“‰ Model error after selection: 1.0\n",
      "ðŸ† Computed Reward: -1.0\n",
      "Quantum state before update: [0.2  0.18 0.2  0.2  0.22]\n",
      "ðŸ”„ Updated Quantum State: [0.18367347 0.18367347 0.20408163 0.20408163 0.2244898 ]\n",
      "\n",
      "ðŸŸ¢ Available features: [np.int64(0), np.int64(2), np.int64(3)]\n",
      "ðŸ”Ž Available features: [np.int64(2), np.int64(3)]\n",
      "ðŸ§ Raw valid_probs before normalization: [0.20408163 0.20408163]\n",
      "âœ… Valid probabilities after normalization: [0.5 0.5]\n",
      "ðŸŽ¯ Selected feature: 3\n",
      "ðŸ“Œ Updated selected features: [np.int64(4), np.int64(1), np.int64(0), np.int64(3)]\n",
      "ðŸ“Š Training set shape: (8, 4), Test set shape: (2, 4)\n",
      "ðŸš€ [QuantumLearner] Starting fit function...\n",
      "ðŸ“Š Dataset size: X_train=(8, 4), y_train=(8,)\n",
      "ðŸ”„ Reinitializing circuit...\n",
      "âœ… Initialized circuit with 4 qubits\n",
      "ðŸ“Œ Using batch size: 8\n",
      "ðŸš€ Training completed in 0.66 seconds\n",
      "ðŸ“‰ Model error after selection: 0.0\n",
      "ðŸ† Computed Reward: 1.0\n",
      "Quantum state before update: [0.18367347 0.18367347 0.20408163 0.20408163 0.2244898 ]\n",
      "ðŸ”„ Updated Quantum State: [0.18 0.18 0.2  0.22 0.22]\n",
      "\n",
      "ðŸŸ¢ Available features: [np.int64(2), np.int64(3)]\n",
      "ðŸ”Ž Available features: [np.int64(2)]\n",
      "ðŸ§ Raw valid_probs before normalization: [0.2]\n",
      "âœ… Valid probabilities after normalization: [1.]\n",
      "ðŸŽ¯ Selected feature: 2\n",
      "ðŸ“Œ Updated selected features: [np.int64(4), np.int64(1), np.int64(0), np.int64(3), np.int64(2)]\n",
      "ðŸ“Š Training set shape: (8, 5), Test set shape: (2, 5)\n",
      "ðŸš€ [QuantumLearner] Starting fit function...\n",
      "ðŸ“Š Dataset size: X_train=(8, 5), y_train=(8,)\n",
      "ðŸ”„ Reinitializing circuit...\n",
      "âœ… Initialized circuit with 5 qubits\n",
      "ðŸ“Œ Using batch size: 8\n",
      "ðŸš€ Training completed in 0.92 seconds\n",
      "ðŸ“‰ Model error after selection: 0.0\n",
      "ðŸ† Computed Reward: 0.0\n",
      "Quantum state before update: [0.18 0.18 0.2  0.22 0.22]\n",
      "ðŸ”„ Updated Quantum State: [0.18367347 0.18367347 0.18367347 0.2244898  0.2244898 ]\n",
      "\n",
      "ðŸŸ¢ Available features: [np.int64(2)]\n",
      "âŒ No available actions left. Terminating episode.\n",
      "Q learning End.\n",
      "Calculating policy...\n",
      "Calculating policy_accuracy...\n",
      "ðŸ“Š Policy dataset shape: Train=(19544, 5), Test=(4886, 5)\n",
      "â³ Running Learner...        hours.per.week  capital.loss  native.country  capital.gain  sex\n",
      "30166              39             0              38             0    1\n",
      "11722              39             0              38             0    1\n",
      "28609              34             0              38             0    1\n",
      "25774              19             0              28             0    1\n",
      "17631              39             0              38             0    0\n",
      "...               ...           ...             ...           ...  ...\n",
      "26416              39             0              38             0    1\n",
      "13906              47             0              38             0    1\n",
      "28394              44             0              38             0    1\n",
      "25758              49             0              38             0    1\n",
      "18465              39             0              38             0    0\n",
      "\n",
      "[19544 rows x 5 columns]        hours.per.week  capital.loss  native.country  capital.gain  sex\n",
      "29987              39             0              38             0    0\n",
      "31242              34             0              32             0    0\n",
      "1571               67             0              38           117    1\n",
      "14115              68             0              38             0    0\n",
      "6343               24             0              38             0    0\n",
      "...               ...           ...             ...           ...  ...\n",
      "22270              13             0              38             0    0\n",
      "1519               59             0              38           117    1\n",
      "1723               44             0              38           113    1\n",
      "3453               39             0              38            63    1\n",
      "792                39            44              38             0    1\n",
      "\n",
      "[4886 rows x 5 columns] 30166    0\n",
      "11722    1\n",
      "28609    0\n",
      "25774    0\n",
      "17631    0\n",
      "        ..\n",
      "26416    0\n",
      "13906    1\n",
      "28394    0\n",
      "25758    1\n",
      "18465    0\n",
      "Name: over50K, Length: 19544, dtype: int64 29987    0\n",
      "31242    0\n",
      "1571     1\n",
      "14115    0\n",
      "6343     0\n",
      "        ..\n",
      "22270    0\n",
      "1519     1\n",
      "1723     1\n",
      "3453     0\n",
      "792      1\n",
      "Name: over50K, Length: 4886, dtype: int64\n",
      "ðŸš€ [QuantumLearner] Starting fit function...\n",
      "ðŸ“Š Dataset size: X_train=(19544, 5), y_train=(19544,)\n",
      "ðŸ”„ Reinitializing circuit...\n",
      "âœ… Initialized circuit with 5 qubits\n",
      "ðŸ“Œ Using batch size: 48\n",
      "ðŸš€ Training completed in 5.85 seconds\n",
      "âœ… Learner execution time: 21.56 seconds\n",
      "ðŸš€ [QuantumLearner] Starting fit function...\n",
      "ðŸ“Š Dataset size: X_train=(2172, 5), y_train=(2172,)\n",
      "ðŸ”„ Reinitializing circuit...\n",
      "âœ… Initialized circuit with 5 qubits\n",
      "ðŸ“Œ Using batch size: 48\n",
      "ðŸš€ Training completed in 5.46 seconds\n",
      "episode 1 start\n",
      "episode columns: [np.int64(4), np.int64(1), np.int64(0), np.int64(3), np.int64(2)] epsilon: 0.9 learning rate: 0.09 error: 0.0\n",
      "episode policy:[np.int64(3), np.int64(2), np.int64(4), np.int64(1), np.int64(0)] train accuracy: 0.18440442079410557 test accuracy: 0.13259668508287292\n",
      "episode 1 end\n",
      "\n",
      "ðŸŸ¢ Available features: [np.int64(0), np.int64(1), np.int64(2), np.int64(3), np.int64(4)]\n",
      "ðŸ”Ž Available features: [np.int64(0), np.int64(1), np.int64(2), np.int64(3), np.int64(4)]\n",
      "ðŸ§ Raw valid_probs before normalization: [0.2 0.2 0.2 0.2 0.2]\n",
      "âœ… Valid probabilities after normalization: [0.2 0.2 0.2 0.2 0.2]\n",
      "ðŸŽ¯ Selected feature: 0\n",
      "ðŸ“Œ Updated selected features: [np.int64(0)]\n",
      "ðŸ“Š Training set shape: (8, 1), Test set shape: (2, 1)\n",
      "ðŸš€ [QuantumLearner] Starting fit function...\n",
      "ðŸ“Š Dataset size: X_train=(8, 1), y_train=(8,)\n",
      "ðŸ”„ Reinitializing circuit...\n",
      "âœ… Initialized circuit with 1 qubits\n",
      "ðŸ“Œ Using batch size: 8\n",
      "ðŸš€ Training completed in 0.25 seconds\n",
      "ðŸ“‰ Model error after selection: 1.0\n",
      "ðŸ† Computed Reward: -0.5\n",
      "Quantum state before update: [0.2 0.2 0.2 0.2 0.2]\n",
      "ðŸ”„ Updated Quantum State: [0.18367347 0.20408163 0.20408163 0.20408163 0.20408163]\n",
      "\n",
      "ðŸŸ¢ Available features: [np.int64(0), np.int64(1), np.int64(2), np.int64(3), np.int64(4)]\n",
      "ðŸ”Ž Available features: [np.int64(1), np.int64(2), np.int64(3), np.int64(4)]\n",
      "ðŸ§ Raw valid_probs before normalization: [0.20408163 0.20408163 0.20408163 0.20408163]\n",
      "âœ… Valid probabilities after normalization: [0.25 0.25 0.25 0.25]\n",
      "ðŸŽ¯ Selected feature: 2\n",
      "ðŸ“Œ Updated selected features: [np.int64(0), np.int64(2)]\n",
      "ðŸ“Š Training set shape: (8, 2), Test set shape: (2, 2)\n",
      "ðŸš€ [QuantumLearner] Starting fit function...\n",
      "ðŸ“Š Dataset size: X_train=(8, 2), y_train=(8,)\n",
      "ðŸ”„ Reinitializing circuit...\n",
      "âœ… Initialized circuit with 2 qubits\n",
      "ðŸ“Œ Using batch size: 8\n",
      "ðŸš€ Training completed in 0.30 seconds\n",
      "ðŸ“‰ Model error after selection: 1.0\n",
      "ðŸ† Computed Reward: 0.0\n",
      "Quantum state before update: [0.18367347 0.20408163 0.20408163 0.20408163 0.20408163]\n",
      "ðŸ”„ Updated Quantum State: [0.1875     0.20833333 0.1875     0.20833333 0.20833333]\n",
      "\n",
      "ðŸŸ¢ Available features: [np.int64(1), np.int64(2), np.int64(3), np.int64(4)]\n",
      "ðŸ”Ž Available features: [np.int64(1), np.int64(3), np.int64(4)]\n",
      "ðŸ§ Raw valid_probs before normalization: [0.20833333 0.20833333 0.20833333]\n",
      "âœ… Valid probabilities after normalization: [0.33333333 0.33333333 0.33333333]\n",
      "ðŸŽ¯ Selected feature: 1\n",
      "ðŸ“Œ Updated selected features: [np.int64(0), np.int64(2), np.int64(1)]\n",
      "ðŸ“Š Training set shape: (8, 3), Test set shape: (2, 3)\n",
      "ðŸš€ [QuantumLearner] Starting fit function...\n",
      "ðŸ“Š Dataset size: X_train=(8, 3), y_train=(8,)\n",
      "ðŸ”„ Reinitializing circuit...\n",
      "âœ… Initialized circuit with 3 qubits\n",
      "ðŸ“Œ Using batch size: 8\n",
      "ðŸš€ Training completed in 0.54 seconds\n",
      "ðŸ“‰ Model error after selection: 0.5\n",
      "ðŸ† Computed Reward: 0.5\n",
      "Quantum state before update: [0.1875     0.20833333 0.1875     0.20833333 0.20833333]\n",
      "ðŸ”„ Updated Quantum State: [0.18367347 0.2244898  0.18367347 0.20408163 0.20408163]\n",
      "\n",
      "ðŸŸ¢ Available features: [np.int64(1), np.int64(3), np.int64(4)]\n",
      "ðŸ”Ž Available features: [np.int64(3), np.int64(4)]\n",
      "ðŸ§ Raw valid_probs before normalization: [0.20408163 0.20408163]\n",
      "âœ… Valid probabilities after normalization: [0.5 0.5]\n",
      "ðŸŽ¯ Selected feature: 4\n",
      "ðŸ“Œ Updated selected features: [np.int64(0), np.int64(2), np.int64(1), np.int64(4)]\n",
      "ðŸ“Š Training set shape: (8, 4), Test set shape: (2, 4)\n",
      "ðŸš€ [QuantumLearner] Starting fit function...\n",
      "ðŸ“Š Dataset size: X_train=(8, 4), y_train=(8,)\n",
      "ðŸ”„ Reinitializing circuit...\n",
      "âœ… Initialized circuit with 4 qubits\n",
      "ðŸ“Œ Using batch size: 8\n",
      "ðŸš€ Training completed in 0.77 seconds\n",
      "ðŸ“‰ Model error after selection: 1.0\n",
      "ðŸ† Computed Reward: -0.5\n",
      "Quantum state before update: [0.18367347 0.2244898  0.18367347 0.20408163 0.20408163]\n",
      "ðŸ”„ Updated Quantum State: [0.1875     0.22916667 0.1875     0.20833333 0.1875    ]\n",
      "\n",
      "ðŸŸ¢ Available features: [np.int64(3), np.int64(4)]\n",
      "ðŸ”Ž Available features: [np.int64(3)]\n",
      "ðŸ§ Raw valid_probs before normalization: [0.20833333]\n",
      "âœ… Valid probabilities after normalization: [1.]\n",
      "ðŸŽ¯ Selected feature: 3\n",
      "ðŸ“Œ Updated selected features: [np.int64(0), np.int64(2), np.int64(1), np.int64(4), np.int64(3)]\n",
      "ðŸ“Š Training set shape: (8, 5), Test set shape: (2, 5)\n",
      "ðŸš€ [QuantumLearner] Starting fit function...\n",
      "ðŸ“Š Dataset size: X_train=(8, 5), y_train=(8,)\n",
      "ðŸ”„ Reinitializing circuit...\n",
      "âœ… Initialized circuit with 5 qubits\n",
      "ðŸ“Œ Using batch size: 8\n",
      "ðŸš€ Training completed in 0.79 seconds\n",
      "ðŸ“‰ Model error after selection: 1.0\n",
      "ðŸ† Computed Reward: 0.0\n",
      "Quantum state before update: [0.1875     0.22916667 0.1875     0.20833333 0.1875    ]\n",
      "ðŸ”„ Updated Quantum State: [0.19148936 0.23404255 0.19148936 0.19148936 0.19148936]\n",
      "\n",
      "ðŸŸ¢ Available features: [np.int64(3)]\n",
      "âŒ No available actions left. Terminating episode.\n",
      "Q learning End.\n",
      "Calculating policy...\n",
      "Calculating policy_accuracy...\n",
      "ðŸ“Š Policy dataset shape: Train=(19544, 5), Test=(4886, 5)\n",
      "â³ Running Learner...        hours.per.week  capital.loss  native.country  capital.gain  sex\n",
      "30166              39             0              38             0    1\n",
      "11722              39             0              38             0    1\n",
      "28609              34             0              38             0    1\n",
      "25774              19             0              28             0    1\n",
      "17631              39             0              38             0    0\n",
      "...               ...           ...             ...           ...  ...\n",
      "26416              39             0              38             0    1\n",
      "13906              47             0              38             0    1\n",
      "28394              44             0              38             0    1\n",
      "25758              49             0              38             0    1\n",
      "18465              39             0              38             0    0\n",
      "\n",
      "[19544 rows x 5 columns]        hours.per.week  capital.loss  native.country  capital.gain  sex\n",
      "29987              39             0              38             0    0\n",
      "31242              34             0              32             0    0\n",
      "1571               67             0              38           117    1\n",
      "14115              68             0              38             0    0\n",
      "6343               24             0              38             0    0\n",
      "...               ...           ...             ...           ...  ...\n",
      "22270              13             0              38             0    0\n",
      "1519               59             0              38           117    1\n",
      "1723               44             0              38           113    1\n",
      "3453               39             0              38            63    1\n",
      "792                39            44              38             0    1\n",
      "\n",
      "[4886 rows x 5 columns] 30166    0\n",
      "11722    1\n",
      "28609    0\n",
      "25774    0\n",
      "17631    0\n",
      "        ..\n",
      "26416    0\n",
      "13906    1\n",
      "28394    0\n",
      "25758    1\n",
      "18465    0\n",
      "Name: over50K, Length: 19544, dtype: int64 29987    0\n",
      "31242    0\n",
      "1571     1\n",
      "14115    0\n",
      "6343     0\n",
      "        ..\n",
      "22270    0\n",
      "1519     1\n",
      "1723     1\n",
      "3453     0\n",
      "792      1\n",
      "Name: over50K, Length: 4886, dtype: int64\n",
      "ðŸš€ [QuantumLearner] Starting fit function...\n",
      "ðŸ“Š Dataset size: X_train=(19544, 5), y_train=(19544,)\n",
      "ðŸ”„ Reinitializing circuit...\n",
      "âœ… Initialized circuit with 5 qubits\n",
      "ðŸ“Œ Using batch size: 48\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[21], line 182\u001b[0m\n\u001b[1;32m    180\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mâ³ Running Learner...\u001b[39m\u001b[38;5;124m\"\u001b[39m, X_train_policy, X_test_policy, y_train_policy, y_test_policy)\n\u001b[1;32m    181\u001b[0m start_time \u001b[38;5;241m=\u001b[39m time\u001b[38;5;241m.\u001b[39mtime()\n\u001b[0;32m--> 182\u001b[0m policy_error \u001b[38;5;241m=\u001b[39m \u001b[43mLearner\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX_train_policy\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mX_test_policy\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my_train_policy\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my_test_policy\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    183\u001b[0m end_time \u001b[38;5;241m=\u001b[39m time\u001b[38;5;241m.\u001b[39mtime()\n\u001b[1;32m    185\u001b[0m \u001b[38;5;66;03m# Print execution time\u001b[39;00m\n",
      "Cell \u001b[0;32mIn[10], line 31\u001b[0m, in \u001b[0;36mLearner\u001b[0;34m(X_train, X_test, y_train, y_test)\u001b[0m\n\u001b[1;32m     29\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m learner_model \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mVQC\u001b[39m\u001b[38;5;124m'\u001b[39m:\n\u001b[1;32m     30\u001b[0m     learner \u001b[38;5;241m=\u001b[39m QuantumLearner()\n\u001b[0;32m---> 31\u001b[0m     learner \u001b[38;5;241m=\u001b[39m \u001b[43mlearner\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfit\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX_train\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my_train\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     32\u001b[0m     y_pred \u001b[38;5;241m=\u001b[39m learner\u001b[38;5;241m.\u001b[39mpredict(X_test)  \n\u001b[1;32m     33\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m learner_model \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mANN\u001b[39m\u001b[38;5;124m'\u001b[39m:\n",
      "Cell \u001b[0;32mIn[19], line 98\u001b[0m, in \u001b[0;36mQuantumLearner.fit\u001b[0;34m(self, X_train, y_train, num_it, batch_size, warm_start)\u001b[0m\n\u001b[1;32m     94\u001b[0m \u001b[38;5;66;03m#print(f\"ðŸ›  Batch selection completed in {batch_time:.2f} seconds\")\u001b[39;00m\n\u001b[1;32m     95\u001b[0m \n\u001b[1;32m     96\u001b[0m \u001b[38;5;66;03m# Time the optimization step\u001b[39;00m\n\u001b[1;32m     97\u001b[0m opt_start \u001b[38;5;241m=\u001b[39m time\u001b[38;5;241m.\u001b[39mtime()\n\u001b[0;32m---> 98\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mweights, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mbias \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mopt\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mstep\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m     99\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43;01mlambda\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mw\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mb\u001b[49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcost\u001b[49m\u001b[43m(\u001b[49m\u001b[43mw\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mb\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mX_batch\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mY_batch\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mweights\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbias\u001b[49m\n\u001b[1;32m    100\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    101\u001b[0m opt_time \u001b[38;5;241m=\u001b[39m time\u001b[38;5;241m.\u001b[39mtime() \u001b[38;5;241m-\u001b[39m opt_start\n\u001b[1;32m    102\u001b[0m \u001b[38;5;66;03m#print(f\"âœ… Optimization step completed in {opt_time:.2f} seconds\")\u001b[39;00m\n",
      "File \u001b[0;32m~/.venv/lib/python3.13/site-packages/pennylane/optimize/gradient_descent.py:93\u001b[0m, in \u001b[0;36mGradientDescentOptimizer.step\u001b[0;34m(self, objective_fn, grad_fn, *args, **kwargs)\u001b[0m\n\u001b[1;32m     75\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mstep\u001b[39m(\u001b[38;5;28mself\u001b[39m, objective_fn, \u001b[38;5;241m*\u001b[39margs, grad_fn\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[1;32m     76\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Update trainable arguments with one step of the optimizer.\u001b[39;00m\n\u001b[1;32m     77\u001b[0m \n\u001b[1;32m     78\u001b[0m \u001b[38;5;124;03m    Args:\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     90\u001b[0m \u001b[38;5;124;03m        If single arg is provided, list [array] is replaced by array.\u001b[39;00m\n\u001b[1;32m     91\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[0;32m---> 93\u001b[0m     g, _ \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcompute_grad\u001b[49m\u001b[43m(\u001b[49m\u001b[43mobjective_fn\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mgrad_fn\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mgrad_fn\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     94\u001b[0m     new_args \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mapply_grad(g, args)\n\u001b[1;32m     96\u001b[0m     \u001b[38;5;66;03m# unwrap from list if one argument, cleaner return\u001b[39;00m\n",
      "File \u001b[0;32m~/.venv/lib/python3.13/site-packages/pennylane/optimize/gradient_descent.py:122\u001b[0m, in \u001b[0;36mGradientDescentOptimizer.compute_grad\u001b[0;34m(objective_fn, args, kwargs, grad_fn)\u001b[0m\n\u001b[1;32m    104\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124mr\u001b[39m\u001b[38;5;124;03m\"\"\"Compute the gradient of the objective function at the given point and return it along with\u001b[39;00m\n\u001b[1;32m    105\u001b[0m \u001b[38;5;124;03mthe objective function forward pass (if available).\u001b[39;00m\n\u001b[1;32m    106\u001b[0m \n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    119\u001b[0m \u001b[38;5;124;03m    will not be evaluated and instead ``None`` will be returned.\u001b[39;00m\n\u001b[1;32m    120\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m    121\u001b[0m g \u001b[38;5;241m=\u001b[39m get_gradient(objective_fn) \u001b[38;5;28;01mif\u001b[39;00m grad_fn \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;28;01melse\u001b[39;00m grad_fn\n\u001b[0;32m--> 122\u001b[0m grad \u001b[38;5;241m=\u001b[39m \u001b[43mg\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    123\u001b[0m forward \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mgetattr\u001b[39m(g, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mforward\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28;01mNone\u001b[39;00m)\n\u001b[1;32m    125\u001b[0m num_trainable_args \u001b[38;5;241m=\u001b[39m \u001b[38;5;28msum\u001b[39m(\u001b[38;5;28mgetattr\u001b[39m(arg, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mrequires_grad\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28;01mFalse\u001b[39;00m) \u001b[38;5;28;01mfor\u001b[39;00m arg \u001b[38;5;129;01min\u001b[39;00m args)\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "for e in range (number_of_experiment):\n",
    "    if not os.path.exists('Experiments/'+ str(experiment)+ '/'+ str(e)):\n",
    "        os.makedirs('Experiments/'+ str(experiment)+ '/'+ str(e))\n",
    "    else:\n",
    "        shutil.rmtree('Experiments/'+ str(experiment)+ '/'+ str(e))          #removes all the subdirectories!\n",
    "        os.makedirs('Experiments/'+ str(experiment)+ '/'+ str(e))\n",
    "    print ('Experiments ' + str(e) + ' start')\n",
    "##########################Experiment setup##########################\n",
    "    # Read the data\n",
    "    data = pd.read_csv(location + '/' + filename, index_col=0)\n",
    "    \n",
    "##### for run time - start #####\n",
    "    import timeit\n",
    "    start = timeit.default_timer()\n",
    "    size= int(N_data* len(data.index))\n",
    "    data = data.sample(n=size)\n",
    "    data=data.iloc[:,-N_features-1:]\n",
    "##### for run time - end #####\n",
    "    \n",
    "    #Set the number of iterations:\n",
    "    interations=10*len(data.index)/episode_size\n",
    "    # Set the number of columns exclude the class column\n",
    "    number_of_columns=data.shape[1]-1 \n",
    "    print (\"number of columns: \"+ str(number_of_columns) +\" (exclude class column)\" ) \n",
    "    # Set the number of episodes \n",
    "    # episodes_number=epocs*len(data.index)/episode_size\n",
    "    episodes_number=interations\n",
    "    print (\"Number of episodes: \"+ str(episodes_number) ) \n",
    "    # Initialize matrix Q as a 1 values matrix:\n",
    "    #Q = np.matrix(np.ones([number_of_columns+1,number_of_columns+1])) # we will use the last dummy columns as initial state s\n",
    "    Q = np.matrix(np.ones([number_of_columns+1,number_of_columns+1])) # we will use the last dummy columns as initial state s\n",
    "    # Set initial_state to be the last dummy column we have created\n",
    "    initial_state=number_of_columns\n",
    "    # define data frame to save episode policies results\n",
    "    df = pd.DataFrame(columns=('episode','episode_columns','policy_columns','policy_accuracy_train','policy_accuracy_test'))\n",
    "    print (\"initial state number: \"+ str(initial_state) + \" (the last dummy column we have created)\") \n",
    "\n",
    "    ##########################  episode  ##########################  \n",
    "    for i in range (int(episodes_number)):\n",
    "    ########## Begining of episode  ############\n",
    "        # Initiate lists for available_act, episode_columns and and the policy mode & episode_error\n",
    "        episode_available_act=list(np.arange(number_of_columns))\n",
    "        episode_columns=[]\n",
    "        policy=0\n",
    "        episode_error=0\n",
    "        # Initiate the error to 0.5\n",
    "        episode_last_error=0.5\n",
    "        # Initiate current_state to be initial_state\n",
    "        episode_current_state=initial_state\n",
    "        # Create the episode data \n",
    "        episode= get_data(episode_size, policy=0, mode='train')\n",
    "        # Separate the episode data into features and label\n",
    "        X_episode,y_episode=data_separate(episode)\n",
    "        # Split the data into train and test \n",
    "        X_train_main_episode, X_test_main_episode, y_train_episode, y_test_episode = data_split(X_episode,y_episode)\n",
    "        if i<episodes_number*0.25:\n",
    "            epsilon=0.9\n",
    "            learning_rate=0.09\n",
    "        elif i<episodes_number*0.5:\n",
    "            epsilon=0.5\n",
    "            learning_rate=0.05\n",
    "        elif i<episodes_number*0.75:\n",
    "            epsilon=0.3\n",
    "            learning_rate=0.01\n",
    "        else:\n",
    "            epsilon=0.1\n",
    "            learning_rate=0.005\n",
    "        ########## Q learning start ############\n",
    "\n",
    "        # Initialize Quantum RL feature selection\n",
    "        quantum_rl = QuantumFeatureSelection(num_features=number_of_columns)\n",
    "        \n",
    "        # While there are features left to select\n",
    "        # while len(episode_available_act) > 0:\n",
    "        #     # Select next feature using quantum measurement\n",
    "        #     episode_action = quantum_rl.measure(episode_available_act)\n",
    "        \n",
    "        #     # Update the selected feature list\n",
    "        #     episode_columns = update_columns(episode_action, episode_columns)\n",
    "        \n",
    "        #     # Prepare training dataset with selected features\n",
    "        #     X_train_episode, X_test_episode = update_X_train_X_test(episode_columns, X_train_main_episode, X_test_main_episode)\n",
    "        \n",
    "        #     # Evaluate the model accuracy with the selected features\n",
    "        #     episode_error = Learner(X_train_episode, X_test_episode, y_train_episode, y_test_episode)\n",
    "        \n",
    "        #     # Compute reward based on improvement\n",
    "        #     episode_reward = episode_last_error - episode_error\n",
    "        \n",
    "        #     # Quantum RL update step\n",
    "        #     quantum_rl.unitary_update(episode_action, episode_reward)\n",
    "        \n",
    "        #     # Move to next state (selected feature)\n",
    "        #     episode_current_state = episode_action\n",
    "        #     episode_last_error = episode_error\n",
    "\n",
    "\n",
    "        while len(episode_available_act) > 0:\n",
    "            print(f\"\\nðŸŸ¢ Available features: {episode_available_act}\")\n",
    "        \n",
    "            # Determine exploration vs. exploitation\n",
    "            exploration = exploration_explotation(epsilon)  # ðŸ”¥ Fix: Define exploration\n",
    "        \n",
    "            # Update the available actions list\n",
    "            episode_available_act = available_actions(number_of_columns, episode_columns, initial_state, episode_current_state, internal_trashold, exploration)\n",
    "            \n",
    "            # If no available actions remain, terminate the loop\n",
    "            if len(episode_available_act) == 0:\n",
    "                print(\"âŒ No available actions left. Terminating episode.\")\n",
    "                break\n",
    "        \n",
    "            # Select next feature using quantum measurement\n",
    "            episode_action = quantum_rl.measure(episode_available_act)\n",
    "            print(f\"ðŸŽ¯ Selected feature: {episode_action}\")\n",
    "        \n",
    "            # Update the selected feature list\n",
    "            episode_columns = update_columns(episode_action, episode_columns)\n",
    "            print(f\"ðŸ“Œ Updated selected features: {episode_columns}\")\n",
    "        \n",
    "            # Prepare training dataset with selected features\n",
    "            X_train_episode, X_test_episode = update_X_train_X_test(\n",
    "                episode_columns, X_train_main_episode, X_test_main_episode\n",
    "            )\n",
    "            print(f\"ðŸ“Š Training set shape: {X_train_episode.shape}, Test set shape: {X_test_episode.shape}\")\n",
    "\n",
    "            # Evaluate the model accuracy with the selected features\n",
    "            episode_error = Learner(X_train_episode, X_test_episode, y_train_episode, y_test_episode)\n",
    "            print(f\"ðŸ“‰ Model error after selection: {episode_error}\")\n",
    "        \n",
    "            # Compute reward based on improvement\n",
    "            episode_reward = episode_last_error - episode_error\n",
    "            print(f\"ðŸ† Computed Reward: {episode_reward}\")\n",
    "        \n",
    "            # Quantum RL update step\n",
    "            print(f\"Quantum state before update: {quantum_rl.state}\")\n",
    "            quantum_rl.unitary_update(episode_action, episode_reward)\n",
    "            print(f\"ðŸ”„ Updated Quantum State: {quantum_rl.state}\")\n",
    "        \n",
    "            # Move to next state (selected feature)\n",
    "            episode_current_state = episode_action\n",
    "            episode_last_error = episode_error\n",
    "\n",
    "        print(\"Q learning End.\")\n",
    "        ########## Q learning End ############\n",
    "\n",
    "        #Save Q matrix: \n",
    "        if (i%100 ==0):\n",
    "            Q_save=pd.DataFrame(Q)\n",
    "            Q_save.to_csv('Experiments/'+ str(experiment)+ '/'+ str(e)+ '/Q.'+ str(i+1) + '.csv') \n",
    "            \n",
    "        print(\"Calculating policy...\")\n",
    "        # Calculate policy \n",
    "        policy_available_actions=list(np.arange(number_of_columns))\n",
    "        policy_columns=[]\n",
    "        policy_current_state=initial_state\n",
    "        while len(policy_available_actions)>0:\n",
    "            # Get available actions in the current state\n",
    "            policy_available_actions = available_actions(number_of_columns,policy_columns,initial_state,policy_current_state, external_trashold, exploration=0)\n",
    "            # # Sample next action to be performed\n",
    "            if len(policy_available_actions)>0:\n",
    "                policy_select_action = sample_next_action(policy_current_state, Q, policy_available_actions, exploration=0)\n",
    "                # Update the episode_columns\n",
    "                policy_columns=update_columns(policy_select_action,policy_columns)\n",
    "                policy_current_state=policy_select_action\n",
    "\n",
    "        print(\"Calculating policy_accuracy...\")\n",
    "        # Calculate policy_accuracy    \n",
    "        if len(policy_columns)>0:\n",
    "            ##for training dataset##\n",
    "            policy_data=get_data(episode_size,policy=1,mode='train')\n",
    "            X_policy,y_policy=data_separate(policy_data)\n",
    "            X_train_main_policy, X_test_main_policy, y_train_policy, y_test_policy = data_split(X,y)\n",
    "            X_train_policy, X_test_policy =update_X_train_X_test(policy_columns, X_train_main_policy, X_test_main_policy)\n",
    "            \n",
    "\n",
    "            # Print dataset shape before training to check if it's too large\n",
    "            print(f\"ðŸ“Š Policy dataset shape: Train={X_train_policy.shape}, Test={X_test_policy.shape}\")\n",
    "            \n",
    "            # Time the execution of the Learner function\n",
    "            print(\"â³ Running Learner...\", X_train_policy, X_test_policy, y_train_policy, y_test_policy)\n",
    "            start_time = time.time()\n",
    "            policy_error = Learner(X_train_policy, X_test_policy, y_train_policy, y_test_policy)\n",
    "            end_time = time.time()\n",
    "            \n",
    "            # Print execution time\n",
    "            print(f\"âœ… Learner execution time: {end_time - start_time:.2f} seconds\")\n",
    "\n",
    "            \n",
    "            \n",
    "            \n",
    "            policy_accuracy_train=1-policy_error\n",
    "            ##for testing dataset##\n",
    "            policy_data=get_data(episode_size,policy=1,mode='test')\n",
    "            X_policy,y_policy=data_separate(policy_data)\n",
    "            X_train_main_policy, X_test_main_policy, y_train_policy, y_test_policy = data_split(X,y)\n",
    "            X_train_policy, X_test_policy =update_X_train_X_test(policy_columns, X_train_main_policy, X_test_main_policy)\n",
    "            policy_error=Learner(X_train_policy, X_test_policy,y_train_policy, y_test_policy)\n",
    "            policy_accuracy_test=1-policy_error \n",
    "        else:\n",
    "            policy_accuracy_train=0 \n",
    "            policy_accuracy_test=0\n",
    "        #df=df.append({'episode':str(i+1), 'episode_columns':str(episode_columns),'policy_columns':str(policy_columns),'policy_accuracy_train':policy_accuracy_train,'policy_accuracy_test':policy_accuracy_test}, ignore_index=True)\n",
    "        #new_row = pd.DataFrame([{'episode': str(i+1),\n",
    "        #                  'episode_columns': str(episode_columns),\n",
    "        #                  'policy_columns': str(policy_columns),\n",
    "        #                  'policy_accuracy_train': policy_accuracy_train,\n",
    "        #                  'policy_accuracy_test': policy_accuracy_test}])\n",
    "        #df = pd.concat([df, new_row], ignore_index=True)\n",
    "        df.loc[len(df)] = {\n",
    "            'episode': str(i+1),\n",
    "            'episode_columns': str(episode_columns),\n",
    "            'policy_columns': str(policy_columns),\n",
    "            'policy_accuracy_train': policy_accuracy_train,\n",
    "            'policy_accuracy_test': policy_accuracy_test\n",
    "        }\n",
    "\n",
    "        #Prints\n",
    "        print (\"episode \"+ str(i+1) +\" start\") \n",
    "        print (\"episode columns: \"+ str(episode_columns) + \" epsilon: \" + str(epsilon) + \" learning rate: \" + str(learning_rate) + \" error: \" +str(episode_error))\n",
    "        print (\"episode policy:\" + str(policy_columns) + \" train accuracy: \" + str(policy_accuracy_train)  + \" test accuracy: \" +str(policy_accuracy_test)) \n",
    "        print (\"episode \"+ str(i+1) +\" end\") \n",
    "    ########## End of episode  ############\n",
    "    #df.to_excel(writer, 'Experiment' + str(e))\n",
    "    df.to_excel(writer, sheet_name='Experiment' + str(e))\n",
    "    df_plot=df[['episode','policy_accuracy_train','policy_accuracy_test']]\n",
    "    plot=df_plot.plot()\n",
    "    fig = plot.get_figure()\n",
    "    fig.savefig('Experiments/'+ str(experiment) + '/plot_experiment_' + str(e) +'.png')\n",
    "    \n",
    "#writer.save()\n",
    "with pd.ExcelWriter('Experiments/'+ str(experiment) + '/df.xlsx') as writer:\n",
    "    df.to_excel(writer, sheet_name='Experiment' + str(e))\n",
    "\n",
    "## for run time ##\n",
    "stop = timeit.default_timer()\n",
    "print (stop - start)\n",
    "## for run time ##"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "All tests passed!\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "def test_quantum_feature_selection():\n",
    "    num_features = 4\n",
    "    learning_rate = 0.1\n",
    "    qfs = QuantumFeatureSelection(num_features, learning_rate)\n",
    "\n",
    "    # Test initialization\n",
    "    assert qfs.state.shape == (num_features,), \"State shape is incorrect.\"\n",
    "    assert np.isclose(qfs.state.sum(), 1.0), \"State is not normalized.\"\n",
    "\n",
    "    # Test measurement\n",
    "    available_features = [0, 1, 2, 3]\n",
    "    for _ in range(10):\n",
    "        action = qfs.measure(available_features)\n",
    "        assert action in available_features, f\"Invalid action selected: {action}\"\n",
    "\n",
    "    # Test unitary update\n",
    "    initial_state = qfs.state.copy()\n",
    "    qfs.unitary_update(action, reward=1)\n",
    "    assert np.isclose(qfs.state.sum(), 1.0), \"State is not normalized after update.\"\n",
    "    assert not np.allclose(qfs.state, initial_state), \"State did not change after update.\"\n",
    "\n",
    "    # Edge case: No available features\n",
    "    try:\n",
    "        qfs.measure([])\n",
    "        assert False, \"Expected ValueError when measuring with no available features.\"\n",
    "    except ValueError:\n",
    "        pass  # Expected behavior\n",
    "\n",
    "    # Edge case: Unitary update does not lead to zero state\n",
    "    qfs.unitary_update(action, reward=-1)\n",
    "    assert np.isclose(qfs.state.sum(), 1.0), \"State is not normalized after negative reward update.\"\n",
    "\n",
    "    print(\"All tests passed!\")\n",
    "\n",
    "# Run the test\n",
    "test_quantum_feature_selection()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ðŸš€ [QuantumLearner] Starting fit function...\n",
      "ðŸ“Š Dataset size: X_train=(4, 2), y_train=(4,)\n",
      "ðŸ”„ Reinitializing circuit...\n",
      "âœ… Initialized circuit with 2 qubits\n",
      "ðŸ“Œ Using batch size: 4\n",
      "ðŸš€ Training completed in 0.10 seconds\n",
      "âœ… All tests passed successfully!\n"
     ]
    }
   ],
   "source": [
    "def test_quantum_learner():\n",
    "    \"\"\"Test the QuantumLearner class with basic training and prediction.\"\"\"\n",
    "    np.random.seed(42)\n",
    "\n",
    "    # Sample training data (binary classification, {-1, 1} labels)\n",
    "    X_train = np.array([\n",
    "        [0.1, 0.2], \n",
    "        [0.2, 0.3], \n",
    "        [0.3, 0.4], \n",
    "        [0.4, 0.5]\n",
    "    ])\n",
    "    y_train = np.array([1, -1, 1, -1])  # Binary labels\n",
    "\n",
    "    # Initialize QuantumLearner\n",
    "    ql = QuantumLearner(num_layers=2)\n",
    "\n",
    "    # Ensure the circuit initializes correctly\n",
    "    ql._initialize_circuit(num_features=2)\n",
    "    assert ql.num_qubits == 2, \"Circuit initialization failed: Incorrect number of qubits.\"\n",
    "\n",
    "    # Test fitting the model\n",
    "    ql.fit(X_train, y_train, num_it=5)  # Small number of iterations for quick test\n",
    "\n",
    "    # Ensure the weights and bias are updated\n",
    "    assert ql.weights is not None, \"Weights were not initialized.\"\n",
    "    assert ql.bias is not None, \"Bias was not initialized.\"\n",
    "\n",
    "    # Test prediction on new samples\n",
    "    X_test = np.array([\n",
    "        [0.15, 0.25],\n",
    "        [0.35, 0.45]\n",
    "    ])\n",
    "    predictions = ql.predict(X_test)\n",
    "    \n",
    "    # Ensure output shape matches input shape\n",
    "    assert predictions.shape == (X_test.shape[0],), \"Prediction output shape mismatch.\"\n",
    "\n",
    "    print(\"âœ… All tests passed successfully!\")\n",
    "\n",
    "# Run the test\n",
    "test_quantum_learner()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "ðŸ”„ Episode 1 start\n",
      "ðŸ§­ Exploration: Selected feature 1\n",
      "ðŸš€ [QuantumLearner] Starting fit function...\n",
      "ðŸ“Š Dataset size: X_train=(4, 1), y_train=(4,)\n",
      "ðŸ”„ Reinitializing circuit...\n",
      "âœ… Initialized circuit with 1 qubits\n",
      "ðŸ“Œ Using batch size: 4\n",
      "ðŸš€ Training completed in 0.05 seconds\n",
      "ðŸŽ¯ Features: [np.int64(1)], Error: 0.2500, Reward: 0.2500\n",
      "ðŸ§­ Exploration: Selected feature 0\n",
      "ðŸš€ [QuantumLearner] Starting fit function...\n",
      "ðŸ“Š Dataset size: X_train=(4, 2), y_train=(4,)\n",
      "ðŸ”„ Reinitializing circuit...\n",
      "âœ… Initialized circuit with 2 qubits\n",
      "ðŸ“Œ Using batch size: 4\n",
      "ðŸš€ Training completed in 0.21 seconds\n",
      "ðŸŽ¯ Features: [np.int64(1), np.int64(0)], Error: 0.2500, Reward: 0.0000\n",
      "ðŸ”½ Epsilon after decay: 0.8000\n",
      "âœ… Episode 1 end\n",
      "\n",
      "ðŸ”„ Episode 2 start\n",
      "ðŸ§­ Exploration: Selected feature 1\n",
      "ðŸš€ [QuantumLearner] Starting fit function...\n",
      "ðŸ“Š Dataset size: X_train=(4, 1), y_train=(4,)\n",
      "ðŸ”„ Reinitializing circuit...\n",
      "âœ… Initialized circuit with 1 qubits\n",
      "ðŸ“Œ Using batch size: 4\n",
      "ðŸš€ Training completed in 0.04 seconds\n",
      "ðŸŽ¯ Features: [np.int64(1)], Error: 0.5000, Reward: 0.0000\n",
      "ðŸ§­ Exploration: Selected feature 0\n",
      "ðŸš€ [QuantumLearner] Starting fit function...\n",
      "ðŸ“Š Dataset size: X_train=(4, 2), y_train=(4,)\n",
      "ðŸ”„ Reinitializing circuit...\n",
      "âœ… Initialized circuit with 2 qubits\n",
      "ðŸ“Œ Using batch size: 4\n",
      "ðŸš€ Training completed in 0.08 seconds\n",
      "ðŸŽ¯ Features: [np.int64(1), np.int64(0)], Error: 0.2500, Reward: 0.2500\n",
      "ðŸ”½ Epsilon after decay: 0.7000\n",
      "âœ… Episode 2 end\n",
      "\n",
      "ðŸ”„ Episode 3 start\n",
      "ðŸ§­ Exploration: Selected feature 1\n",
      "ðŸš€ [QuantumLearner] Starting fit function...\n",
      "ðŸ“Š Dataset size: X_train=(4, 1), y_train=(4,)\n",
      "ðŸ”„ Reinitializing circuit...\n",
      "âœ… Initialized circuit with 1 qubits\n",
      "ðŸ“Œ Using batch size: 4\n",
      "ðŸš€ Training completed in 0.04 seconds\n",
      "ðŸŽ¯ Features: [np.int64(1)], Error: 0.5000, Reward: 0.0000\n",
      "ðŸ§­ Exploration: Selected feature 0\n",
      "ðŸš€ [QuantumLearner] Starting fit function...\n",
      "ðŸ“Š Dataset size: X_train=(4, 2), y_train=(4,)\n",
      "ðŸ”„ Reinitializing circuit...\n",
      "âœ… Initialized circuit with 2 qubits\n",
      "ðŸ“Œ Using batch size: 4\n",
      "ðŸš€ Training completed in 0.08 seconds\n",
      "ðŸŽ¯ Features: [np.int64(1), np.int64(0)], Error: 0.2500, Reward: 0.2500\n",
      "ðŸ”½ Epsilon after decay: 0.6000\n",
      "âœ… Episode 3 end\n",
      "\n",
      "ðŸ”„ Episode 4 start\n",
      "ðŸ§­ Exploration: Selected feature 1\n",
      "ðŸš€ [QuantumLearner] Starting fit function...\n",
      "ðŸ“Š Dataset size: X_train=(4, 1), y_train=(4,)\n",
      "ðŸ”„ Reinitializing circuit...\n",
      "âœ… Initialized circuit with 1 qubits\n",
      "ðŸ“Œ Using batch size: 4\n",
      "ðŸš€ Training completed in 0.04 seconds\n",
      "ðŸŽ¯ Features: [np.int64(1)], Error: 0.5000, Reward: 0.0000\n",
      "ðŸ§­ Exploration: Selected feature 0\n",
      "ðŸš€ [QuantumLearner] Starting fit function...\n",
      "ðŸ“Š Dataset size: X_train=(4, 2), y_train=(4,)\n",
      "ðŸ”„ Reinitializing circuit...\n",
      "âœ… Initialized circuit with 2 qubits\n",
      "ðŸ“Œ Using batch size: 4\n",
      "ðŸš€ Training completed in 0.08 seconds\n",
      "ðŸŽ¯ Features: [np.int64(1), np.int64(0)], Error: 0.2500, Reward: 0.2500\n",
      "ðŸ”½ Epsilon after decay: 0.5000\n",
      "âœ… Episode 4 end\n",
      "\n",
      "ðŸ”„ Episode 5 start\n",
      "ðŸ§­ Exploration: Selected feature 1\n",
      "ðŸš€ [QuantumLearner] Starting fit function...\n",
      "ðŸ“Š Dataset size: X_train=(4, 1), y_train=(4,)\n",
      "ðŸ”„ Reinitializing circuit...\n",
      "âœ… Initialized circuit with 1 qubits\n",
      "ðŸ“Œ Using batch size: 4\n",
      "ðŸš€ Training completed in 0.04 seconds\n",
      "ðŸŽ¯ Features: [np.int64(1)], Error: 0.5000, Reward: 0.0000\n",
      "ðŸ§­ Exploration: Selected feature 0\n",
      "ðŸš€ [QuantumLearner] Starting fit function...\n",
      "ðŸ“Š Dataset size: X_train=(4, 2), y_train=(4,)\n",
      "ðŸ”„ Reinitializing circuit...\n",
      "âœ… Initialized circuit with 2 qubits\n",
      "ðŸ“Œ Using batch size: 4\n",
      "ðŸš€ Training completed in 0.12 seconds\n",
      "ðŸŽ¯ Features: [np.int64(1), np.int64(0)], Error: 0.2500, Reward: 0.2500\n",
      "ðŸ”½ Epsilon after decay: 0.4000\n",
      "âœ… Episode 5 end\n",
      "\n",
      "ðŸ”„ Episode 6 start\n",
      "ðŸ“ˆ Exploitation: Selected feature 0\n",
      "ðŸš€ [QuantumLearner] Starting fit function...\n",
      "ðŸ“Š Dataset size: X_train=(4, 1), y_train=(4,)\n",
      "ðŸ”„ Reinitializing circuit...\n",
      "âœ… Initialized circuit with 1 qubits\n",
      "ðŸ“Œ Using batch size: 4\n",
      "ðŸš€ Training completed in 0.04 seconds\n",
      "ðŸŽ¯ Features: [0], Error: 0.5000, Reward: 0.0000\n",
      "ðŸ§­ Exploration: Selected feature 1\n",
      "ðŸš€ [QuantumLearner] Starting fit function...\n",
      "ðŸ“Š Dataset size: X_train=(4, 2), y_train=(4,)\n",
      "ðŸ”„ Reinitializing circuit...\n",
      "âœ… Initialized circuit with 2 qubits\n",
      "ðŸ“Œ Using batch size: 4\n",
      "ðŸš€ Training completed in 0.08 seconds\n",
      "ðŸŽ¯ Features: [0, np.int64(1)], Error: 0.2500, Reward: 0.2500\n",
      "ðŸ”½ Epsilon after decay: 0.3000\n",
      "âœ… Episode 6 end\n",
      "\n",
      "ðŸ”„ Episode 7 start\n",
      "ðŸ“ˆ Exploitation: Selected feature 0\n",
      "ðŸš€ [QuantumLearner] Starting fit function...\n",
      "ðŸ“Š Dataset size: X_train=(4, 1), y_train=(4,)\n",
      "ðŸ”„ Reinitializing circuit...\n",
      "âœ… Initialized circuit with 1 qubits\n",
      "ðŸ“Œ Using batch size: 4\n",
      "ðŸš€ Training completed in 0.04 seconds\n",
      "ðŸŽ¯ Features: [0], Error: 0.5000, Reward: 0.0000\n",
      "ðŸ§­ Exploration: Selected feature 1\n",
      "ðŸš€ [QuantumLearner] Starting fit function...\n",
      "ðŸ“Š Dataset size: X_train=(4, 2), y_train=(4,)\n",
      "ðŸ”„ Reinitializing circuit...\n",
      "âœ… Initialized circuit with 2 qubits\n",
      "ðŸ“Œ Using batch size: 4\n",
      "ðŸš€ Training completed in 0.08 seconds\n",
      "ðŸŽ¯ Features: [0, np.int64(1)], Error: 0.2500, Reward: 0.2500\n",
      "ðŸ”½ Epsilon after decay: 0.2000\n",
      "âœ… Episode 7 end\n",
      "\n",
      "ðŸ”„ Episode 8 start\n",
      "ðŸ“ˆ Exploitation: Selected feature 0\n",
      "ðŸš€ [QuantumLearner] Starting fit function...\n",
      "ðŸ“Š Dataset size: X_train=(4, 1), y_train=(4,)\n",
      "ðŸ”„ Reinitializing circuit...\n",
      "âœ… Initialized circuit with 1 qubits\n",
      "ðŸ“Œ Using batch size: 4\n",
      "ðŸš€ Training completed in 0.04 seconds\n",
      "ðŸŽ¯ Features: [0], Error: 0.5000, Reward: 0.0000\n",
      "ðŸ§­ Exploration: Selected feature 1\n",
      "ðŸš€ [QuantumLearner] Starting fit function...\n",
      "ðŸ“Š Dataset size: X_train=(4, 2), y_train=(4,)\n",
      "ðŸ”„ Reinitializing circuit...\n",
      "âœ… Initialized circuit with 2 qubits\n",
      "ðŸ“Œ Using batch size: 4\n",
      "ðŸš€ Training completed in 0.08 seconds\n",
      "ðŸŽ¯ Features: [0, np.int64(1)], Error: 0.2500, Reward: 0.2500\n",
      "ðŸ”½ Epsilon after decay: 0.1000\n",
      "âœ… Episode 8 end\n",
      "\n",
      "ðŸ”„ Episode 9 start\n",
      "ðŸ“ˆ Exploitation: Selected feature 0\n",
      "ðŸš€ [QuantumLearner] Starting fit function...\n",
      "ðŸ“Š Dataset size: X_train=(4, 1), y_train=(4,)\n",
      "ðŸ”„ Reinitializing circuit...\n",
      "âœ… Initialized circuit with 1 qubits\n",
      "ðŸ“Œ Using batch size: 4\n",
      "ðŸš€ Training completed in 0.04 seconds\n",
      "ðŸŽ¯ Features: [0], Error: 0.5000, Reward: 0.0000\n",
      "ðŸ§­ Exploration: Selected feature 1\n",
      "ðŸš€ [QuantumLearner] Starting fit function...\n",
      "ðŸ“Š Dataset size: X_train=(4, 2), y_train=(4,)\n",
      "ðŸ”„ Reinitializing circuit...\n",
      "âœ… Initialized circuit with 2 qubits\n",
      "ðŸ“Œ Using batch size: 4\n",
      "ðŸš€ Training completed in 0.07 seconds\n",
      "ðŸŽ¯ Features: [0, np.int64(1)], Error: 0.2500, Reward: 0.2500\n",
      "ðŸ”½ Epsilon after decay: 0.1000\n",
      "âœ… Episode 9 end\n",
      "\n",
      "ðŸ”„ Episode 10 start\n",
      "ðŸ“ˆ Exploitation: Selected feature 0\n",
      "ðŸš€ [QuantumLearner] Starting fit function...\n",
      "ðŸ“Š Dataset size: X_train=(4, 1), y_train=(4,)\n",
      "ðŸ”„ Reinitializing circuit...\n",
      "âœ… Initialized circuit with 1 qubits\n",
      "ðŸ“Œ Using batch size: 4\n",
      "ðŸš€ Training completed in 0.04 seconds\n",
      "ðŸŽ¯ Features: [0], Error: 0.5000, Reward: 0.0000\n",
      "ðŸ§­ Exploration: Selected feature 1\n",
      "ðŸš€ [QuantumLearner] Starting fit function...\n",
      "ðŸ“Š Dataset size: X_train=(4, 2), y_train=(4,)\n",
      "ðŸ”„ Reinitializing circuit...\n",
      "âœ… Initialized circuit with 2 qubits\n",
      "ðŸ“Œ Using batch size: 4\n",
      "ðŸš€ Training completed in 0.08 seconds\n",
      "ðŸŽ¯ Features: [0, np.int64(1)], Error: 0.2500, Reward: 0.2500\n",
      "ðŸ”½ Epsilon after decay: 0.1000\n",
      "âœ… Episode 10 end\n"
     ]
    },
    {
     "ename": "AssertionError",
     "evalue": "Rewards did not improve over episodes.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAssertionError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[49], line 73\u001b[0m\n\u001b[1;32m     70\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124mâœ… All tests passed successfully!\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m     72\u001b[0m \u001b[38;5;66;03m# Run the test\u001b[39;00m\n\u001b[0;32m---> 73\u001b[0m \u001b[43mtest_quantum_learner_multiple_episodes\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[0;32mIn[49], line 68\u001b[0m, in \u001b[0;36mtest_quantum_learner_multiple_episodes\u001b[0;34m()\u001b[0m\n\u001b[1;32m     66\u001b[0m \u001b[38;5;66;03m# Check if rewards improve over episodes\u001b[39;00m\n\u001b[1;32m     67\u001b[0m \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(rewards) \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m0\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mNo rewards were calculated.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m---> 68\u001b[0m \u001b[38;5;28;01massert\u001b[39;00m rewards[\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m] \u001b[38;5;241m>\u001b[39m rewards[\u001b[38;5;241m0\u001b[39m], \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mRewards did not improve over episodes.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m     70\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124mâœ… All tests passed successfully!\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "\u001b[0;31mAssertionError\u001b[0m: Rewards did not improve over episodes."
     ]
    }
   ],
   "source": [
    "def test_quantum_learner_multiple_episodes():\n",
    "    \"\"\"Test the QuantumLearner class with multiple episodes to observe improvement.\"\"\"\n",
    "    np.random.seed(42)\n",
    "\n",
    "    # Sample training data (binary classification, {-1, 1} labels)\n",
    "    X_train = np.array([\n",
    "        [0.1, 0.2], \n",
    "        [0.2, 0.3], \n",
    "        [0.3, 0.4], \n",
    "        [0.4, 0.5]\n",
    "    ])\n",
    "    y_train = np.array([1, -1, 1, -1])  # Binary labels\n",
    "\n",
    "    # Initialize QuantumLearner\n",
    "    ql = QuantumLearner(num_layers=2)\n",
    "\n",
    "    # Ensure the circuit initializes correctly\n",
    "    ql._initialize_circuit(num_features=2)\n",
    "    assert ql.num_qubits == 2, \"Circuit initialization failed: Incorrect number of qubits.\"\n",
    "\n",
    "    # Parameters for multiple episodes\n",
    "    num_episodes = 10\n",
    "    epsilon = 0.9  # Initial exploration rate\n",
    "    epsilon_decay = 0.1  # Decay rate for epsilon\n",
    "    learning_rate = 0.1\n",
    "    rewards = []\n",
    "\n",
    "    for episode in range(num_episodes):\n",
    "        print(f\"\\nðŸ”„ Episode {episode + 1} start\")\n",
    "\n",
    "        # Simulate feature selection process\n",
    "        selected_features = []\n",
    "        available_features = list(range(X_train.shape[1]))\n",
    "        last_error = 0.5  # Initialize error\n",
    "\n",
    "        while available_features:\n",
    "            # Exploration vs. exploitation\n",
    "            explore = np.random.rand() < epsilon\n",
    "            if explore:\n",
    "                action = np.random.choice(available_features)  # Randomly select a feature\n",
    "                print(f\"ðŸ§­ Exploration: Selected feature {action}\")\n",
    "            else:\n",
    "                action = available_features[0]  # Exploit (select the first available feature)\n",
    "                print(f\"ðŸ“ˆ Exploitation: Selected feature {action}\")\n",
    "\n",
    "            # Update selected features and remove from available features\n",
    "            selected_features.append(action)\n",
    "            available_features.remove(action)\n",
    "\n",
    "            # Simulate training with selected features\n",
    "            X_train_selected = X_train[:, selected_features]\n",
    "            ql.fit(X_train_selected, y_train, num_it=5)  # Train the learner\n",
    "            error = 1 - np.mean(ql.predict(X_train_selected) == y_train)  # Calculate error\n",
    "            reward = last_error - error  # Calculate reward\n",
    "            rewards.append(reward)\n",
    "            last_error = error\n",
    "\n",
    "            print(f\"ðŸŽ¯ Features: {selected_features}, Error: {error:.4f}, Reward: {reward:.4f}\")\n",
    "\n",
    "        # Decay epsilon to reduce exploration over time\n",
    "        epsilon = max(0.1, epsilon - epsilon_decay)\n",
    "        print(f\"ðŸ”½ Epsilon after decay: {epsilon:.4f}\")\n",
    "\n",
    "        print(f\"âœ… Episode {episode + 1} end\")\n",
    "\n",
    "    # Check if rewards improve over episodes\n",
    "    assert len(rewards) > 0, \"No rewards were calculated.\"\n",
    "    assert rewards[-1] > rewards[0], \"Rewards did not improve over episodes.\"\n",
    "\n",
    "    print(\"\\nâœ… All tests passed successfully!\")\n",
    "\n",
    "# Run the test\n",
    "test_quantum_learner_multiple_episodes()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
