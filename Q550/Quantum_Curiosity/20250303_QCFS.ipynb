{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    },
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "#### 1. Importing all the libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "import shutil\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from matplotlib.collections import LineCollection\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import pennylane as qml\n",
    "\n",
    "from sklearn import tree, metrics\n",
    "from sklearn.model_selection import train_test_split, cross_val_score\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.ensemble import AdaBoostClassifier, GradientBoostingClassifier, ExtraTreesClassifier\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "#### 2. Define the Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "# Function that create the episode data - sample randomaly\n",
    "def get_data(episode_size,policy,mode):\n",
    "    global dataset\n",
    "    if mode=='train':\n",
    "        if policy==0:\n",
    "             dataset=data.sample(n=episode_size)\n",
    "        else:\n",
    "            dataset=data\n",
    "    else:\n",
    "        dataset = pd.read_csv(location + '/' + file +'_test_int.csv', index_col=0)\n",
    "    return dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "# Function that separate the episode data into features and label\n",
    "def data_separate (dataset):\n",
    "    global X\n",
    "    global y    \n",
    "    X = dataset.iloc[:,0:dataset.shape[1]-1]  # all rows, all the features and no labels\n",
    "    y = dataset.iloc[:, -1]  # all rows, label only\n",
    "    return X,y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "# Function that split the episode data into train and test\n",
    "def data_split(X,y):\n",
    "    global X_train_main\n",
    "    global X_test_main   \n",
    "    global y_train\n",
    "    global y_test  \n",
    "    from sklearn.model_selection import train_test_split\n",
    "    X_train_main, X_test_main, y_train, y_test = train_test_split(X,y,test_size=0.2,random_state=4)\n",
    "    return X_train_main, X_test_main, y_train, y_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "# Function that chooses exploration or explotation method\n",
    "def exploration_explotation(epsilon):\n",
    "    global exploration \n",
    "    if np.random.rand() < epsilon:  \n",
    "        exploration=1\n",
    "    else:\n",
    "        exploration=0    \n",
    "    return exploration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "# Function that returns all available actions in the state given as an argument: \n",
    "def available_actions(number_of_columns,columns,initial_state,current_state,trashold, exploration):\n",
    "    global exclude\n",
    "    global all_columns\n",
    "#    exclude=[]\n",
    "    all_columns=np.arange(number_of_columns+1)\n",
    "    # remove columns that have been already selected\n",
    "    exclude=columns.copy()\n",
    "    # remove the initial_state and the current_state\n",
    "    exclude.extend([initial_state, current_state])\n",
    "    available_act = list(set(all_columns)-set(exclude))\n",
    "    # remove actions that have negetiv Q value\n",
    "    if exploration==0:\n",
    "        index = np.where(Q[current_state,available_act] > trashold)[1]\n",
    "        available_act= [available_act[i] for i in index.tolist()]\n",
    "    return available_act"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "def sample_next_action(current_state, Q, available_act, exploration):\n",
    "    global available_act_q_value\n",
    "    available_act_q_value = [float(q) for q in np.array(Q[current_state, available_act]).reshape(-1)]\n",
    "    \n",
    "    if exploration == 1: \n",
    "        # Random selection\n",
    "        next_action = int(np.random.choice(available_act, 1).item())\n",
    "    else: \n",
    "        # Greedy selection according to max value\n",
    "        maxQ = max(available_act_q_value)\n",
    "        count = available_act_q_value.count(maxQ)\n",
    "        \n",
    "        if count > 1:\n",
    "            max_columns = [i for i in range(len(available_act_q_value)) if available_act_q_value[i] == maxQ]\n",
    "            i = int(np.random.choice(max_columns, 1).item())\n",
    "        else:\n",
    "            i = available_act_q_value.index(maxQ)\n",
    "        \n",
    "        next_action = available_act[i]  \n",
    "    \n",
    "    return next_action\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "# function that update a list with all selected columns in the episode\n",
    "def update_columns(action, columns):\n",
    "    update_columns=columns\n",
    "    update_columns.append(action)\n",
    "    return update_columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "# function that update the X_train and X_test according to the current episode columns list \n",
    "def update_X_train_X_test(columns,X_train_main, X_test_main):\n",
    "    X_train=X_train_main.iloc[:,columns]\n",
    "    X_test=X_test_main.iloc[:,columns]\n",
    "    X_train=pd.DataFrame(X_train)\n",
    "    X_test=pd.DataFrame(X_test)\n",
    "    return X_train, X_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "# Function that run the learner and get the error to the current episode columns list\n",
    "def Learner(X_train, X_test,y_train, y_test):\n",
    "    global learner\n",
    "    global y_pred\n",
    "    if learner_model == 'DT':\n",
    "        learner = tree.DecisionTreeClassifier()\n",
    "        learner = learner.fit(X_train, y_train)\n",
    "        y_pred = learner.predict(X_test)\n",
    "    elif learner_model == 'KNN':\n",
    "        learner = KNeighborsClassifier(metric='hamming',n_neighbors=5)\n",
    "        learner = learner.fit(X_train, y_train)\n",
    "        y_pred = learner.predict(X_test)        \n",
    "    elif learner_model == 'SVM':\n",
    "        learner = SVC()\n",
    "        learner = learner.fit(X_train, y_train)\n",
    "        y_pred = learner.predict(X_test)        \n",
    "    elif learner_model == 'NB':\n",
    "        learner = MultinomialNB()\n",
    "        learner = learner.fit(X_train, y_train)\n",
    "        y_pred = learner.predict(X_test)\n",
    "    elif learner_model == 'AB':\n",
    "        learner = AdaBoostClassifier()\n",
    "        learner = learner.fit(X_train, y_train)\n",
    "        y_pred = learner.predict(X_test)  \n",
    "    elif learner_model == 'GB':\n",
    "        learner = GradientBoostingClassifier()\n",
    "        learner = learner.fit(X_train, y_train)\n",
    "        y_pred = learner.predict(X_test)  \n",
    "    elif learner_model == 'VQC':\n",
    "        learner = QuantumLearner()\n",
    "        learner = learner.fit(X_train, y_train)\n",
    "        y_pred = learner.predict(X_test)  \n",
    "    elif learner_model == 'ANN':\n",
    "        learner = ClassicalLearner()\n",
    "        learner = learner.fit(X_train, y_train)\n",
    "        y_pred = learner.predict(X_test)  \n",
    "    accuracy=metrics.accuracy_score(y_test, y_pred)\n",
    "    error=1-accuracy\n",
    "    return error\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "def q_update(current_state, action, learning_rate, reward):\n",
    "    # next_state = current action\n",
    "    max_index = np.where(Q[action,] == np.max(Q[action,]))[0]  # Use [0] instead of [1] for 1D arrays\n",
    "    \n",
    "    if max_index.shape[0] > 1:\n",
    "        # Resolve tie by selecting one randomly\n",
    "        max_index = int(np.random.choice(max_index, size=1).item())\n",
    "    else:\n",
    "        max_index = int(max_index[0])  # Convert the first element to a scalar\n",
    "\n",
    "    max_value = Q[action, max_index]\n",
    "\n",
    "    # Update the Q matrix\n",
    "    if Q[current_state, action] == 1:\n",
    "        Q[current_state, action] = learning_rate * reward\n",
    "    else:\n",
    "        Q[current_state, action] = Q[current_state, action] + learning_rate * (\n",
    "            reward + (discount_factor * max_value) - Q[current_state, action]\n",
    "        )\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Experiment mangment"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "#### 3. Define the parameters "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "## for run time ##\n",
    "N_features=2\n",
    "N_data=1\n",
    "## for run time ##\n",
    "\n",
    "#Experiment: \n",
    "experiment='test'\n",
    "number_of_experiment=1\n",
    "\n",
    "# Dataset parameters #\n",
    "location = 'Datasets/adult'\n",
    "outputlocation='Datasets'\n",
    "file='adult' #adult #diabetic_data #no_show\n",
    "#np.random.seed(3)\n",
    "\n",
    "# Q learning parameter # \n",
    "learning_rate=0.005\n",
    "discount_factor = 0.01 #0\n",
    "epsilon = 0.1\n",
    "\n",
    "# Learner and episode parameters #\n",
    "learner_model = 'VQC' #DT #KNN #SVM\n",
    "episode_size=100\n",
    "internal_trashold=0\n",
    "external_trashold=0\n",
    "filename= file +'_int.csv'\n",
    "\n",
    "#Experiments folder management: \n",
    "#if not os.path.exists('/Experiments'):\n",
    "#    os.makedirs('/Experiments') \n",
    "if not os.path.exists('Experiments/'+ str(experiment)):\n",
    "    os.makedirs('Experiments/'+ str(experiment))\n",
    "else:\n",
    "    shutil.rmtree('Experiments/'+ str(experiment))          #removes all the subdirectories!\n",
    "    os.makedirs('Experiments/'+ str(experiment))\n",
    "#writer = pd.ExcelWriter('Experiments/'+ str(experiment) + '/df.xlsx') \n",
    "\n",
    "\n",
    "\n",
    "text_file = open('Experiments/'+ str(experiment) +'/parameters.txt', \"w\")\n",
    "text_file.write('experiment: ' + str(experiment)+ '\\n')\n",
    "text_file.write('number of experiments: ' + str(number_of_experiment)+ '\\n')\n",
    "text_file.write('file: ' + str(file)+ '\\n')\n",
    "text_file.write('learner model: ' + str(learner_model)+ '\\n')\n",
    "text_file.write('episode size: ' + str(episode_size)+ '\\n')\n",
    "#text_file.write('numbers of epocs: ' + str(epocs)+ '\\n')\n",
    "text_file.write('internal trashold: ' + str(internal_trashold)+ '\\n')\n",
    "text_file.write('external trashold: ' + str(external_trashold)+ '\\n')\n",
    " \n",
    "text_file.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "#### 4. Run all experiments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Experiments 0 start\n",
      "number of columns: 2 (exclude class column)\n",
      "Number of episodes: 2443.0\n",
      "initial state number: 2 (the last dummy column we have created)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "KeyboardInterrupt\n",
      "\n"
     ]
    }
   ],
   "source": [
    "for e in range (number_of_experiment):\n",
    "    if not os.path.exists('Experiments/'+ str(experiment)+ '/'+ str(e)):\n",
    "        os.makedirs('Experiments/'+ str(experiment)+ '/'+ str(e))\n",
    "    else:\n",
    "        shutil.rmtree('Experiments/'+ str(experiment)+ '/'+ str(e))          #removes all the subdirectories!\n",
    "        os.makedirs('Experiments/'+ str(experiment)+ '/'+ str(e))\n",
    "    print ('Experiments ' + str(e) + ' start')\n",
    "##########################Experiment setup##########################\n",
    "    # Read the data\n",
    "    data = pd.read_csv(location + '/' + filename, index_col=0)\n",
    "    \n",
    "##### for run time - start #####\n",
    "    import timeit\n",
    "    start = timeit.default_timer()\n",
    "    size= int(N_data* len(data.index))\n",
    "    data = data.sample(n=size)\n",
    "    data=data.iloc[:,-N_features-1:]\n",
    "##### for run time - end #####\n",
    "    \n",
    "    #Set the number of iterations:\n",
    "    interations=10*len(data.index)/episode_size\n",
    "    # Set the number of columns exclude the class column\n",
    "    number_of_columns=data.shape[1]-1 \n",
    "    print (\"number of columns: \"+ str(number_of_columns) +\" (exclude class column)\" ) \n",
    "    # Set the number of episodes \n",
    "    # episodes_number=epocs*len(data.index)/episode_size\n",
    "    episodes_number=interations\n",
    "    print (\"Number of episodes: \"+ str(episodes_number) ) \n",
    "    # Initialize matrix Q as a 1 values matrix:\n",
    "    #Q = np.matrix(np.ones([number_of_columns+1,number_of_columns+1])) # we will use the last dummy columns as initial state s\n",
    "    Q = np.matrix(np.ones([number_of_columns+1,number_of_columns+1])) # we will use the last dummy columns as initial state s\n",
    "    # Set initial_state to be the last dummy column we have created\n",
    "    initial_state=number_of_columns\n",
    "    # define data frame to save episode policies results\n",
    "    df = pd.DataFrame(columns=('episode','episode_columns','policy_columns','policy_accuracy_train','policy_accuracy_test'))\n",
    "    print (\"initial state number: \"+ str(initial_state) + \" (the last dummy column we have created)\") \n",
    "\n",
    "    ##########################  episode  ##########################  \n",
    "    for i in range (int(episodes_number)):\n",
    "    ########## Begining of episode  ############\n",
    "        # Initiate lists for available_act, episode_columns and and the policy mode & episode_error\n",
    "        episode_available_act=list(np.arange(number_of_columns))\n",
    "        episode_columns=[]\n",
    "        policy=0\n",
    "        episode_error=0\n",
    "        # Initiate the error to 0.5\n",
    "        episode_last_error=0.5\n",
    "        # Initiate current_state to be initial_state\n",
    "        episode_current_state=initial_state\n",
    "        # Create the episode data \n",
    "        episode= get_data(episode_size, policy=0, mode='train')\n",
    "        # Separate the episode data into features and label\n",
    "        X_episode,y_episode=data_separate(episode)\n",
    "        # Split the data into train and test \n",
    "        X_train_main_episode, X_test_main_episode, y_train_episode, y_test_episode = data_split(X_episode,y_episode)\n",
    "        if i<episodes_number*0.25:\n",
    "            epsilon=0.9\n",
    "            learning_rate=0.09\n",
    "        elif i<episodes_number*0.5:\n",
    "            epsilon=0.5\n",
    "            learning_rate=0.05\n",
    "        elif i<episodes_number*0.75:\n",
    "            epsilon=0.3\n",
    "            learning_rate=0.01\n",
    "        else:\n",
    "            epsilon=0.1\n",
    "            learning_rate=0.005\n",
    "        ########## Q learning start ############\n",
    "\n",
    "        while len(episode_available_act)>0:\n",
    "            # Get exploration or explotation flag \n",
    "            exploration=exploration_explotation(epsilon)\n",
    "            # Get available actions in the current state\n",
    "            episode_available_act = available_actions(number_of_columns,episode_columns,initial_state,episode_current_state,internal_trashold,exploration)\n",
    "            if len(episode_available_act)>0:\n",
    "                # Sample next action to be performed\n",
    "                episode_action = sample_next_action(episode_current_state, Q, episode_available_act, exploration)\n",
    "                # Update the episode_columns\n",
    "                episode_columns=update_columns(episode_action,episode_columns)\n",
    "                # Update the dataset to include all episode columns + current selected action (column)\n",
    "                X_train_episode, X_test_episode =update_X_train_X_test(episode_columns,X_train_main_episode, X_test_main_episode)\n",
    "                # Update the accuracy of the current columns\n",
    "                episode_error= Learner(X_train_episode, X_test_episode, y_train_episode, y_test_episode)\n",
    "                # Update reward\n",
    "                episode_reward=episode_last_error-episode_error\n",
    "                # Update Q matrix\n",
    "                q_update(episode_current_state,episode_action,learning_rate, episode_reward)\n",
    "                # Update parameters for next round \n",
    "#                 if episode_current_state==initial_state:\n",
    "#                     beta=abs(episode_reward-Q[episode_current_state,episode_action])\n",
    "#                     epsilon=final_epsilon+(beta*(1-final_epsilon))\n",
    "                #    learning_rate=final_learning_rate+(beta*(1-final_learning_rate))\n",
    "                episode_current_state=episode_action\n",
    "                episode_last_error=episode_error\n",
    "                 \n",
    "        ########## Q learning End ############\n",
    "\n",
    "        #Save Q matrix: \n",
    "        if (i%100 ==0):\n",
    "            Q_save=pd.DataFrame(Q)\n",
    "            Q_save.to_csv('Experiments/'+ str(experiment)+ '/'+ str(e)+ '/Q.'+ str(i+1) + '.csv') \n",
    "\n",
    "        # Calculate policy \n",
    "        policy_available_actions=list(np.arange(number_of_columns))\n",
    "        policy_columns=[]\n",
    "        policy_current_state=initial_state\n",
    "        while len(policy_available_actions)>0:\n",
    "            # Get available actions in the current state\n",
    "            policy_available_actions = available_actions(number_of_columns,policy_columns,initial_state,policy_current_state, external_trashold, exploration=0)\n",
    "            # # Sample next action to be performed\n",
    "            if len(policy_available_actions)>0:\n",
    "                policy_select_action = sample_next_action(policy_current_state, Q, policy_available_actions, exploration=0)\n",
    "                # Update the episode_columns\n",
    "                policy_columns=update_columns(policy_select_action,policy_columns)\n",
    "                policy_current_state=policy_select_action\n",
    "        # Calculate policy_accuracy    \n",
    "        if len(policy_columns)>0:\n",
    "            ##for training dataset##\n",
    "            policy_data=get_data(episode_size,policy=1,mode='train')\n",
    "            X_policy,y_policy=data_separate(policy_data)\n",
    "            X_train_main_policy, X_test_main_policy, y_train_policy, y_test_policy = data_split(X,y)\n",
    "            X_train_policy, X_test_policy =update_X_train_X_test(policy_columns, X_train_main_policy, X_test_main_policy)\n",
    "            policy_error=Learner(X_train_policy, X_test_policy,y_train_policy, y_test_policy)\n",
    "            policy_accuracy_train=1-policy_error\n",
    "            ##for testing dataset##\n",
    "            policy_data=get_data(episode_size,policy=1,mode='test')\n",
    "            X_policy,y_policy=data_separate(policy_data)\n",
    "            X_train_main_policy, X_test_main_policy, y_train_policy, y_test_policy = data_split(X,y)\n",
    "            X_train_policy, X_test_policy =update_X_train_X_test(policy_columns, X_train_main_policy, X_test_main_policy)\n",
    "            policy_error=Learner(X_train_policy, X_test_policy,y_train_policy, y_test_policy)\n",
    "            policy_accuracy_test=1-policy_error \n",
    "        else:\n",
    "            policy_accuracy_train=0 \n",
    "            policy_accuracy_test=0\n",
    "        #df=df.append({'episode':str(i+1), 'episode_columns':str(episode_columns),'policy_columns':str(policy_columns),'policy_accuracy_train':policy_accuracy_train,'policy_accuracy_test':policy_accuracy_test}, ignore_index=True)\n",
    "        #new_row = pd.DataFrame([{'episode': str(i+1),\n",
    "        #                  'episode_columns': str(episode_columns),\n",
    "        #                  'policy_columns': str(policy_columns),\n",
    "        #                  'policy_accuracy_train': policy_accuracy_train,\n",
    "        #                  'policy_accuracy_test': policy_accuracy_test}])\n",
    "        #df = pd.concat([df, new_row], ignore_index=True)\n",
    "        df.loc[len(df)] = {\n",
    "            'episode': str(i+1),\n",
    "            'episode_columns': str(episode_columns),\n",
    "            'policy_columns': str(policy_columns),\n",
    "            'policy_accuracy_train': policy_accuracy_train,\n",
    "            'policy_accuracy_test': policy_accuracy_test\n",
    "        }\n",
    "\n",
    "        #Prints\n",
    "        print (\"episode \"+ str(i+1) +\" start\") \n",
    "        print (\"episode columns: \"+ str(episode_columns) + \" epsilon: \" + str(epsilon) + \" learning rate: \" + str(learning_rate) + \" error: \" +str(episode_error))\n",
    "        print (\"episode policy:\" + str(policy_columns) + \" train accuracy: \" + str(policy_accuracy_train)  + \" test accuracy: \" +str(policy_accuracy_test)) \n",
    "        print (\"episode \"+ str(i+1) +\" end\") \n",
    "    ########## End of episode  ############\n",
    "    #df.to_excel(writer, 'Experiment' + str(e))\n",
    "    df.to_excel(writer, sheet_name='Experiment' + str(e))\n",
    "    df_plot=df[['episode','policy_accuracy_train','policy_accuracy_test']]\n",
    "    plot=df_plot.plot()\n",
    "    fig = plot.get_figure()\n",
    "    fig.savefig('Experiments/'+ str(experiment) + '/plot_experiment_' + str(e) +'.png')\n",
    "    \n",
    "#writer.save()\n",
    "with pd.ExcelWriter('Experiments/'+ str(experiment) + '/df.xlsx') as writer:\n",
    "    df.to_excel(writer, sheet_name='Experiment' + str(e))\n",
    "\n",
    "## for run time ##\n",
    "stop = timeit.default_timer()\n",
    "print (stop - start)\n",
    "## for run time ##"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Quantum Learner based on Benedetti et al.\n",
    "class QuantumLearner:\n",
    "    def __init__(self, num_layers=2):\n",
    "        self.num_layers = num_layers\n",
    "        self.weights = None\n",
    "        self.bias = None\n",
    "        self.num_qubits = None\n",
    "        self.dev = None\n",
    "        self.opt = qml.optimize.AdamOptimizer(0.05)\n",
    "\n",
    "    def _initialize_circuit(self, num_features):\n",
    "        # Update the number of qubits to match the current feature count.\n",
    "        self.num_qubits = num_features\n",
    "        self.dev = qml.device('qiskit.aer', wires=list(range(self.num_qubits)))\n",
    "        #self.dev = qml.device(\"default.qubit\", wires=list(range(self.num_qubits)))\n",
    "\n",
    "        @qml.qnode(self.dev, interface=\"autograd\")\n",
    "        def circuit(weights, x):\n",
    "            self.feature_encoding(x)\n",
    "            for W in weights:\n",
    "                self.variational_layer(W)\n",
    "            return qml.expval(qml.PauliZ(0))\n",
    "        \n",
    "        self.circuit = circuit\n",
    "\n",
    "    def feature_encoding(self, x):\n",
    "        for i in range(self.num_qubits):\n",
    "            qml.RY(np.pi * x[i], wires=i)\n",
    "        for i in range(self.num_qubits - 1):\n",
    "            qml.CZ(wires=[i, i + 1])\n",
    "\n",
    "    def variational_layer(self, W):\n",
    "        for i in range(self.num_qubits):\n",
    "            qml.Rot(W[i, 0], W[i, 1], W[i, 2], wires=i)\n",
    "        for i in range(self.num_qubits - 1):\n",
    "            qml.CNOT(wires=[i, i + 1])\n",
    "        if self.num_qubits > 1:\n",
    "            qml.CNOT(wires=[self.num_qubits - 1, 0])\n",
    "\n",
    "    def variational_classifier(self, weights, bias, x):\n",
    "        return self.circuit(weights, x) + bias\n",
    "\n",
    "    def cost(self, weights, bias, X, Y):\n",
    "        # X is expected to be a NumPy array.\n",
    "        predictions = qml.numpy.array([self.variational_classifier(weights, bias, x) for x in X])\n",
    "        return qml.numpy.mean((qml.numpy.array(Y) - predictions) ** 2)\n",
    "\n",
    "    def fit(self, X_train, y_train, num_it=100, batch_size=48, warm_start=False):\n",
    "        # Convert inputs to NumPy arrays if needed.\n",
    "        if hasattr(X_train, \"values\"):\n",
    "            X_train = X_train.values.astype(np.float64)\n",
    "        else:\n",
    "            X_train = np.array(X_train, dtype=np.float64)\n",
    "        if hasattr(y_train, \"values\"):\n",
    "            y_train = y_train.values\n",
    "        else:\n",
    "            y_train = np.array(y_train)\n",
    "        \n",
    "        current_features = X_train.shape[1]\n",
    "        \n",
    "        # If not warm starting or if the number of features has changed, reinitialize the circuit and parameters.\n",
    "        if not warm_start or (self.num_qubits is None) or (self.num_qubits != current_features):\n",
    "            self._initialize_circuit(current_features)\n",
    "            np.random.seed(0)\n",
    "            self.weights = qml.numpy.tensor(\n",
    "                0.01 * np.random.randn(self.num_layers, self.num_qubits, 3),\n",
    "                requires_grad=True,\n",
    "            )\n",
    "            self.bias = qml.numpy.tensor(0.0, requires_grad=True)\n",
    "\n",
    "        batch_size = min(batch_size, len(X_train))\n",
    "        for it in range(num_it):\n",
    "            batch_index = np.random.choice(len(X_train), batch_size, replace=False)\n",
    "            X_batch = X_train[batch_index]\n",
    "            Y_batch = y_train[batch_index]\n",
    "            self.weights, self.bias = self.opt.step(\n",
    "                lambda w, b: self.cost(w, b, X_batch, Y_batch), self.weights, self.bias\n",
    "            )\n",
    "        return self\n",
    "\n",
    "    def predict(self, X_test):\n",
    "        if hasattr(X_test, \"values\"):\n",
    "            X_test = X_test.values.astype(np.float64)\n",
    "        else:\n",
    "            X_test = np.array(X_test, dtype=np.float64)\n",
    "        return np.array([\n",
    "            float(qml.numpy.sign(self.variational_classifier(self.weights, self.bias, x)))\n",
    "            for x in X_test\n",
    "        ])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Classical Learner based on a simple ANN\n",
    "class ClassicalLearner(nn.Module):\n",
    "    def __init__(self, num_layers=2, hidden_size=5):\n",
    "        super().__init__()\n",
    "        self.layers = None\n",
    "        self.sigmoid = nn.Sigmoid()\n",
    "\n",
    "    def initialize_layers(self, input_size, num_layers, hidden_size=5):\n",
    "        layers = [input_size] + [hidden_size] * (num_layers - 1) + [1]\n",
    "        self.layers = nn.ModuleList([nn.Linear(layers[i], layers[i+1], dtype=torch.float64) for i in range(len(layers) - 1)])\n",
    "    \n",
    "    def forward(self, x):\n",
    "        x = x.to(torch.float64)\n",
    "        for layer in self.layers[:-1]:\n",
    "            x = torch.relu(layer(x))\n",
    "        return self.sigmoid(self.layers[-1](x))\n",
    "    \n",
    "    def fit(self, X_train, y_train, num_it=50, lr=0.01):\n",
    "        input_size = X_train.shape[1]\n",
    "        self.initialize_layers(input_size, num_layers=2)\n",
    "        optimizer = optim.Adam(self.parameters(), lr=lr)\n",
    "        y_train = torch.tensor(y_train.values, dtype=torch.float64).reshape(-1, 1)\n",
    "        \n",
    "        for epoch in range(num_it):\n",
    "            optimizer.zero_grad()\n",
    "            y_pred = self.forward(torch.tensor(X_train.values, dtype=torch.float64)).reshape(-1, 1)\n",
    "            loss = nn.BCELoss()(y_pred, y_train)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "        \n",
    "        return self\n",
    "    \n",
    "    def predict(self, X_test):\n",
    "        with torch.no_grad():\n",
    "            X_test_tensor = torch.tensor(X_test.values, dtype=torch.float64)\n",
    "            y_pred = self.forward(X_test_tensor).reshape(-1, 1)\n",
    "            return (y_pred.numpy().flatten() > 0.5).astype(int)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Quantum Learner based on Benedetti et al.\n",
    "class QuantumLearner:\n",
    "    def __init__(self, num_layers=2):\n",
    "        self.num_layers = num_layers\n",
    "        self.weights = None\n",
    "        self.bias = None\n",
    "        self.num_qubits = None\n",
    "        self.dev = None\n",
    "        self.opt = qml.optimize.AdamOptimizer(0.05)\n",
    "\n",
    "    def _initialize_circuit(self, num_features):\n",
    "        # Update number of qubits (and rebuild device) to match the current feature count.\n",
    "        self.num_qubits = num_features\n",
    "        self.dev = qml.device(\"default.qubit\", wires=list(range(self.num_qubits)))\n",
    "\n",
    "        @qml.qnode(self.dev, interface=\"autograd\")\n",
    "        def circuit(weights, x):\n",
    "            self.feature_encoding(x)\n",
    "            for W in weights:\n",
    "                self.variational_layer(W)\n",
    "            return qml.expval(qml.PauliZ(0))\n",
    "        \n",
    "        self.circuit = circuit\n",
    "\n",
    "    def feature_encoding(self, x):\n",
    "        # Encode features using RY rotations and entangle adjacent qubits with CZ.\n",
    "        for i in range(self.num_qubits):\n",
    "            qml.RY(np.pi * x[i], wires=i)\n",
    "        for i in range(self.num_qubits - 1):\n",
    "            qml.CZ(wires=[i, i + 1])\n",
    "\n",
    "    def variational_layer(self, W):\n",
    "        # Apply a parameterized rotation on each qubit.\n",
    "        for i in range(self.num_qubits):\n",
    "            qml.Rot(W[i, 0], W[i, 1], W[i, 2], wires=i)\n",
    "        # Apply CNOT gates between adjacent qubits.\n",
    "        for i in range(self.num_qubits - 1):\n",
    "            qml.CNOT(wires=[i, i + 1])\n",
    "        # If there is more than one qubit, add a wrap‑around CNOT.\n",
    "        if self.num_qubits > 1:\n",
    "            qml.CNOT(wires=[self.num_qubits - 1, 0])\n",
    "\n",
    "    def variational_classifier(self, weights, bias, x):\n",
    "        # Compute the circuit’s output and add a bias.\n",
    "        return self.circuit(weights, x) + bias\n",
    "\n",
    "    def cost(self, weights, bias, X, Y):\n",
    "        # Compute mean squared error cost over the entire training set.\n",
    "        predictions = qml.numpy.array([self.variational_classifier(weights, bias, x) for x in X])\n",
    "        return qml.numpy.mean((qml.numpy.array(Y) - predictions) ** 2)\n",
    "\n",
    "    def fit(self, X_train, y_train, num_it=50, warm_start=False, verbose=False):\n",
    "        \"\"\"\n",
    "        Fit the quantum learner.\n",
    "        \n",
    "        Parameters:\n",
    "          - X_train, y_train: training data (can be pandas DataFrame/Series or NumPy arrays)\n",
    "          - num_it: number of epochs (full forward/backward passes over the full training set)\n",
    "          - warm_start: if True, retain parameters if the feature set (number of qubits) is unchanged.\n",
    "          - verbose: if True, print cost and training accuracy each epoch.\n",
    "        \"\"\"\n",
    "        # Convert inputs to NumPy arrays if needed.\n",
    "        if hasattr(X_train, \"values\"):\n",
    "            X_train = X_train.values.astype(np.float64)\n",
    "        else:\n",
    "            X_train = np.array(X_train, dtype=np.float64)\n",
    "        if hasattr(y_train, \"values\"):\n",
    "            y_train = y_train.values\n",
    "        else:\n",
    "            y_train = np.array(y_train)\n",
    "        \n",
    "        current_features = X_train.shape[1]\n",
    "        # If not warm starting, or if the number of features has changed, reinitialize.\n",
    "        if (not warm_start) or (self.num_qubits is None) or (self.num_qubits != current_features):\n",
    "            self._initialize_circuit(current_features)\n",
    "            np.random.seed(0)\n",
    "            self.weights = qml.numpy.tensor(\n",
    "                0.01 * np.random.randn(self.num_layers, self.num_qubits, 3),\n",
    "                requires_grad=True,\n",
    "            )\n",
    "            self.bias = qml.numpy.tensor(0.0, requires_grad=True)\n",
    "        \n",
    "        # Train over multiple epochs using the full training set each epoch.\n",
    "        for epoch in range(num_it):\n",
    "            # Perform one optimization step on the entire training set.\n",
    "            self.weights, self.bias = self.opt.step(\n",
    "                lambda w, b: self.cost(w, b, X_train, y_train),\n",
    "                self.weights, self.bias\n",
    "            )\n",
    "            if verbose:\n",
    "                cost_val = self.cost(self.weights, self.bias, X_train, y_train)\n",
    "                # Compute training accuracy by thresholding the circuit output.\n",
    "                predictions = qml.numpy.array([\n",
    "                    qml.numpy.sign(self.variational_classifier(self.weights, self.bias, x))\n",
    "                    for x in X_train\n",
    "                ])\n",
    "                # Note: assuming y_train labels are in {-1, 1}.\n",
    "                acc = qml.numpy.mean(qml.numpy.abs(qml.numpy.array(y_train) - predictions) < 1e-5)\n",
    "                print(f\"Epoch: {epoch+1:3d} | Cost: {cost_val:0.7f} | Accuracy: {acc:0.7f}\")\n",
    "        \n",
    "        return self\n",
    "\n",
    "    def predict(self, X_test):\n",
    "        # Convert to NumPy array if needed.\n",
    "        if hasattr(X_test, \"values\"):\n",
    "            X_test = X_test.values.astype(np.float64)\n",
    "        else:\n",
    "            X_test = np.array(X_test, dtype=np.float64)\n",
    "        return np.array([\n",
    "            float(qml.numpy.sign(self.variational_classifier(self.weights, self.bias, x)))\n",
    "            for x in X_test\n",
    "        ])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (venv)",
   "language": "python",
   "name": "venv"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
