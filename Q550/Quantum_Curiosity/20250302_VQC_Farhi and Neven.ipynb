{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "9eff16e2-8186-41cb-8129-91132103e173",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Raw circuit outputs before training: [-1. -1. -1. -1. -1. -1. -1. -1. -1. -1.]\n",
      "\n",
      "Iteration 0: Parameter Check\n",
      "Theta Mean: 0.013460, Std: 0.007080\n",
      "Iteration 0: Unique Prediction Values: [-1.]\n",
      "WARNING: Model is outputting the same value for all inputs!\n",
      "Iter:     1 | Cost: 1.0282440 | Accuracy: 0.7429390\n",
      "\n",
      "Iteration 1: Parameter Check\n",
      "Theta Mean: 0.013460, Std: 0.007080\n",
      "Iteration 1: Unique Prediction Values: [-1.]\n",
      "WARNING: Model is outputting the same value for all inputs!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/jbg/.venv/lib/python3.13/site-packages/pennylane/_grad.py:216: UserWarning: Attempted to differentiate a function with no trainable parameters. If this is unintended, please add trainable parameters via the 'requires_grad' attribute or 'argnum' keyword.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iter:     2 | Cost: 1.0282440 | Accuracy: 0.7429390\n",
      "\n",
      "Iteration 2: Parameter Check\n",
      "Theta Mean: 0.013460, Std: 0.007080\n",
      "Iteration 2: Unique Prediction Values: [-1.]\n",
      "WARNING: Model is outputting the same value for all inputs!\n",
      "Iter:     3 | Cost: 1.0282440 | Accuracy: 0.7429390\n",
      "\n",
      "Iteration 3: Parameter Check\n",
      "Theta Mean: 0.013460, Std: 0.007080\n",
      "Iteration 3: Unique Prediction Values: [-1.]\n",
      "WARNING: Model is outputting the same value for all inputs!\n",
      "Iter:     4 | Cost: 1.0282440 | Accuracy: 0.7429390\n",
      "\n",
      "Iteration 4: Parameter Check\n",
      "Theta Mean: 0.013460, Std: 0.007080\n",
      "Iteration 4: Unique Prediction Values: [-1.]\n",
      "WARNING: Model is outputting the same value for all inputs!\n",
      "Iter:     5 | Cost: 1.0282440 | Accuracy: 0.7429390\n",
      "\n",
      "Iteration 5: Parameter Check\n",
      "Theta Mean: 0.013460, Std: 0.007080\n",
      "Iteration 5: Unique Prediction Values: [-1.]\n",
      "WARNING: Model is outputting the same value for all inputs!\n",
      "Iter:     6 | Cost: 1.0282440 | Accuracy: 0.7429390\n",
      "\n",
      "Iteration 6: Parameter Check\n",
      "Theta Mean: 0.013460, Std: 0.007080\n",
      "Iteration 6: Unique Prediction Values: [-1.]\n",
      "WARNING: Model is outputting the same value for all inputs!\n",
      "Iter:     7 | Cost: 1.0282440 | Accuracy: 0.7429390\n",
      "\n",
      "Iteration 7: Parameter Check\n",
      "Theta Mean: 0.013460, Std: 0.007080\n",
      "Iteration 7: Unique Prediction Values: [-1.]\n",
      "WARNING: Model is outputting the same value for all inputs!\n",
      "Iter:     8 | Cost: 1.0282440 | Accuracy: 0.7429390\n",
      "\n",
      "Iteration 8: Parameter Check\n",
      "Theta Mean: 0.013460, Std: 0.007080\n",
      "Iteration 8: Unique Prediction Values: [-1.]\n",
      "WARNING: Model is outputting the same value for all inputs!\n",
      "Iter:     9 | Cost: 1.0282440 | Accuracy: 0.7429390\n",
      "\n",
      "Iteration 9: Parameter Check\n",
      "Theta Mean: 0.013460, Std: 0.007080\n",
      "Iteration 9: Unique Prediction Values: [-1.]\n",
      "WARNING: Model is outputting the same value for all inputs!\n",
      "Iter:    10 | Cost: 1.0282440 | Accuracy: 0.7429390\n",
      "Total training time: 53.62 seconds\n",
      "Iteration Final Test Set: Unique Prediction Values: [-1.]\n",
      "WARNING: Model is outputting the same value for all inputs!\n",
      "\n",
      "Quantum Model Performance:\n",
      "Accuracy: 0.7426\n",
      "Precision: 1.0000\n",
      "Recall: 0.0000\n",
      "F1 Score: 0.4262\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import pennylane as qml\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "import time\n",
    "\n",
    "# === Load and Preprocess the Adult Dataset ===\n",
    "df_adult = pd.read_csv('Datasets/adult/adult_test_int.csv')\n",
    "df_adult = df_adult.drop(columns=[\"Unnamed: 0\"])  # Drop index column\n",
    "\n",
    "# Select numerical features for the Quantum Model\n",
    "selected_features = [\"age\", \"capital.gain\", \"capital.loss\", \"hours.per.week\"]\n",
    "\n",
    "# Standardize numerical features\n",
    "scaler = StandardScaler()\n",
    "df_adult[selected_features] = scaler.fit_transform(df_adult[selected_features])\n",
    "\n",
    "# ✅ FIX: Convert numerical features to {-1,1} encoding (instead of {0,1})\n",
    "X_quantum = 2 * (df_adult[selected_features].values >= 0).astype(float) - 1\n",
    "\n",
    "# Define Target Variable and convert to {-1, +1}\n",
    "y = df_adult[\"over50K\"].values\n",
    "y_quantum = y * 2 - 1  # Converts {0,1} → {-1,1}\n",
    "\n",
    "# Split dataset into training and testing sets\n",
    "X_train_q, X_test_q, y_train_q, y_test_q = train_test_split(\n",
    "    X_quantum, y_quantum, test_size=0.10, random_state=42, stratify=y\n",
    ")\n",
    "\n",
    "# === Quantum Model Setup (Farhi & Neven Ansatz) ===\n",
    "num_data_qubits = 4  # Number of features\n",
    "total_wires = num_data_qubits + 1  # Extra readout qubit\n",
    "dev = qml.device(\"default.qubit\", wires=total_wires)\n",
    "\n",
    "def state_preparation(x):\n",
    "    \"\"\"Prepare the state |x,1>:\n",
    "    - Data qubits: encode {-1,1} features (flip qubit if x[i] == 1)\n",
    "    - Readout qubit: prepare in |1>\n",
    "    \"\"\"\n",
    "    for i in range(num_data_qubits):\n",
    "        if x[i] == 1:\n",
    "            qml.PauliX(wires=i)\n",
    "    qml.PauliX(wires=num_data_qubits)  # Readout qubit to |1>\n",
    "\n",
    "@qml.qnode(dev)\n",
    "def circuit(theta, x):\n",
    "    \"\"\"Implements the Farhi–Neven ansatz with correct encoding\"\"\"\n",
    "    state_preparation(x)\n",
    "    \n",
    "    # Controlled rotations for each data qubit\n",
    "    for j in range(num_data_qubits):\n",
    "        qml.RZ(-np.pi/2, wires=total_wires - 1)\n",
    "        qml.CNOT(wires=[j, total_wires - 1])\n",
    "        qml.RZ(-2 * theta[j], wires=total_wires - 1)\n",
    "        qml.CNOT(wires=[j, total_wires - 1])\n",
    "        qml.RZ(np.pi/2, wires=total_wires - 1)\n",
    "\n",
    "    # ✅ FIX: Final rotation e^(i π/4 X) is correctly applied\n",
    "    qml.RX(-np.pi/2, wires=total_wires - 1)\n",
    "\n",
    "    # ✅ FIX: Ensuring correct measurement in Pauli-Y basis\n",
    "    return qml.expval(qml.PauliY(wires=total_wires - 1))\n",
    "\n",
    "def variational_classifier(theta, x):\n",
    "    \"\"\"Ensures that the expectation value output is converted to a float.\"\"\"\n",
    "    return float(circuit(theta, x))\n",
    "\n",
    "def square_loss(labels, predictions):\n",
    "    return np.mean((labels - predictions) ** 2)\n",
    "\n",
    "def cost(theta, X, Y):\n",
    "    \"\"\"Computes the cost function.\"\"\"\n",
    "    predictions = np.array([variational_classifier(theta, x) for x in X])\n",
    "    return square_loss(Y, predictions)\n",
    "\n",
    "# === Initialize Quantum Model Parameters ===\n",
    "np.random.seed(0)\n",
    "theta_init = np.random.randn(num_data_qubits) * 0.01  # One parameter per data qubit\n",
    "\n",
    "# ✅ FIX: Use Adam instead of simple gradient descent\n",
    "opt = qml.AdamOptimizer(stepsize=0.1)  # Adaptive learning\n",
    "\n",
    "num_it = 10\n",
    "batch_size = min(48, len(X_train_q))  # Ensure batch_size does not exceed dataset size\n",
    "\n",
    "# === Debugging Helpers ===\n",
    "def debug_parameters(theta, iteration):\n",
    "    \"\"\"Print parameter statistics to check if theta is updating.\"\"\"\n",
    "    print(f\"\\nIteration {iteration}: Parameter Check\")\n",
    "    print(f\"Theta Mean: {np.mean(theta):.6f}, Std: {np.std(theta):.6f}\")\n",
    "\n",
    "def debug_predictions(predictions, iteration):\n",
    "    \"\"\"Print unique predictions to check for constant outputs.\"\"\"\n",
    "    unique_values = np.unique(predictions)\n",
    "    print(f\"Iteration {iteration}: Unique Prediction Values: {unique_values}\")\n",
    "    if len(unique_values) == 1:\n",
    "        print(\"WARNING: Model is outputting the same value for all inputs!\")\n",
    "\n",
    "# === Debug: Check Circuit Output Before Training ===\n",
    "raw_outputs = np.array([variational_classifier(theta_init, x) for x in X_train_q[:10]])\n",
    "print(\"Raw circuit outputs before training:\", raw_outputs)\n",
    "\n",
    "# === Train Quantum Model ===\n",
    "start_time = time.time()\n",
    "theta = theta_init.copy()\n",
    "\n",
    "for it in range(num_it):\n",
    "    batch_idx = np.random.choice(len(X_train_q), batch_size, replace=False)\n",
    "    X_batch = X_train_q[batch_idx]\n",
    "    Y_batch = y_train_q[batch_idx]\n",
    "\n",
    "    # Debug parameter values before update\n",
    "    debug_parameters(theta, it)\n",
    "\n",
    "    # Optimization step\n",
    "    theta = opt.step(lambda th: cost(th, X_batch, Y_batch), theta)\n",
    "\n",
    "    # Compute training accuracy\n",
    "    predictions = np.array([np.sign(variational_classifier(theta, x)) for x in X_train_q])\n",
    "    acc = np.mean(predictions == y_train_q)\n",
    "\n",
    "    # Debug if predictions are constant\n",
    "    debug_predictions(predictions, it)\n",
    "\n",
    "    print(f\"Iter: {it+1:5d} | Cost: {cost(theta, X_train_q, y_train_q):0.7f} | Accuracy: {acc:0.7f}\")\n",
    "\n",
    "print(f\"Total training time: {time.time() - start_time:.2f} seconds\")\n",
    "\n",
    "# === Quantum Model Evaluation ===\n",
    "predictions_q = np.array([np.sign(variational_classifier(theta, x)) for x in X_test_q])\n",
    "\n",
    "debug_predictions(predictions_q, \"Final Test Set\")\n",
    "\n",
    "print(\"\\nQuantum Model Performance:\")\n",
    "print(f\"Accuracy: {accuracy_score(y_test_q, predictions_q):.4f}\")\n",
    "print(f\"Precision: {precision_score(y_test_q, predictions_q, zero_division=1):.4f}\")\n",
    "print(f\"Recall: {recall_score(y_test_q, predictions_q, zero_division=1):.4f}\")\n",
    "print(f\"F1 Score: {f1_score(y_test_q, predictions_q, average='macro'):.4f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "3c1c4237-d9e0-4cca-a182-34f058a3d0c9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Classical Model Performance:\n",
      "Accuracy: 0.7978\n",
      "Precision: 0.7586\n",
      "Recall: 0.3143\n",
      "F1 Score: 0.6604\n"
     ]
    }
   ],
   "source": [
    "# Select same features for Classical Model\n",
    "selected_features = [\"age\", \"capital.gain\", \"capital.loss\", \"hours.per.week\"]\n",
    "X_classical = df_adult[selected_features].values  # Now same as Quantum model\n",
    "\n",
    "# Update Train/Test Split\n",
    "X_train_c, X_test_c, y_train_c, y_test_c = train_test_split(X_classical, y, test_size=0.10, random_state=42, stratify=y)\n",
    "\n",
    "# Define Classical ANN Model\n",
    "class ClassicalANN(nn.Module):\n",
    "    def __init__(self, layers):\n",
    "        super().__init__()\n",
    "        self.layers = nn.ModuleList([nn.Linear(layers[i], layers[i+1], dtype=torch.float64) for i in range(len(layers) - 1)])\n",
    "        self.sigmoid = nn.Sigmoid()\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = x.to(torch.float64)\n",
    "        for layer in self.layers[:-1]:\n",
    "            x = torch.relu(layer(x))\n",
    "        return self.sigmoid(self.layers[-1](x))\n",
    "\n",
    "# Train Classical Model with the same 4 features\n",
    "classical_model = ClassicalANN([4, 5, 1])  # Input layer now has 4 neurons\n",
    "optimizer_classical = optim.Adam(classical_model.parameters(), lr=0.01)\n",
    "\n",
    "def train_classical(model, optimizer, X_train, y_train, epochs=50):\n",
    "    y_train = torch.tensor(y_train.tolist(), dtype=torch.float64).reshape(-1, 1)\n",
    "    for epoch in range(epochs):\n",
    "        optimizer.zero_grad()\n",
    "        y_pred = model(torch.tensor(X_train.tolist(), dtype=torch.float64)).reshape(-1, 1)\n",
    "        loss = nn.BCELoss()(y_pred, y_train)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "train_classical(classical_model, optimizer_classical, X_train_c, y_train_c, epochs=50)\n",
    "\n",
    "# Evaluate Classical Model\n",
    "with torch.no_grad():\n",
    "    X_test_c_numeric = np.array(X_test_c, dtype=np.float64)\n",
    "    y_pred_classical = classical_model(torch.tensor(X_test_c_numeric, dtype=torch.float64)).reshape(-1, 1)\n",
    "    y_pred_classical = (y_pred_classical.numpy().flatten() > 0.5).astype(int)\n",
    "\n",
    "print(\"\\nClassical Model Performance:\")\n",
    "print(f\"Accuracy: {accuracy_score(y_test_c, y_pred_classical):.4f}\")\n",
    "print(f\"Precision: {precision_score(y_test_c, y_pred_classical):.4f}\")\n",
    "print(f\"Recall: {recall_score(y_test_c, y_pred_classical):.4f}\")\n",
    "print(f\"F1 Score: {f1_score(y_test_c, y_pred_classical, average='macro'):.4f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e588777c-22b4-4ca4-8e95-900f901e8813",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (venv)",
   "language": "python",
   "name": "venv"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
